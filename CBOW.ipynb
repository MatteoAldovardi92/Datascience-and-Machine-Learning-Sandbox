{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOwN3Po/nLQTEi2O/GsYgMw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoAldovardi92/Datascience-and-Machine-Learning-Sandbox/blob/main/CBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P204DBkQz91y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import re # For more robust tokenization\n",
        "import matplotlib.pyplot as plt # For visualization\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "# --- Preprocessing Functions (from previous response) ---\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and tokenizes raw text.\n",
        "    Converts to lowercase, removes most special characters, and splits into words.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    # Remove characters that are not letters, numbers, spaces, or selected punctuation (', -, .)\n",
        "    text = re.sub(r'[^a-z0-9\\s\\'-.]', '', text)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def build_vocabulary(tokens, min_freq=5):\n",
        "    \"\"\"\n",
        "    Builds a word-to-ID and ID-to-word mapping, filtering by minimum frequency.\n",
        "    Adds special tokens for padding and unknown words.\n",
        "    \"\"\"\n",
        "    word_counts = defaultdict(int)\n",
        "    for word in tokens:\n",
        "        word_counts[word] += 1\n",
        "\n",
        "    # Filter out words that appear less than min_freq times\n",
        "    filtered_vocab_items = [item for item in word_counts.items() if item[1] >= min_freq]\n",
        "    # Sort by frequency for consistent ID assignment\n",
        "    sorted_vocab = sorted(filtered_vocab_items, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    word_to_id = {'<PAD>': 0, '<UNK>': 1} # Initialize with special tokens\n",
        "    id_to_word = {0: '<PAD>', 1: '<UNK>'}\n",
        "\n",
        "    # Assign IDs to words based on sorted frequency\n",
        "    for word, _ in sorted_vocab:\n",
        "        if word not in word_to_id: # Ensure special tokens aren't overwritten\n",
        "            word_to_id[word] = len(word_to_id)\n",
        "            id_to_word[len(id_to_word)] = word\n",
        "\n",
        "    vocab_size = len(word_to_id)\n",
        "    return word_to_id, id_to_word, vocab_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Dowload tinyshakespeare.txt\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "file_path = 'tinyshakespeare.txt'\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(response.text)\n",
        "    print(f\"Downloaded '{url}' to '{file_path}'\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading the file: {e}\")\n",
        "except IOError as e:\n",
        "    print(f\"Error writing the file '{file_path}': {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJxF8wFx1lH-",
        "outputId": "c7328123-6544-4ed7-be98-46441ee589f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt' to 'tinyshakespeare.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "file_path = 'tinyshakespeare.txt'\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_text = f.read()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '{file_path}' not found. Please download it from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
        "    exit()\n",
        "\n",
        "print(\"--- Step 1: Initial Text Loading ---\")\n",
        "print(f\"First 500 characters of raw text:\\n{raw_text[:500]}...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0I2bKmb0p_N",
        "outputId": "9faccfaf-c61e-47a2-9223-0b07afc61204"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Initial Text Loading ---\n",
            "First 500 characters of raw text:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Perform tokenization\n",
        "tokens = preprocess_text(raw_text)\n",
        "\n",
        "print(\"--- Step 2: Preprocessing and Tokenization ---\")\n",
        "print(f\"Total tokens after preprocessing: {len(tokens)}\")\n",
        "print(f\"First 20 tokens:\\n{tokens[:20]}\\n\")\n",
        "print(f\"Last 20 tokens:\\n{tokens[-20:]}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IlWs_T-e1V76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d1a5c6f-4f01-4b2a-d1b4-481156d42433"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 2: Preprocessing and Tokenization ---\n",
            "Total tokens after preprocessing: 202649\n",
            "First 20 tokens:\n",
            "['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.', 'all', 'speak,', 'speak.', 'first', 'citizen', 'you', 'are', 'all', 'resolved', 'rather']\n",
            "\n",
            "Last 20 tokens:\n",
            "['moving,', 'and', 'yet', 'so', 'fast', 'asleep.', 'antonio', 'noble', 'sebastian,', 'thou', \"let'st\", 'thy', 'fortune', 'sleep--die,', 'rather', \"wink'st\", 'whiles', 'thou', 'art', 'waking.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary\n",
        "min_word_frequency = 10 # Words appearing less than 10 times will be <UNK>\n",
        "word_to_id, id_to_word, vocab_size = build_vocabulary(tokens, min_freq=min_word_frequency)\n",
        "\n",
        "print(\"--- Step 3: Vocabulary Construction ---\")\n",
        "print(f\"Vocabulary Size (with min_freq={min_word_frequency}): {vocab_size}\")\n",
        "\n",
        "print(\"\\nTop 10 most frequent words (by ID):\")\n",
        "for i in range(2, 12): # Start from 2 to skip <PAD> and <UNK>\n",
        "    if i < vocab_size:\n",
        "        print(f\"ID: {i}, Word: '{id_to_word[i]}'\")\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print(\"\\nBottom 10 words (least frequent words that met min_freq, by ID):\")\n",
        "# Get the last 10 entries from the sorted vocabulary (before special tokens)\n",
        "num_to_show = min(10, vocab_size - 2) # Don't show more than available\n",
        "for i in range(vocab_size - num_to_show, vocab_size):\n",
        "      print(f\"ID: {i}, Word: '{id_to_word[i]}'\")\n",
        "\n",
        "\n",
        "# Convert entire corpus to numerical IDs\n",
        "indexed_corpus = [word_to_id.get(word, word_to_id['<UNK>']) for word in tokens]\n",
        "\n",
        "print(\"\\n--- Step 4: Corpus Indexing (Conversion to Numbers) ---\")\n",
        "print(f\"Length of indexed corpus: {len(indexed_corpus)}\")\n",
        "print(f\"First 20 indexed tokens:\\n{indexed_corpus[:20]}\\n\")\n",
        "print(f\"Last 20 indexed tokens:\\n{indexed_corpus[-20:]}\\n\")\n",
        "\n",
        "# Verify a few translations\n",
        "print(\"--- Step 5: Verification ---\")\n",
        "sample_text = \"the king loves his queen, and the queen loves her king.\"\n",
        "sample_tokens = preprocess_text(sample_text)\n",
        "sample_indexed = [word_to_id.get(word, word_to_id['<UNK>']) for word in sample_tokens]\n",
        "\n",
        "print(f\"Sample text: '{sample_text}'\")\n",
        "print(f\"Sample tokens: {sample_tokens}\")\n",
        "print(f\"Sample indexed: {sample_indexed}\")\n",
        "\n",
        "# Decode back for verification\n",
        "decoded_sample = [id_to_word.get(idx, '<UNK>') for idx in sample_indexed]\n",
        "print(f\"Decoded sample: {decoded_sample}\")\n",
        "\n",
        "# Check for an unknown word\n",
        "unknown_word = \"xyzzy\" # Highly unlikely to be in Shakespeare\n",
        "unknown_id = word_to_id.get(unknown_word, word_to_id['<UNK>'])\n",
        "print(f\"\\nID for unknown word '{unknown_word}': {unknown_id} (which should be {word_to_id['<UNK>']})\")\n",
        "print(f\"Word for ID {word_to_id['<UNK>']}: '{id_to_word[word_to_id['<UNK>']]}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnegaIgv3ArG",
        "outputId": "a87a0460-a8df-4454-f2e7-d8ba493bf6e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 3: Vocabulary Construction ---\n",
            "Vocabulary Size (with min_freq=10): 2001\n",
            "\n",
            "Top 10 most frequent words (by ID):\n",
            "ID: 2, Word: 'the'\n",
            "ID: 3, Word: 'and'\n",
            "ID: 4, Word: 'to'\n",
            "ID: 5, Word: 'i'\n",
            "ID: 6, Word: 'of'\n",
            "ID: 7, Word: 'my'\n",
            "ID: 8, Word: 'a'\n",
            "ID: 9, Word: 'you'\n",
            "ID: 10, Word: 'that'\n",
            "ID: 11, Word: 'in'\n",
            "\n",
            "Bottom 10 words (least frequent words that met min_freq, by ID):\n",
            "ID: 1991, Word: 'houses'\n",
            "ID: 1992, Word: 'clear'\n",
            "ID: 1993, Word: 'bona'\n",
            "ID: 1994, Word: 'instruct'\n",
            "ID: 1995, Word: 'curst'\n",
            "ID: 1996, Word: 'angelo.'\n",
            "ID: 1997, Word: 'claudio,'\n",
            "ID: 1998, Word: 'provost,'\n",
            "ID: 1999, Word: 'lucentio.'\n",
            "ID: 2000, Word: 'alonso'\n",
            "\n",
            "--- Step 4: Corpus Indexing (Conversion to Numbers) ---\n",
            "Length of indexed corpus: 202649\n",
            "First 20 indexed tokens:\n",
            "[86, 250, 143, 33, 1291, 136, 1, 130, 25, 591, 35, 571, 591, 86, 250, 9, 39, 35, 1468, 352]\n",
            "\n",
            "Last 20 indexed tokens:\n",
            "[1, 3, 82, 28, 881, 1, 590, 142, 1, 26, 1, 27, 450, 1, 352, 1, 1139, 26, 132, 1]\n",
            "\n",
            "--- Step 5: Verification ---\n",
            "Sample text: 'the king loves his queen, and the queen loves her king.'\n",
            "Sample tokens: ['the', 'king', 'loves', 'his', 'queen,', 'and', 'the', 'queen', 'loves', 'her', 'king.']\n",
            "Sample indexed: [2, 38, 880, 18, 588, 3, 2, 93, 880, 41, 569]\n",
            "Decoded sample: ['the', 'king', 'loves', 'his', 'queen,', 'and', 'the', 'queen', 'loves', 'her', 'king.']\n",
            "\n",
            "ID for unknown word 'xyzzy': 1 (which should be 1)\n",
            "Word for ID 1: '<UNK>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexed_corpus = torch.tensor(indexed_corpus)\n",
        "window_size = 2"
      ],
      "metadata": {
        "id": "agm8ubgg6XZa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step to generate context-target pairs ---\n",
        "\n",
        "# Initialize an empty list to store context-target pairs\n",
        "context_target_pairs = []\n",
        "\n",
        "# Define the context window size (2 words before and 2 words after)\n",
        "context_length = 2\n",
        "\n",
        "# Iterate through the indexed corpus to create context-target pairs\n",
        "# We need to ensure we have enough words for the context window around the target word.\n",
        "# The target word is at index 'i'. The context is from i-context_length to i+context_length, excluding i.\n",
        "# We need i to be at least context_length to have words before it.\n",
        "# We need i to be at most len(indexed_corpus) - context_length - 1 to have words after it.\n",
        "for i in range(context_length, len(indexed_corpus) - context_length):\n",
        "    target_id = indexed_corpus[i]\n",
        "\n",
        "    # Get the indices for the words before the target\n",
        "    before_context_ids = indexed_corpus[i - context_length:i]\n",
        "\n",
        "    # Get the indices for the words after the target\n",
        "    after_context_ids = indexed_corpus[i + 1:i + context_length + 1]\n",
        "\n",
        "    # Combine the context word IDs\n",
        "    # We need to handle padding here if a full window isn't available at the beginning or end.\n",
        "    # For now, we'll assume a full window is available based on the loop range.\n",
        "    # The padding strategy will be addressed in a later step.\n",
        "    context_ids = torch.cat((before_context_ids, after_context_ids))\n",
        "\n",
        "    # Append the context-target pair to the list\n",
        "    context_target_pairs.append((context_ids, target_id))\n",
        "\n",
        "print(f\"Generated {len(context_target_pairs)} context-target pairs.\")\n"
      ],
      "metadata": {
        "id": "5BhnU7E344g6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "499cf99a"
      },
      "source": [
        "# Task\n",
        "Generate context-target pairs from \"input.txt\" where the context is the two words before and two words after the target word, excluding the target word. Implement padding for contexts at the beginning and end of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7efe01aa"
      },
      "source": [
        "## Modify context window generation\n",
        "\n",
        "### Subtask:\n",
        "Update the code to generate context-target pairs where the context includes two words before and two words after the target word, excluding the target word itself.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "359fff3d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to generate context-target pairs failed due to an incorrect loop structure and an attempt to drop elements from a tensor in place. This code will iterate through the `indexed_corpus` with the correct bounds to create context-target pairs as specified in the instructions, ensuring the context includes two words before and two words after the target word, excluding the target word itself.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f8df4df",
        "outputId": "0a9c4bda-334e-4648-e436-e220b98c59d4"
      },
      "source": [
        "# Assuming you have generated context_target_pairs in a previous step\n",
        "# If not, you'll need to run the code to generate them first.\n",
        "\n",
        "print(\"\\n--- Step 6: Displaying Sample Context-Target Pairs ---\")\n",
        "# Print a few sample pairs\n",
        "num_samples_to_display = 5\n",
        "\n",
        "for i in range(min(num_samples_to_display, len(context_target_pairs))):\n",
        "    context_ids = context_target_pairs[i][0]\n",
        "    target_id = context_target_pairs[i][1]\n",
        "\n",
        "    context_words = [id_to_word.get(idx.item() if isinstance(idx, torch.Tensor) else idx, '<UNK>') for idx in context_ids]\n",
        "    target_word = id_to_word.get(target_id.item() if isinstance(target_id, torch.Tensor) else target_id, '<UNK>')\n",
        "\n",
        "    print(f\"Pair {i+1}:\")\n",
        "    print(f\"  Context IDs: {context_ids}\")\n",
        "    print(f\"  Context Words: {context_words}\")\n",
        "    print(f\"  Target ID: {target_id}\")\n",
        "    print(f\"  Target Word: {target_word}\\n\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 6: Displaying Sample Context-Target Pairs ---\n",
            "Pair 1:\n",
            "  Context IDs: tensor([  86,  250,   33, 1291])\n",
            "  Context Words: ['first', 'citizen', 'we', 'proceed']\n",
            "  Target ID: 143\n",
            "  Target Word: before\n",
            "\n",
            "Pair 2:\n",
            "  Context IDs: tensor([ 250,  143, 1291,  136])\n",
            "  Context Words: ['citizen', 'before', 'proceed', 'any']\n",
            "  Target ID: 33\n",
            "  Target Word: we\n",
            "\n",
            "Pair 3:\n",
            "  Context IDs: tensor([143,  33, 136,   1])\n",
            "  Context Words: ['before', 'we', 'any', '<UNK>']\n",
            "  Target ID: 1291\n",
            "  Target Word: proceed\n",
            "\n",
            "Pair 4:\n",
            "  Context IDs: tensor([  33, 1291,    1,  130])\n",
            "  Context Words: ['we', 'proceed', '<UNK>', 'hear']\n",
            "  Target ID: 136\n",
            "  Target Word: any\n",
            "\n",
            "Pair 5:\n",
            "  Context IDs: tensor([1291,  136,  130,   25])\n",
            "  Context Words: ['proceed', 'any', 'hear', 'me']\n",
            "  Target ID: 1\n",
            "  Target Word: <UNK>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.8\n",
        "train_data = context_target_pairs[:int(train_ratio * len(context_target_pairs))]\n",
        "test_data = context_target_pairs[int(train_ratio * len(context_target_pairs)):]"
      ],
      "metadata": {
        "id": "XFbHiGeK8nI2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2c7d9d4",
        "outputId": "0d497ac1-a371-46be-cc25-61e0cad268d1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --- Hints for Building Your Model ---\n",
        "\n",
        "# 1. Define the model architecture:\n",
        "#    - You'll typically use an Embedding layer to convert word IDs into dense vectors.\n",
        "#    - A few Linear (dense) layers with activation functions (like ReLU) can process the context embeddings.\n",
        "#    - The final layer should be a Linear layer with an output size equal to your vocabulary size.\n",
        "#    - Why vocab_size and not batch_size for the final layer's output?\n",
        "#      # The model's goal is to predict the probability of each possible word in your vocabulary\n",
        "#      # being the target word, given the context.\n",
        "#      # Therefore, the output layer needs to produce a score (or logit) for every word\n",
        "#      # in your vocabulary, representing how likely that word is to be the target.\n",
        "#      # The size of this output is directly tied to the total number of unique words\n",
        "#      # your model knows (the vocabulary size), not the number of examples\n",
        "#      # being processed in a single step (the batch size).\n",
        "#    - A Softmax layer (or combine with the loss function) will convert the final layer's outputs into probabilities over the vocabulary.\n",
        "\n",
        "# Example (replace with your actual model definition):\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, context_window_size):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # linear1 takes the flattened context embeddings.\n",
        "        # The input dimension here is (context_window_size * embedding_dim) because\n",
        "        # for each example in the batch, we have 'context_window_size' words,\n",
        "        # and each word is represented by an 'embedding_dim' vector.\n",
        "        # The batch size is handled implicitly by PyTorch's linear layer;\n",
        "        # it operates on each example in the batch independently.\n",
        "        self.linear1 = nn.Linear(embedding_dim * context_window_size, hidden_dim)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context_ids):\n",
        "        # context_ids shape: (batch_size, context_window_size)\n",
        "        embedded = self.embedding(context_ids) # shape: (batch_size, context_window_size, embedding_dim)\n",
        "        # Flatten the embedded context for the linear layers\n",
        "        # The .view() operation preserves the batch dimension implicitly.\n",
        "        # It reshapes each item in the batch from (context_window_size, embedding_dim)\n",
        "        # to a single vector of size (context_window_size * embedding_dim).\n",
        "        flattened_context = embedded.view(embedded.size(0), -1) # shape: (batch_size, context_window_size * embedding_dim)\n",
        "        hidden = self.layer_norm(self.linear1(flattened_context)) # shape: (batch_size, hidden_dim)\n",
        "        hidden = self.relu(hidden) # shape: (batch_size, hidden_dim)\n",
        "        output = self.linear2(hidden) # shape: (batch_size, vocab_size)\n",
        "        # Note: Softmax is often included in the loss function (e.g., nn.CrossEntropyLoss)\n",
        "        return output\n",
        "\n",
        "# 2. Instantiate the model:\n",
        "embedding_dim = 1000 # Choose an appropriate dimension\n",
        "hidden_dim = 300   # Choose an appropriate dimension\n",
        "# # Remember to define context_window_size based on your context (e.g., 4 for 2 before and 2 after)\n",
        "context_window_size = 4 # Example value, adjust based on your definition\n",
        "model = LanguageModel(vocab_size, embedding_dim, hidden_dim, context_window_size)\n",
        "\n",
        "# --- Set device for CUDA ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# 3. Define the loss function and optimizer:\n",
        "#    - For multi-class classification like predicting the next word, Cross-Entropy Loss is suitable.\n",
        "#    - Adam or SGD are common optimizers.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                  weight_decay=0.01, amsgrad=False)\n",
        " # Choose a learning rate\n",
        "\n",
        "# 4. Prepare data for training (create DataLoaders for batching):\n",
        "#    - You'll need to convert your train_data and test_data lists of tuples into PyTorch Tensors.\n",
        "#    - Use `torch.utils.data.TensorDataset` and `torch.utils.data.DataLoader` to handle batching and shuffling (for training data).\n",
        "train_contexts = torch.stack([pair[0] for pair in train_data])\n",
        "train_targets = torch.tensor([pair[1] for pair in train_data])\n",
        "train_dataset = torch.utils.data.TensorDataset(train_contexts, train_targets)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) # Choose a batch size\n",
        "\n",
        "test_contexts = torch.stack([pair[0] for pair in test_data])\n",
        "test_targets = torch.tensor([pair[1] for pair in test_data])\n",
        "test_dataset = torch.utils.data.TensorDataset(test_contexts, test_targets)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False) # No need to shuffle test data\n",
        "\n",
        "# 5. Training loop:\n",
        "#    - Iterate over epochs.\n",
        "#    - In each epoch, iterate over batches from the train_loader.\n",
        "#    - For each batch:\n",
        "#        - context_batch, target_batch = batch # Get context and target tensors for the batch\n",
        "#        - Zero the gradients: optimizer.zero_grad()\n",
        "#        - Forward pass: outputs = model(context_batch)\n",
        "#        - Calculate loss: loss = criterion(outputs, target_batch)\n",
        "#        - Backward pass: loss.backward()\n",
        "#        - Update weights: optimizer.step()\n",
        "#        - Print loss periodically to monitor training progress.\n",
        "\n",
        "number_of_epochs = 100 # Choose an appropriate number of epochs\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        # Move batch to the chosen device\n",
        "        context_batch, target_batch = batch\n",
        "        context_batch, target_batch = context_batch.to(device), target_batch.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(context_batch)\n",
        "\n",
        "        # Calculate loss\n",
        "        # Use the criterion instance you defined earlier\n",
        "        loss = criterion(outputs, target_batch)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Print loss periodically\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{number_of_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    average_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{number_of_epochs}] Average Training Loss: {average_train_loss:.4f}')\n",
        "\n",
        "\n",
        "# 6. Evaluation loop (after training):\n",
        "#    - Use the test_loader.\n",
        "#    - In each batch:\n",
        "#        - context_batch, target_batch = batch # Get context and target tensors for the batch\n",
        "#        - Forward pass: outputs = model(context_batch)\n",
        "#        - Calculate loss: test_loss = criterion(outputs, target_batch)\n",
        "#        - Calculate accuracy or other relevant metrics.\n",
        "#    - Report the average test loss and metrics.\n",
        "\n",
        "# Remember to adjust the 'context_window_size' in the model definition based on your actual context size (2 words before + 2 words after = 4).\n",
        "# The padding strategy will affect how you handle the input to the embedding layer, potentially requiring masks or special padding tokens."
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch [1/100], Step [100/2534], Loss: 5.7524\n",
            "Epoch [1/100], Step [200/2534], Loss: 5.9729\n",
            "Epoch [1/100], Step [300/2534], Loss: 4.3181\n",
            "Epoch [1/100], Step [400/2534], Loss: 4.7428\n",
            "Epoch [1/100], Step [500/2534], Loss: 5.6846\n",
            "Epoch [1/100], Step [600/2534], Loss: 4.0967\n",
            "Epoch [1/100], Step [700/2534], Loss: 5.3944\n",
            "Epoch [1/100], Step [800/2534], Loss: 5.3077\n",
            "Epoch [1/100], Step [900/2534], Loss: 4.4231\n",
            "Epoch [1/100], Step [1000/2534], Loss: 4.8612\n",
            "Epoch [1/100], Step [1100/2534], Loss: 5.0123\n",
            "Epoch [1/100], Step [1200/2534], Loss: 4.1624\n",
            "Epoch [1/100], Step [1300/2534], Loss: 4.9190\n",
            "Epoch [1/100], Step [1400/2534], Loss: 5.0924\n",
            "Epoch [1/100], Step [1500/2534], Loss: 4.2574\n",
            "Epoch [1/100], Step [1600/2534], Loss: 5.2324\n",
            "Epoch [1/100], Step [1700/2534], Loss: 3.7018\n",
            "Epoch [1/100], Step [1800/2534], Loss: 3.8588\n",
            "Epoch [1/100], Step [1900/2534], Loss: 4.4659\n",
            "Epoch [1/100], Step [2000/2534], Loss: 4.6094\n",
            "Epoch [1/100], Step [2100/2534], Loss: 4.3800\n",
            "Epoch [1/100], Step [2200/2534], Loss: 3.8823\n",
            "Epoch [1/100], Step [2300/2534], Loss: 4.1183\n",
            "Epoch [1/100], Step [2400/2534], Loss: 5.1578\n",
            "Epoch [1/100], Step [2500/2534], Loss: 4.7345\n",
            "Epoch [1/100] Average Training Loss: 4.6524\n",
            "Epoch [2/100], Step [100/2534], Loss: 3.8295\n",
            "Epoch [2/100], Step [200/2534], Loss: 3.8652\n",
            "Epoch [2/100], Step [300/2534], Loss: 3.9458\n",
            "Epoch [2/100], Step [400/2534], Loss: 3.4311\n",
            "Epoch [2/100], Step [500/2534], Loss: 3.2507\n",
            "Epoch [2/100], Step [600/2534], Loss: 3.9776\n",
            "Epoch [2/100], Step [700/2534], Loss: 3.9625\n",
            "Epoch [2/100], Step [800/2534], Loss: 3.6523\n",
            "Epoch [2/100], Step [900/2534], Loss: 3.7577\n",
            "Epoch [2/100], Step [1000/2534], Loss: 3.6219\n",
            "Epoch [2/100], Step [1100/2534], Loss: 3.6900\n",
            "Epoch [2/100], Step [1200/2534], Loss: 3.3698\n",
            "Epoch [2/100], Step [1300/2534], Loss: 4.0469\n",
            "Epoch [2/100], Step [1400/2534], Loss: 3.4654\n",
            "Epoch [2/100], Step [1500/2534], Loss: 3.2716\n",
            "Epoch [2/100], Step [1600/2534], Loss: 4.3051\n",
            "Epoch [2/100], Step [1700/2534], Loss: 4.0815\n",
            "Epoch [2/100], Step [1800/2534], Loss: 3.6132\n",
            "Epoch [2/100], Step [1900/2534], Loss: 3.4836\n",
            "Epoch [2/100], Step [2000/2534], Loss: 4.0478\n",
            "Epoch [2/100], Step [2100/2534], Loss: 3.5956\n",
            "Epoch [2/100], Step [2200/2534], Loss: 3.8492\n",
            "Epoch [2/100], Step [2300/2534], Loss: 3.8793\n",
            "Epoch [2/100], Step [2400/2534], Loss: 3.8282\n",
            "Epoch [2/100], Step [2500/2534], Loss: 3.6512\n",
            "Epoch [2/100] Average Training Loss: 3.7871\n",
            "Epoch [3/100], Step [100/2534], Loss: 3.3474\n",
            "Epoch [3/100], Step [200/2534], Loss: 3.0816\n",
            "Epoch [3/100], Step [300/2534], Loss: 3.5291\n",
            "Epoch [3/100], Step [400/2534], Loss: 3.1262\n",
            "Epoch [3/100], Step [500/2534], Loss: 3.3800\n",
            "Epoch [3/100], Step [600/2534], Loss: 2.6548\n",
            "Epoch [3/100], Step [700/2534], Loss: 2.8162\n",
            "Epoch [3/100], Step [800/2534], Loss: 3.5457\n",
            "Epoch [3/100], Step [900/2534], Loss: 2.9185\n",
            "Epoch [3/100], Step [1000/2534], Loss: 3.4071\n",
            "Epoch [3/100], Step [1100/2534], Loss: 3.6416\n",
            "Epoch [3/100], Step [1200/2534], Loss: 3.3618\n",
            "Epoch [3/100], Step [1300/2534], Loss: 3.3214\n",
            "Epoch [3/100], Step [1400/2534], Loss: 2.8969\n",
            "Epoch [3/100], Step [1500/2534], Loss: 3.1894\n",
            "Epoch [3/100], Step [1600/2534], Loss: 2.9492\n",
            "Epoch [3/100], Step [1700/2534], Loss: 3.4061\n",
            "Epoch [3/100], Step [1800/2534], Loss: 2.9110\n",
            "Epoch [3/100], Step [1900/2534], Loss: 2.9002\n",
            "Epoch [3/100], Step [2000/2534], Loss: 3.3121\n",
            "Epoch [3/100], Step [2100/2534], Loss: 3.3367\n",
            "Epoch [3/100], Step [2200/2534], Loss: 2.6351\n",
            "Epoch [3/100], Step [2300/2534], Loss: 2.9472\n",
            "Epoch [3/100], Step [2400/2534], Loss: 3.4729\n",
            "Epoch [3/100], Step [2500/2534], Loss: 2.8178\n",
            "Epoch [3/100] Average Training Loss: 3.2202\n",
            "Epoch [4/100], Step [100/2534], Loss: 2.6659\n",
            "Epoch [4/100], Step [200/2534], Loss: 2.2803\n",
            "Epoch [4/100], Step [300/2534], Loss: 2.4460\n",
            "Epoch [4/100], Step [400/2534], Loss: 2.5744\n",
            "Epoch [4/100], Step [500/2534], Loss: 2.3075\n",
            "Epoch [4/100], Step [600/2534], Loss: 2.6473\n",
            "Epoch [4/100], Step [700/2534], Loss: 2.5901\n",
            "Epoch [4/100], Step [800/2534], Loss: 2.3800\n",
            "Epoch [4/100], Step [900/2534], Loss: 2.6686\n",
            "Epoch [4/100], Step [1000/2534], Loss: 2.6488\n",
            "Epoch [4/100], Step [1100/2534], Loss: 2.5214\n",
            "Epoch [4/100], Step [1200/2534], Loss: 2.3584\n",
            "Epoch [4/100], Step [1300/2534], Loss: 2.8304\n",
            "Epoch [4/100], Step [1400/2534], Loss: 2.8235\n",
            "Epoch [4/100], Step [1500/2534], Loss: 2.5694\n",
            "Epoch [4/100], Step [1600/2534], Loss: 2.8031\n",
            "Epoch [4/100], Step [1700/2534], Loss: 2.9412\n",
            "Epoch [4/100], Step [1800/2534], Loss: 2.8389\n",
            "Epoch [4/100], Step [1900/2534], Loss: 2.9931\n",
            "Epoch [4/100], Step [2000/2534], Loss: 2.5194\n",
            "Epoch [4/100], Step [2100/2534], Loss: 2.8997\n",
            "Epoch [4/100], Step [2200/2534], Loss: 2.8534\n",
            "Epoch [4/100], Step [2300/2534], Loss: 2.7163\n",
            "Epoch [4/100], Step [2400/2534], Loss: 2.9101\n",
            "Epoch [4/100], Step [2500/2534], Loss: 2.6329\n",
            "Epoch [4/100] Average Training Loss: 2.6904\n",
            "Epoch [5/100], Step [100/2534], Loss: 1.7794\n",
            "Epoch [5/100], Step [200/2534], Loss: 2.1961\n",
            "Epoch [5/100], Step [300/2534], Loss: 2.3267\n",
            "Epoch [5/100], Step [400/2534], Loss: 1.6172\n",
            "Epoch [5/100], Step [500/2534], Loss: 2.3248\n",
            "Epoch [5/100], Step [600/2534], Loss: 2.3626\n",
            "Epoch [5/100], Step [700/2534], Loss: 2.4108\n",
            "Epoch [5/100], Step [800/2534], Loss: 2.3251\n",
            "Epoch [5/100], Step [900/2534], Loss: 2.4313\n",
            "Epoch [5/100], Step [1000/2534], Loss: 2.0835\n",
            "Epoch [5/100], Step [1100/2534], Loss: 2.1668\n",
            "Epoch [5/100], Step [1200/2534], Loss: 2.3058\n",
            "Epoch [5/100], Step [1300/2534], Loss: 2.2267\n",
            "Epoch [5/100], Step [1400/2534], Loss: 2.2669\n",
            "Epoch [5/100], Step [1500/2534], Loss: 2.4824\n",
            "Epoch [5/100], Step [1600/2534], Loss: 2.4506\n",
            "Epoch [5/100], Step [1700/2534], Loss: 2.6917\n",
            "Epoch [5/100], Step [1800/2534], Loss: 2.3188\n",
            "Epoch [5/100], Step [1900/2534], Loss: 2.1606\n",
            "Epoch [5/100], Step [2000/2534], Loss: 2.2520\n",
            "Epoch [5/100], Step [2100/2534], Loss: 2.6697\n",
            "Epoch [5/100], Step [2200/2534], Loss: 2.5303\n",
            "Epoch [5/100], Step [2300/2534], Loss: 2.2678\n",
            "Epoch [5/100], Step [2400/2534], Loss: 1.8619\n",
            "Epoch [5/100], Step [2500/2534], Loss: 2.8791\n",
            "Epoch [5/100] Average Training Loss: 2.2208\n",
            "Epoch [6/100], Step [100/2534], Loss: 1.3923\n",
            "Epoch [6/100], Step [200/2534], Loss: 1.3217\n",
            "Epoch [6/100], Step [300/2534], Loss: 1.8887\n",
            "Epoch [6/100], Step [400/2534], Loss: 1.5899\n",
            "Epoch [6/100], Step [500/2534], Loss: 1.4196\n",
            "Epoch [6/100], Step [600/2534], Loss: 1.5318\n",
            "Epoch [6/100], Step [700/2534], Loss: 1.7357\n",
            "Epoch [6/100], Step [800/2534], Loss: 1.7321\n",
            "Epoch [6/100], Step [900/2534], Loss: 1.8293\n",
            "Epoch [6/100], Step [1000/2534], Loss: 1.7959\n",
            "Epoch [6/100], Step [1100/2534], Loss: 1.8754\n",
            "Epoch [6/100], Step [1200/2534], Loss: 1.9619\n",
            "Epoch [6/100], Step [1300/2534], Loss: 2.1334\n",
            "Epoch [6/100], Step [1400/2534], Loss: 1.6850\n",
            "Epoch [6/100], Step [1500/2534], Loss: 1.7825\n",
            "Epoch [6/100], Step [1600/2534], Loss: 1.4913\n",
            "Epoch [6/100], Step [1700/2534], Loss: 2.2324\n",
            "Epoch [6/100], Step [1800/2534], Loss: 1.8001\n",
            "Epoch [6/100], Step [1900/2534], Loss: 1.9076\n",
            "Epoch [6/100], Step [2000/2534], Loss: 2.1328\n",
            "Epoch [6/100], Step [2100/2534], Loss: 2.3516\n",
            "Epoch [6/100], Step [2200/2534], Loss: 2.1406\n",
            "Epoch [6/100], Step [2300/2534], Loss: 1.7271\n",
            "Epoch [6/100], Step [2400/2534], Loss: 2.2197\n",
            "Epoch [6/100], Step [2500/2534], Loss: 2.2972\n",
            "Epoch [6/100] Average Training Loss: 1.8434\n",
            "Epoch [7/100], Step [100/2534], Loss: 1.1350\n",
            "Epoch [7/100], Step [200/2534], Loss: 0.9521\n",
            "Epoch [7/100], Step [300/2534], Loss: 1.6077\n",
            "Epoch [7/100], Step [400/2534], Loss: 1.5369\n",
            "Epoch [7/100], Step [500/2534], Loss: 1.6607\n",
            "Epoch [7/100], Step [600/2534], Loss: 1.3058\n",
            "Epoch [7/100], Step [700/2534], Loss: 1.8045\n",
            "Epoch [7/100], Step [800/2534], Loss: 1.9346\n",
            "Epoch [7/100], Step [900/2534], Loss: 1.3799\n",
            "Epoch [7/100], Step [1000/2534], Loss: 1.6938\n",
            "Epoch [7/100], Step [1100/2534], Loss: 1.8865\n",
            "Epoch [7/100], Step [1200/2534], Loss: 1.3598\n",
            "Epoch [7/100], Step [1300/2534], Loss: 1.9914\n",
            "Epoch [7/100], Step [1400/2534], Loss: 1.6612\n",
            "Epoch [7/100], Step [1500/2534], Loss: 1.7879\n",
            "Epoch [7/100], Step [1600/2534], Loss: 1.5237\n",
            "Epoch [7/100], Step [1700/2534], Loss: 1.9122\n",
            "Epoch [7/100], Step [1800/2534], Loss: 1.8402\n",
            "Epoch [7/100], Step [1900/2534], Loss: 1.7570\n",
            "Epoch [7/100], Step [2000/2534], Loss: 1.7836\n",
            "Epoch [7/100], Step [2100/2534], Loss: 1.7637\n",
            "Epoch [7/100], Step [2200/2534], Loss: 2.3732\n",
            "Epoch [7/100], Step [2300/2534], Loss: 1.3433\n",
            "Epoch [7/100], Step [2400/2534], Loss: 1.9157\n",
            "Epoch [7/100], Step [2500/2534], Loss: 1.7468\n",
            "Epoch [7/100] Average Training Loss: 1.5499\n",
            "Epoch [8/100], Step [100/2534], Loss: 0.8598\n",
            "Epoch [8/100], Step [200/2534], Loss: 1.2518\n",
            "Epoch [8/100], Step [300/2534], Loss: 1.2213\n",
            "Epoch [8/100], Step [400/2534], Loss: 1.0843\n",
            "Epoch [8/100], Step [500/2534], Loss: 1.3097\n",
            "Epoch [8/100], Step [600/2534], Loss: 1.2112\n",
            "Epoch [8/100], Step [700/2534], Loss: 1.2502\n",
            "Epoch [8/100], Step [800/2534], Loss: 1.3418\n",
            "Epoch [8/100], Step [900/2534], Loss: 0.9106\n",
            "Epoch [8/100], Step [1000/2534], Loss: 1.3076\n",
            "Epoch [8/100], Step [1100/2534], Loss: 1.4854\n",
            "Epoch [8/100], Step [1200/2534], Loss: 1.2503\n",
            "Epoch [8/100], Step [1300/2534], Loss: 1.1417\n",
            "Epoch [8/100], Step [1400/2534], Loss: 1.7432\n",
            "Epoch [8/100], Step [1500/2534], Loss: 1.5202\n",
            "Epoch [8/100], Step [1600/2534], Loss: 1.2599\n",
            "Epoch [8/100], Step [1700/2534], Loss: 1.5750\n",
            "Epoch [8/100], Step [1800/2534], Loss: 1.1878\n",
            "Epoch [8/100], Step [1900/2534], Loss: 1.7648\n",
            "Epoch [8/100], Step [2000/2534], Loss: 1.4062\n",
            "Epoch [8/100], Step [2100/2534], Loss: 1.3961\n",
            "Epoch [8/100], Step [2200/2534], Loss: 1.4705\n",
            "Epoch [8/100], Step [2300/2534], Loss: 1.7674\n",
            "Epoch [8/100], Step [2400/2534], Loss: 1.8601\n",
            "Epoch [8/100], Step [2500/2534], Loss: 1.5517\n",
            "Epoch [8/100] Average Training Loss: 1.3228\n",
            "Epoch [9/100], Step [100/2534], Loss: 1.0774\n",
            "Epoch [9/100], Step [200/2534], Loss: 0.7289\n",
            "Epoch [9/100], Step [300/2534], Loss: 0.9147\n",
            "Epoch [9/100], Step [400/2534], Loss: 0.8047\n",
            "Epoch [9/100], Step [500/2534], Loss: 1.0514\n",
            "Epoch [9/100], Step [600/2534], Loss: 0.9425\n",
            "Epoch [9/100], Step [700/2534], Loss: 0.7270\n",
            "Epoch [9/100], Step [800/2534], Loss: 1.0265\n",
            "Epoch [9/100], Step [900/2534], Loss: 1.1455\n",
            "Epoch [9/100], Step [1000/2534], Loss: 0.8545\n",
            "Epoch [9/100], Step [1100/2534], Loss: 0.9191\n",
            "Epoch [9/100], Step [1200/2534], Loss: 1.0638\n",
            "Epoch [9/100], Step [1300/2534], Loss: 1.7936\n",
            "Epoch [9/100], Step [1400/2534], Loss: 1.2875\n",
            "Epoch [9/100], Step [1500/2534], Loss: 1.2858\n",
            "Epoch [9/100], Step [1600/2534], Loss: 1.0986\n",
            "Epoch [9/100], Step [1700/2534], Loss: 1.2573\n",
            "Epoch [9/100], Step [1800/2534], Loss: 1.2539\n",
            "Epoch [9/100], Step [1900/2534], Loss: 1.3040\n",
            "Epoch [9/100], Step [2000/2534], Loss: 1.2635\n",
            "Epoch [9/100], Step [2100/2534], Loss: 1.4621\n",
            "Epoch [9/100], Step [2200/2534], Loss: 1.3026\n",
            "Epoch [9/100], Step [2300/2534], Loss: 1.0718\n",
            "Epoch [9/100], Step [2400/2534], Loss: 1.6268\n",
            "Epoch [9/100], Step [2500/2534], Loss: 1.4300\n",
            "Epoch [9/100] Average Training Loss: 1.1483\n",
            "Epoch [10/100], Step [100/2534], Loss: 1.1046\n",
            "Epoch [10/100], Step [200/2534], Loss: 0.4773\n",
            "Epoch [10/100], Step [300/2534], Loss: 0.6773\n",
            "Epoch [10/100], Step [400/2534], Loss: 1.1637\n",
            "Epoch [10/100], Step [500/2534], Loss: 1.1669\n",
            "Epoch [10/100], Step [600/2534], Loss: 1.0096\n",
            "Epoch [10/100], Step [700/2534], Loss: 0.8901\n",
            "Epoch [10/100], Step [800/2534], Loss: 0.9577\n",
            "Epoch [10/100], Step [900/2534], Loss: 0.6063\n",
            "Epoch [10/100], Step [1000/2534], Loss: 0.8548\n",
            "Epoch [10/100], Step [1100/2534], Loss: 1.0644\n",
            "Epoch [10/100], Step [1200/2534], Loss: 0.9013\n",
            "Epoch [10/100], Step [1300/2534], Loss: 0.7078\n",
            "Epoch [10/100], Step [1400/2534], Loss: 1.2240\n",
            "Epoch [10/100], Step [1500/2534], Loss: 1.1853\n",
            "Epoch [10/100], Step [1600/2534], Loss: 1.2698\n",
            "Epoch [10/100], Step [1700/2534], Loss: 1.3404\n",
            "Epoch [10/100], Step [1800/2534], Loss: 1.0292\n",
            "Epoch [10/100], Step [1900/2534], Loss: 1.4185\n",
            "Epoch [10/100], Step [2000/2534], Loss: 1.0794\n",
            "Epoch [10/100], Step [2100/2534], Loss: 1.0135\n",
            "Epoch [10/100], Step [2200/2534], Loss: 1.3751\n",
            "Epoch [10/100], Step [2300/2534], Loss: 1.1070\n",
            "Epoch [10/100], Step [2400/2534], Loss: 1.2394\n",
            "Epoch [10/100], Step [2500/2534], Loss: 1.0874\n",
            "Epoch [10/100] Average Training Loss: 1.0112\n",
            "Epoch [11/100], Step [100/2534], Loss: 0.8026\n",
            "Epoch [11/100], Step [200/2534], Loss: 0.4509\n",
            "Epoch [11/100], Step [300/2534], Loss: 0.7396\n",
            "Epoch [11/100], Step [400/2534], Loss: 0.5744\n",
            "Epoch [11/100], Step [500/2534], Loss: 0.7846\n",
            "Epoch [11/100], Step [600/2534], Loss: 0.6393\n",
            "Epoch [11/100], Step [700/2534], Loss: 0.5909\n",
            "Epoch [11/100], Step [800/2534], Loss: 0.7726\n",
            "Epoch [11/100], Step [900/2534], Loss: 0.9347\n",
            "Epoch [11/100], Step [1000/2534], Loss: 0.9549\n",
            "Epoch [11/100], Step [1100/2534], Loss: 0.9197\n",
            "Epoch [11/100], Step [1200/2534], Loss: 0.9767\n",
            "Epoch [11/100], Step [1300/2534], Loss: 0.5431\n",
            "Epoch [11/100], Step [1400/2534], Loss: 1.1481\n",
            "Epoch [11/100], Step [1500/2534], Loss: 0.8626\n",
            "Epoch [11/100], Step [1600/2534], Loss: 0.8792\n",
            "Epoch [11/100], Step [1700/2534], Loss: 1.1489\n",
            "Epoch [11/100], Step [1800/2534], Loss: 0.8102\n",
            "Epoch [11/100], Step [1900/2534], Loss: 1.0798\n",
            "Epoch [11/100], Step [2000/2534], Loss: 0.7836\n",
            "Epoch [11/100], Step [2100/2534], Loss: 1.2756\n",
            "Epoch [11/100], Step [2200/2534], Loss: 0.9134\n",
            "Epoch [11/100], Step [2300/2534], Loss: 0.9891\n",
            "Epoch [11/100], Step [2400/2534], Loss: 1.0838\n",
            "Epoch [11/100], Step [2500/2534], Loss: 1.3361\n",
            "Epoch [11/100] Average Training Loss: 0.9056\n",
            "Epoch [12/100], Step [100/2534], Loss: 0.4855\n",
            "Epoch [12/100], Step [200/2534], Loss: 1.1314\n",
            "Epoch [12/100], Step [300/2534], Loss: 0.7545\n",
            "Epoch [12/100], Step [400/2534], Loss: 0.7173\n",
            "Epoch [12/100], Step [500/2534], Loss: 0.6631\n",
            "Epoch [12/100], Step [600/2534], Loss: 0.8238\n",
            "Epoch [12/100], Step [700/2534], Loss: 0.6758\n",
            "Epoch [12/100], Step [800/2534], Loss: 0.4348\n",
            "Epoch [12/100], Step [900/2534], Loss: 0.8898\n",
            "Epoch [12/100], Step [1000/2534], Loss: 0.7248\n",
            "Epoch [12/100], Step [1100/2534], Loss: 0.9163\n",
            "Epoch [12/100], Step [1200/2534], Loss: 1.0015\n",
            "Epoch [12/100], Step [1300/2534], Loss: 0.7217\n",
            "Epoch [12/100], Step [1400/2534], Loss: 1.0793\n",
            "Epoch [12/100], Step [1500/2534], Loss: 0.9891\n",
            "Epoch [12/100], Step [1600/2534], Loss: 1.2696\n",
            "Epoch [12/100], Step [1700/2534], Loss: 0.6108\n",
            "Epoch [12/100], Step [1800/2534], Loss: 0.6289\n",
            "Epoch [12/100], Step [1900/2534], Loss: 1.0799\n",
            "Epoch [12/100], Step [2000/2534], Loss: 0.6785\n",
            "Epoch [12/100], Step [2100/2534], Loss: 1.0076\n",
            "Epoch [12/100], Step [2200/2534], Loss: 0.7961\n",
            "Epoch [12/100], Step [2300/2534], Loss: 1.1194\n",
            "Epoch [12/100], Step [2400/2534], Loss: 1.1077\n",
            "Epoch [12/100], Step [2500/2534], Loss: 1.0540\n",
            "Epoch [12/100] Average Training Loss: 0.8193\n",
            "Epoch [13/100], Step [100/2534], Loss: 0.7852\n",
            "Epoch [13/100], Step [200/2534], Loss: 0.5105\n",
            "Epoch [13/100], Step [300/2534], Loss: 0.5094\n",
            "Epoch [13/100], Step [400/2534], Loss: 0.3609\n",
            "Epoch [13/100], Step [500/2534], Loss: 0.6634\n",
            "Epoch [13/100], Step [600/2534], Loss: 0.5999\n",
            "Epoch [13/100], Step [700/2534], Loss: 0.8616\n",
            "Epoch [13/100], Step [800/2534], Loss: 0.7308\n",
            "Epoch [13/100], Step [900/2534], Loss: 0.7529\n",
            "Epoch [13/100], Step [1000/2534], Loss: 0.6635\n",
            "Epoch [13/100], Step [1100/2534], Loss: 0.6179\n",
            "Epoch [13/100], Step [1200/2534], Loss: 0.9019\n",
            "Epoch [13/100], Step [1300/2534], Loss: 0.7477\n",
            "Epoch [13/100], Step [1400/2534], Loss: 1.0261\n",
            "Epoch [13/100], Step [1500/2534], Loss: 0.9166\n",
            "Epoch [13/100], Step [1600/2534], Loss: 0.7874\n",
            "Epoch [13/100], Step [1700/2534], Loss: 0.7582\n",
            "Epoch [13/100], Step [1800/2534], Loss: 0.9116\n",
            "Epoch [13/100], Step [1900/2534], Loss: 0.5862\n",
            "Epoch [13/100], Step [2000/2534], Loss: 0.8847\n",
            "Epoch [13/100], Step [2100/2534], Loss: 0.6052\n",
            "Epoch [13/100], Step [2200/2534], Loss: 1.0387\n",
            "Epoch [13/100], Step [2300/2534], Loss: 1.1140\n",
            "Epoch [13/100], Step [2400/2534], Loss: 0.8443\n",
            "Epoch [13/100], Step [2500/2534], Loss: 0.6014\n",
            "Epoch [13/100] Average Training Loss: 0.7499\n",
            "Epoch [14/100], Step [100/2534], Loss: 0.6742\n",
            "Epoch [14/100], Step [200/2534], Loss: 0.5806\n",
            "Epoch [14/100], Step [300/2534], Loss: 0.5500\n",
            "Epoch [14/100], Step [400/2534], Loss: 0.5332\n",
            "Epoch [14/100], Step [500/2534], Loss: 0.7259\n",
            "Epoch [14/100], Step [600/2534], Loss: 0.4339\n",
            "Epoch [14/100], Step [700/2534], Loss: 0.5351\n",
            "Epoch [14/100], Step [800/2534], Loss: 0.7507\n",
            "Epoch [14/100], Step [900/2534], Loss: 0.4681\n",
            "Epoch [14/100], Step [1000/2534], Loss: 0.6398\n",
            "Epoch [14/100], Step [1100/2534], Loss: 0.6087\n",
            "Epoch [14/100], Step [1200/2534], Loss: 0.8054\n",
            "Epoch [14/100], Step [1300/2534], Loss: 0.5258\n",
            "Epoch [14/100], Step [1400/2534], Loss: 0.6461\n",
            "Epoch [14/100], Step [1500/2534], Loss: 0.4237\n",
            "Epoch [14/100], Step [1600/2534], Loss: 0.5063\n",
            "Epoch [14/100], Step [1700/2534], Loss: 0.9674\n",
            "Epoch [14/100], Step [1800/2534], Loss: 0.8030\n",
            "Epoch [14/100], Step [1900/2534], Loss: 0.6129\n",
            "Epoch [14/100], Step [2000/2534], Loss: 0.7061\n",
            "Epoch [14/100], Step [2100/2534], Loss: 0.5488\n",
            "Epoch [14/100], Step [2200/2534], Loss: 0.6328\n",
            "Epoch [14/100], Step [2300/2534], Loss: 0.7922\n",
            "Epoch [14/100], Step [2400/2534], Loss: 0.6108\n",
            "Epoch [14/100], Step [2500/2534], Loss: 0.8610\n",
            "Epoch [14/100] Average Training Loss: 0.6927\n",
            "Epoch [15/100], Step [100/2534], Loss: 0.7028\n",
            "Epoch [15/100], Step [200/2534], Loss: 0.3860\n",
            "Epoch [15/100], Step [300/2534], Loss: 0.5206\n",
            "Epoch [15/100], Step [400/2534], Loss: 0.6455\n",
            "Epoch [15/100], Step [500/2534], Loss: 0.8378\n",
            "Epoch [15/100], Step [600/2534], Loss: 0.6465\n",
            "Epoch [15/100], Step [700/2534], Loss: 0.5542\n",
            "Epoch [15/100], Step [800/2534], Loss: 0.3095\n",
            "Epoch [15/100], Step [900/2534], Loss: 0.4115\n",
            "Epoch [15/100], Step [1000/2534], Loss: 0.8245\n",
            "Epoch [15/100], Step [1100/2534], Loss: 0.4470\n",
            "Epoch [15/100], Step [1200/2534], Loss: 0.4789\n",
            "Epoch [15/100], Step [1300/2534], Loss: 0.7247\n",
            "Epoch [15/100], Step [1400/2534], Loss: 0.8681\n",
            "Epoch [15/100], Step [1500/2534], Loss: 0.8233\n",
            "Epoch [15/100], Step [1600/2534], Loss: 0.3802\n",
            "Epoch [15/100], Step [1700/2534], Loss: 0.8533\n",
            "Epoch [15/100], Step [1800/2534], Loss: 0.9304\n",
            "Epoch [15/100], Step [1900/2534], Loss: 0.7664\n",
            "Epoch [15/100], Step [2000/2534], Loss: 0.4953\n",
            "Epoch [15/100], Step [2100/2534], Loss: 0.8222\n",
            "Epoch [15/100], Step [2200/2534], Loss: 0.7357\n",
            "Epoch [15/100], Step [2300/2534], Loss: 0.5713\n",
            "Epoch [15/100], Step [2400/2534], Loss: 0.7473\n",
            "Epoch [15/100], Step [2500/2534], Loss: 0.7931\n",
            "Epoch [15/100] Average Training Loss: 0.6457\n",
            "Epoch [16/100], Step [100/2534], Loss: 0.4532\n",
            "Epoch [16/100], Step [200/2534], Loss: 0.6185\n",
            "Epoch [16/100], Step [300/2534], Loss: 0.7434\n",
            "Epoch [16/100], Step [400/2534], Loss: 0.3864\n",
            "Epoch [16/100], Step [500/2534], Loss: 0.6466\n",
            "Epoch [16/100], Step [600/2534], Loss: 0.7901\n",
            "Epoch [16/100], Step [700/2534], Loss: 0.6860\n",
            "Epoch [16/100], Step [800/2534], Loss: 0.5212\n",
            "Epoch [16/100], Step [900/2534], Loss: 0.6625\n",
            "Epoch [16/100], Step [1000/2534], Loss: 0.6501\n",
            "Epoch [16/100], Step [1100/2534], Loss: 0.6658\n",
            "Epoch [16/100], Step [1200/2534], Loss: 0.5323\n",
            "Epoch [16/100], Step [1300/2534], Loss: 0.5850\n",
            "Epoch [16/100], Step [1400/2534], Loss: 0.8441\n",
            "Epoch [16/100], Step [1500/2534], Loss: 0.6416\n",
            "Epoch [16/100], Step [1600/2534], Loss: 0.5788\n",
            "Epoch [16/100], Step [1700/2534], Loss: 0.6153\n",
            "Epoch [16/100], Step [1800/2534], Loss: 0.6495\n",
            "Epoch [16/100], Step [1900/2534], Loss: 0.6189\n",
            "Epoch [16/100], Step [2000/2534], Loss: 0.8184\n",
            "Epoch [16/100], Step [2100/2534], Loss: 0.7649\n",
            "Epoch [16/100], Step [2200/2534], Loss: 0.9543\n",
            "Epoch [16/100], Step [2300/2534], Loss: 0.6423\n",
            "Epoch [16/100], Step [2400/2534], Loss: 0.8125\n",
            "Epoch [16/100], Step [2500/2534], Loss: 0.5097\n",
            "Epoch [16/100] Average Training Loss: 0.6090\n",
            "Epoch [17/100], Step [100/2534], Loss: 0.3453\n",
            "Epoch [17/100], Step [200/2534], Loss: 0.5327\n",
            "Epoch [17/100], Step [300/2534], Loss: 0.6022\n",
            "Epoch [17/100], Step [400/2534], Loss: 0.4496\n",
            "Epoch [17/100], Step [500/2534], Loss: 0.4824\n",
            "Epoch [17/100], Step [600/2534], Loss: 0.6623\n",
            "Epoch [17/100], Step [700/2534], Loss: 0.3536\n",
            "Epoch [17/100], Step [800/2534], Loss: 0.2920\n",
            "Epoch [17/100], Step [900/2534], Loss: 0.4389\n",
            "Epoch [17/100], Step [1000/2534], Loss: 0.3018\n",
            "Epoch [17/100], Step [1100/2534], Loss: 0.4741\n",
            "Epoch [17/100], Step [1200/2534], Loss: 0.5948\n",
            "Epoch [17/100], Step [1300/2534], Loss: 0.6708\n",
            "Epoch [17/100], Step [1400/2534], Loss: 0.4562\n",
            "Epoch [17/100], Step [1500/2534], Loss: 0.5393\n",
            "Epoch [17/100], Step [1600/2534], Loss: 0.5980\n",
            "Epoch [17/100], Step [1700/2534], Loss: 0.6879\n",
            "Epoch [17/100], Step [1800/2534], Loss: 0.6703\n",
            "Epoch [17/100], Step [1900/2534], Loss: 0.7373\n",
            "Epoch [17/100], Step [2000/2534], Loss: 0.4908\n",
            "Epoch [17/100], Step [2100/2534], Loss: 0.8012\n",
            "Epoch [17/100], Step [2200/2534], Loss: 0.7536\n",
            "Epoch [17/100], Step [2300/2534], Loss: 0.7155\n",
            "Epoch [17/100], Step [2400/2534], Loss: 0.4279\n",
            "Epoch [17/100], Step [2500/2534], Loss: 0.7448\n",
            "Epoch [17/100] Average Training Loss: 0.5741\n",
            "Epoch [18/100], Step [100/2534], Loss: 0.4924\n",
            "Epoch [18/100], Step [200/2534], Loss: 0.6623\n",
            "Epoch [18/100], Step [300/2534], Loss: 0.3063\n",
            "Epoch [18/100], Step [400/2534], Loss: 0.6733\n",
            "Epoch [18/100], Step [500/2534], Loss: 0.4953\n",
            "Epoch [18/100], Step [600/2534], Loss: 0.5577\n",
            "Epoch [18/100], Step [700/2534], Loss: 0.6568\n",
            "Epoch [18/100], Step [800/2534], Loss: 0.4183\n",
            "Epoch [18/100], Step [900/2534], Loss: 0.4602\n",
            "Epoch [18/100], Step [1000/2534], Loss: 0.7375\n",
            "Epoch [18/100], Step [1100/2534], Loss: 0.7842\n",
            "Epoch [18/100], Step [1200/2534], Loss: 0.4938\n",
            "Epoch [18/100], Step [1300/2534], Loss: 0.5363\n",
            "Epoch [18/100], Step [1400/2534], Loss: 0.6199\n",
            "Epoch [18/100], Step [1500/2534], Loss: 0.3899\n",
            "Epoch [18/100], Step [1600/2534], Loss: 0.7728\n",
            "Epoch [18/100], Step [1700/2534], Loss: 0.5212\n",
            "Epoch [18/100], Step [1800/2534], Loss: 0.4776\n",
            "Epoch [18/100], Step [1900/2534], Loss: 0.6437\n",
            "Epoch [18/100], Step [2000/2534], Loss: 0.4006\n",
            "Epoch [18/100], Step [2100/2534], Loss: 0.6837\n",
            "Epoch [18/100], Step [2200/2534], Loss: 0.4718\n",
            "Epoch [18/100], Step [2300/2534], Loss: 0.6203\n",
            "Epoch [18/100], Step [2400/2534], Loss: 0.5972\n",
            "Epoch [18/100], Step [2500/2534], Loss: 0.3943\n",
            "Epoch [18/100] Average Training Loss: 0.5446\n",
            "Epoch [19/100], Step [100/2534], Loss: 0.4021\n",
            "Epoch [19/100], Step [200/2534], Loss: 0.6690\n",
            "Epoch [19/100], Step [300/2534], Loss: 0.3430\n",
            "Epoch [19/100], Step [400/2534], Loss: 0.4872\n",
            "Epoch [19/100], Step [500/2534], Loss: 0.3615\n",
            "Epoch [19/100], Step [600/2534], Loss: 0.2998\n",
            "Epoch [19/100], Step [700/2534], Loss: 0.4606\n",
            "Epoch [19/100], Step [800/2534], Loss: 0.3580\n",
            "Epoch [19/100], Step [900/2534], Loss: 0.3574\n",
            "Epoch [19/100], Step [1000/2534], Loss: 0.4942\n",
            "Epoch [19/100], Step [1100/2534], Loss: 0.5814\n",
            "Epoch [19/100], Step [1200/2534], Loss: 0.4159\n",
            "Epoch [19/100], Step [1300/2534], Loss: 0.6420\n",
            "Epoch [19/100], Step [1400/2534], Loss: 0.5994\n",
            "Epoch [19/100], Step [1500/2534], Loss: 0.7492\n",
            "Epoch [19/100], Step [1600/2534], Loss: 0.5690\n",
            "Epoch [19/100], Step [1700/2534], Loss: 0.6176\n",
            "Epoch [19/100], Step [1800/2534], Loss: 0.5919\n",
            "Epoch [19/100], Step [1900/2534], Loss: 0.8320\n",
            "Epoch [19/100], Step [2000/2534], Loss: 0.5431\n",
            "Epoch [19/100], Step [2100/2534], Loss: 0.7247\n",
            "Epoch [19/100], Step [2200/2534], Loss: 0.6502\n",
            "Epoch [19/100], Step [2300/2534], Loss: 0.4749\n",
            "Epoch [19/100], Step [2400/2534], Loss: 0.6814\n",
            "Epoch [19/100], Step [2500/2534], Loss: 0.5941\n",
            "Epoch [19/100] Average Training Loss: 0.5210\n",
            "Epoch [20/100], Step [100/2534], Loss: 0.4066\n",
            "Epoch [20/100], Step [200/2534], Loss: 0.7187\n",
            "Epoch [20/100], Step [300/2534], Loss: 0.5119\n",
            "Epoch [20/100], Step [400/2534], Loss: 0.6561\n",
            "Epoch [20/100], Step [500/2534], Loss: 0.3234\n",
            "Epoch [20/100], Step [600/2534], Loss: 0.5953\n",
            "Epoch [20/100], Step [700/2534], Loss: 0.7658\n",
            "Epoch [20/100], Step [800/2534], Loss: 0.2908\n",
            "Epoch [20/100], Step [900/2534], Loss: 0.2652\n",
            "Epoch [20/100], Step [1000/2534], Loss: 0.5243\n",
            "Epoch [20/100], Step [1100/2534], Loss: 0.3572\n",
            "Epoch [20/100], Step [1200/2534], Loss: 0.4510\n",
            "Epoch [20/100], Step [1300/2534], Loss: 0.4262\n",
            "Epoch [20/100], Step [1400/2534], Loss: 0.4165\n",
            "Epoch [20/100], Step [1500/2534], Loss: 0.4593\n",
            "Epoch [20/100], Step [1600/2534], Loss: 0.5778\n",
            "Epoch [20/100], Step [1700/2534], Loss: 0.7240\n",
            "Epoch [20/100], Step [1800/2534], Loss: 0.4449\n",
            "Epoch [20/100], Step [1900/2534], Loss: 0.4366\n",
            "Epoch [20/100], Step [2000/2534], Loss: 0.5358\n",
            "Epoch [20/100], Step [2100/2534], Loss: 0.5169\n",
            "Epoch [20/100], Step [2200/2534], Loss: 0.5562\n",
            "Epoch [20/100], Step [2300/2534], Loss: 0.8654\n",
            "Epoch [20/100], Step [2400/2534], Loss: 0.3393\n",
            "Epoch [20/100], Step [2500/2534], Loss: 0.3313\n",
            "Epoch [20/100] Average Training Loss: 0.4984\n",
            "Epoch [21/100], Step [100/2534], Loss: 0.2658\n",
            "Epoch [21/100], Step [200/2534], Loss: 0.2193\n",
            "Epoch [21/100], Step [300/2534], Loss: 0.4702\n",
            "Epoch [21/100], Step [400/2534], Loss: 0.4297\n",
            "Epoch [21/100], Step [500/2534], Loss: 0.4605\n",
            "Epoch [21/100], Step [600/2534], Loss: 0.4595\n",
            "Epoch [21/100], Step [700/2534], Loss: 0.4840\n",
            "Epoch [21/100], Step [800/2534], Loss: 0.5187\n",
            "Epoch [21/100], Step [900/2534], Loss: 0.4114\n",
            "Epoch [21/100], Step [1000/2534], Loss: 0.4051\n",
            "Epoch [21/100], Step [1100/2534], Loss: 0.4277\n",
            "Epoch [21/100], Step [1200/2534], Loss: 0.5911\n",
            "Epoch [21/100], Step [1300/2534], Loss: 0.4665\n",
            "Epoch [21/100], Step [1400/2534], Loss: 0.4623\n",
            "Epoch [21/100], Step [1500/2534], Loss: 0.4835\n",
            "Epoch [21/100], Step [1600/2534], Loss: 0.4382\n",
            "Epoch [21/100], Step [1700/2534], Loss: 0.6285\n",
            "Epoch [21/100], Step [1800/2534], Loss: 0.6574\n",
            "Epoch [21/100], Step [1900/2534], Loss: 0.3766\n",
            "Epoch [21/100], Step [2000/2534], Loss: 0.3652\n",
            "Epoch [21/100], Step [2100/2534], Loss: 0.8764\n",
            "Epoch [21/100], Step [2200/2534], Loss: 0.5016\n",
            "Epoch [21/100], Step [2300/2534], Loss: 0.5872\n",
            "Epoch [21/100], Step [2400/2534], Loss: 0.6082\n",
            "Epoch [21/100], Step [2500/2534], Loss: 0.5342\n",
            "Epoch [21/100] Average Training Loss: 0.4782\n",
            "Epoch [22/100], Step [100/2534], Loss: 0.4811\n",
            "Epoch [22/100], Step [200/2534], Loss: 0.3765\n",
            "Epoch [22/100], Step [300/2534], Loss: 0.4246\n",
            "Epoch [22/100], Step [400/2534], Loss: 0.4139\n",
            "Epoch [22/100], Step [500/2534], Loss: 0.3077\n",
            "Epoch [22/100], Step [600/2534], Loss: 0.1142\n",
            "Epoch [22/100], Step [700/2534], Loss: 0.5510\n",
            "Epoch [22/100], Step [800/2534], Loss: 0.3511\n",
            "Epoch [22/100], Step [900/2534], Loss: 0.5416\n",
            "Epoch [22/100], Step [1000/2534], Loss: 0.5136\n",
            "Epoch [22/100], Step [1100/2534], Loss: 0.2587\n",
            "Epoch [22/100], Step [1200/2534], Loss: 0.4102\n",
            "Epoch [22/100], Step [1300/2534], Loss: 0.3947\n",
            "Epoch [22/100], Step [1400/2534], Loss: 0.4632\n",
            "Epoch [22/100], Step [1500/2534], Loss: 0.4161\n",
            "Epoch [22/100], Step [1600/2534], Loss: 0.4602\n",
            "Epoch [22/100], Step [1700/2534], Loss: 0.3181\n",
            "Epoch [22/100], Step [1800/2534], Loss: 0.4979\n",
            "Epoch [22/100], Step [1900/2534], Loss: 0.2582\n",
            "Epoch [22/100], Step [2000/2534], Loss: 0.5012\n",
            "Epoch [22/100], Step [2100/2534], Loss: 0.7044\n",
            "Epoch [22/100], Step [2200/2534], Loss: 0.4321\n",
            "Epoch [22/100], Step [2300/2534], Loss: 0.5778\n",
            "Epoch [22/100], Step [2400/2534], Loss: 0.6258\n",
            "Epoch [22/100], Step [2500/2534], Loss: 0.3521\n",
            "Epoch [22/100] Average Training Loss: 0.4648\n",
            "Epoch [23/100], Step [100/2534], Loss: 0.2689\n",
            "Epoch [23/100], Step [200/2534], Loss: 0.2280\n",
            "Epoch [23/100], Step [300/2534], Loss: 0.4338\n",
            "Epoch [23/100], Step [400/2534], Loss: 0.2214\n",
            "Epoch [23/100], Step [500/2534], Loss: 0.2425\n",
            "Epoch [23/100], Step [600/2534], Loss: 0.4363\n",
            "Epoch [23/100], Step [700/2534], Loss: 0.3250\n",
            "Epoch [23/100], Step [800/2534], Loss: 0.4869\n",
            "Epoch [23/100], Step [900/2534], Loss: 0.4755\n",
            "Epoch [23/100], Step [1000/2534], Loss: 0.3159\n",
            "Epoch [23/100], Step [1100/2534], Loss: 0.5107\n",
            "Epoch [23/100], Step [1200/2534], Loss: 0.2916\n",
            "Epoch [23/100], Step [1300/2534], Loss: 0.4106\n",
            "Epoch [23/100], Step [1400/2534], Loss: 0.5307\n",
            "Epoch [23/100], Step [1500/2534], Loss: 0.3064\n",
            "Epoch [23/100], Step [1600/2534], Loss: 0.2591\n",
            "Epoch [23/100], Step [1700/2534], Loss: 0.5129\n",
            "Epoch [23/100], Step [1800/2534], Loss: 0.3841\n",
            "Epoch [23/100], Step [1900/2534], Loss: 0.4724\n",
            "Epoch [23/100], Step [2000/2534], Loss: 0.4403\n",
            "Epoch [23/100], Step [2100/2534], Loss: 0.9738\n",
            "Epoch [23/100], Step [2200/2534], Loss: 0.4713\n",
            "Epoch [23/100], Step [2300/2534], Loss: 0.5494\n",
            "Epoch [23/100], Step [2400/2534], Loss: 0.4221\n",
            "Epoch [23/100], Step [2500/2534], Loss: 0.5965\n",
            "Epoch [23/100] Average Training Loss: 0.4477\n",
            "Epoch [24/100], Step [100/2534], Loss: 0.4218\n",
            "Epoch [24/100], Step [200/2534], Loss: 0.7352\n",
            "Epoch [24/100], Step [300/2534], Loss: 0.3145\n",
            "Epoch [24/100], Step [400/2534], Loss: 0.3062\n",
            "Epoch [24/100], Step [500/2534], Loss: 0.2825\n",
            "Epoch [24/100], Step [600/2534], Loss: 0.2790\n",
            "Epoch [24/100], Step [700/2534], Loss: 0.2521\n",
            "Epoch [24/100], Step [800/2534], Loss: 0.1542\n",
            "Epoch [24/100], Step [900/2534], Loss: 0.5797\n",
            "Epoch [24/100], Step [1000/2534], Loss: 0.6907\n",
            "Epoch [24/100], Step [1100/2534], Loss: 0.3293\n",
            "Epoch [24/100], Step [1200/2534], Loss: 0.3585\n",
            "Epoch [24/100], Step [1300/2534], Loss: 0.3994\n",
            "Epoch [24/100], Step [1400/2534], Loss: 0.5400\n",
            "Epoch [24/100], Step [1500/2534], Loss: 0.2872\n",
            "Epoch [24/100], Step [1600/2534], Loss: 0.6444\n",
            "Epoch [24/100], Step [1700/2534], Loss: 0.3800\n",
            "Epoch [24/100], Step [1800/2534], Loss: 0.5091\n",
            "Epoch [24/100], Step [1900/2534], Loss: 0.3673\n",
            "Epoch [24/100], Step [2000/2534], Loss: 0.3792\n",
            "Epoch [24/100], Step [2100/2534], Loss: 0.3888\n",
            "Epoch [24/100], Step [2200/2534], Loss: 0.5040\n",
            "Epoch [24/100], Step [2300/2534], Loss: 0.6774\n",
            "Epoch [24/100], Step [2400/2534], Loss: 0.4341\n",
            "Epoch [24/100], Step [2500/2534], Loss: 0.4687\n",
            "Epoch [24/100] Average Training Loss: 0.4311\n",
            "Epoch [25/100], Step [100/2534], Loss: 0.2601\n",
            "Epoch [25/100], Step [200/2534], Loss: 0.2506\n",
            "Epoch [25/100], Step [300/2534], Loss: 0.2833\n",
            "Epoch [25/100], Step [400/2534], Loss: 0.3930\n",
            "Epoch [25/100], Step [500/2534], Loss: 0.4320\n",
            "Epoch [25/100], Step [600/2534], Loss: 0.3017\n",
            "Epoch [25/100], Step [700/2534], Loss: 0.5265\n",
            "Epoch [25/100], Step [800/2534], Loss: 0.3470\n",
            "Epoch [25/100], Step [900/2534], Loss: 0.3115\n",
            "Epoch [25/100], Step [1000/2534], Loss: 0.4294\n",
            "Epoch [25/100], Step [1100/2534], Loss: 0.4708\n",
            "Epoch [25/100], Step [1200/2534], Loss: 0.6410\n",
            "Epoch [25/100], Step [1300/2534], Loss: 0.4587\n",
            "Epoch [25/100], Step [1400/2534], Loss: 0.2198\n",
            "Epoch [25/100], Step [1500/2534], Loss: 0.4986\n",
            "Epoch [25/100], Step [1600/2534], Loss: 0.4181\n",
            "Epoch [25/100], Step [1700/2534], Loss: 0.6824\n",
            "Epoch [25/100], Step [1800/2534], Loss: 0.5700\n",
            "Epoch [25/100], Step [1900/2534], Loss: 0.6645\n",
            "Epoch [25/100], Step [2000/2534], Loss: 0.7827\n",
            "Epoch [25/100], Step [2100/2534], Loss: 0.4358\n",
            "Epoch [25/100], Step [2200/2534], Loss: 0.3954\n",
            "Epoch [25/100], Step [2300/2534], Loss: 0.3796\n",
            "Epoch [25/100], Step [2400/2534], Loss: 0.3238\n",
            "Epoch [25/100], Step [2500/2534], Loss: 0.6173\n",
            "Epoch [25/100] Average Training Loss: 0.4235\n",
            "Epoch [26/100], Step [100/2534], Loss: 0.3007\n",
            "Epoch [26/100], Step [200/2534], Loss: 0.5887\n",
            "Epoch [26/100], Step [300/2534], Loss: 0.3521\n",
            "Epoch [26/100], Step [400/2534], Loss: 0.3054\n",
            "Epoch [26/100], Step [500/2534], Loss: 0.0980\n",
            "Epoch [26/100], Step [600/2534], Loss: 0.3474\n",
            "Epoch [26/100], Step [700/2534], Loss: 0.1608\n",
            "Epoch [26/100], Step [800/2534], Loss: 0.3382\n",
            "Epoch [26/100], Step [900/2534], Loss: 0.3490\n",
            "Epoch [26/100], Step [1000/2534], Loss: 0.4753\n",
            "Epoch [26/100], Step [1100/2534], Loss: 0.4037\n",
            "Epoch [26/100], Step [1200/2534], Loss: 0.6094\n",
            "Epoch [26/100], Step [1300/2534], Loss: 0.2929\n",
            "Epoch [26/100], Step [1400/2534], Loss: 0.4403\n",
            "Epoch [26/100], Step [1500/2534], Loss: 0.3736\n",
            "Epoch [26/100], Step [1600/2534], Loss: 0.1834\n",
            "Epoch [26/100], Step [1700/2534], Loss: 0.4585\n",
            "Epoch [26/100], Step [1800/2534], Loss: 0.2488\n",
            "Epoch [26/100], Step [1900/2534], Loss: 0.5911\n",
            "Epoch [26/100], Step [2000/2534], Loss: 0.3415\n",
            "Epoch [26/100], Step [2100/2534], Loss: 0.3674\n",
            "Epoch [26/100], Step [2200/2534], Loss: 0.5171\n",
            "Epoch [26/100], Step [2300/2534], Loss: 0.3563\n",
            "Epoch [26/100], Step [2400/2534], Loss: 0.5148\n",
            "Epoch [26/100], Step [2500/2534], Loss: 0.4342\n",
            "Epoch [26/100] Average Training Loss: 0.4103\n",
            "Epoch [27/100], Step [100/2534], Loss: 0.1373\n",
            "Epoch [27/100], Step [200/2534], Loss: 0.3011\n",
            "Epoch [27/100], Step [300/2534], Loss: 0.5717\n",
            "Epoch [27/100], Step [400/2534], Loss: 0.3144\n",
            "Epoch [27/100], Step [500/2534], Loss: 0.4372\n",
            "Epoch [27/100], Step [600/2534], Loss: 0.3944\n",
            "Epoch [27/100], Step [700/2534], Loss: 0.4683\n",
            "Epoch [27/100], Step [800/2534], Loss: 0.2364\n",
            "Epoch [27/100], Step [900/2534], Loss: 0.1879\n",
            "Epoch [27/100], Step [1000/2534], Loss: 0.3337\n",
            "Epoch [27/100], Step [1100/2534], Loss: 0.1778\n",
            "Epoch [27/100], Step [1200/2534], Loss: 0.2181\n",
            "Epoch [27/100], Step [1300/2534], Loss: 0.4523\n",
            "Epoch [27/100], Step [1400/2534], Loss: 0.3737\n",
            "Epoch [27/100], Step [1500/2534], Loss: 0.3809\n",
            "Epoch [27/100], Step [1600/2534], Loss: 0.4798\n",
            "Epoch [27/100], Step [1700/2534], Loss: 0.3019\n",
            "Epoch [27/100], Step [1800/2534], Loss: 0.6771\n",
            "Epoch [27/100], Step [1900/2534], Loss: 0.2590\n",
            "Epoch [27/100], Step [2000/2534], Loss: 0.5084\n",
            "Epoch [27/100], Step [2100/2534], Loss: 0.4653\n",
            "Epoch [27/100], Step [2200/2534], Loss: 0.3834\n",
            "Epoch [27/100], Step [2300/2534], Loss: 0.6109\n",
            "Epoch [27/100], Step [2400/2534], Loss: 0.5082\n",
            "Epoch [27/100], Step [2500/2534], Loss: 0.2848\n",
            "Epoch [27/100] Average Training Loss: 0.3999\n",
            "Epoch [28/100], Step [100/2534], Loss: 0.2453\n",
            "Epoch [28/100], Step [200/2534], Loss: 0.1498\n",
            "Epoch [28/100], Step [300/2534], Loss: 0.3371\n",
            "Epoch [28/100], Step [400/2534], Loss: 0.1632\n",
            "Epoch [28/100], Step [500/2534], Loss: 0.3095\n",
            "Epoch [28/100], Step [600/2534], Loss: 0.3047\n",
            "Epoch [28/100], Step [700/2534], Loss: 0.3677\n",
            "Epoch [28/100], Step [800/2534], Loss: 0.5711\n",
            "Epoch [28/100], Step [900/2534], Loss: 0.5456\n",
            "Epoch [28/100], Step [1000/2534], Loss: 0.4226\n",
            "Epoch [28/100], Step [1100/2534], Loss: 0.3351\n",
            "Epoch [28/100], Step [1200/2534], Loss: 0.2398\n",
            "Epoch [28/100], Step [1300/2534], Loss: 0.3815\n",
            "Epoch [28/100], Step [1400/2534], Loss: 0.2821\n",
            "Epoch [28/100], Step [1500/2534], Loss: 0.4088\n",
            "Epoch [28/100], Step [1600/2534], Loss: 0.4226\n",
            "Epoch [28/100], Step [1700/2534], Loss: 0.2411\n",
            "Epoch [28/100], Step [1800/2534], Loss: 0.3216\n",
            "Epoch [28/100], Step [1900/2534], Loss: 0.1868\n",
            "Epoch [28/100], Step [2000/2534], Loss: 0.5958\n",
            "Epoch [28/100], Step [2100/2534], Loss: 0.6110\n",
            "Epoch [28/100], Step [2200/2534], Loss: 0.8336\n",
            "Epoch [28/100], Step [2300/2534], Loss: 0.4896\n",
            "Epoch [28/100], Step [2400/2534], Loss: 0.2867\n",
            "Epoch [28/100], Step [2500/2534], Loss: 0.3536\n",
            "Epoch [28/100] Average Training Loss: 0.3915\n",
            "Epoch [29/100], Step [100/2534], Loss: 0.2283\n",
            "Epoch [29/100], Step [200/2534], Loss: 0.1105\n",
            "Epoch [29/100], Step [300/2534], Loss: 0.3070\n",
            "Epoch [29/100], Step [400/2534], Loss: 0.2854\n",
            "Epoch [29/100], Step [500/2534], Loss: 0.2230\n",
            "Epoch [29/100], Step [600/2534], Loss: 0.2890\n",
            "Epoch [29/100], Step [700/2534], Loss: 0.3456\n",
            "Epoch [29/100], Step [800/2534], Loss: 0.3012\n",
            "Epoch [29/100], Step [900/2534], Loss: 0.1567\n",
            "Epoch [29/100], Step [1000/2534], Loss: 0.2329\n",
            "Epoch [29/100], Step [1100/2534], Loss: 0.4106\n",
            "Epoch [29/100], Step [1200/2534], Loss: 0.3391\n",
            "Epoch [29/100], Step [1300/2534], Loss: 0.4946\n",
            "Epoch [29/100], Step [1400/2534], Loss: 0.4341\n",
            "Epoch [29/100], Step [1500/2534], Loss: 0.3693\n",
            "Epoch [29/100], Step [1600/2534], Loss: 0.4526\n",
            "Epoch [29/100], Step [1700/2534], Loss: 0.2778\n",
            "Epoch [29/100], Step [1800/2534], Loss: 0.5156\n",
            "Epoch [29/100], Step [1900/2534], Loss: 0.3570\n",
            "Epoch [29/100], Step [2000/2534], Loss: 0.4737\n",
            "Epoch [29/100], Step [2100/2534], Loss: 0.6603\n",
            "Epoch [29/100], Step [2200/2534], Loss: 0.3453\n",
            "Epoch [29/100], Step [2300/2534], Loss: 0.5433\n",
            "Epoch [29/100], Step [2400/2534], Loss: 0.5240\n",
            "Epoch [29/100], Step [2500/2534], Loss: 0.4035\n",
            "Epoch [29/100] Average Training Loss: 0.3834\n",
            "Epoch [30/100], Step [100/2534], Loss: 0.1668\n",
            "Epoch [30/100], Step [200/2534], Loss: 0.1558\n",
            "Epoch [30/100], Step [300/2534], Loss: 0.1928\n",
            "Epoch [30/100], Step [400/2534], Loss: 0.3837\n",
            "Epoch [30/100], Step [500/2534], Loss: 0.0895\n",
            "Epoch [30/100], Step [600/2534], Loss: 0.3439\n",
            "Epoch [30/100], Step [700/2534], Loss: 0.3270\n",
            "Epoch [30/100], Step [800/2534], Loss: 0.3546\n",
            "Epoch [30/100], Step [900/2534], Loss: 0.3407\n",
            "Epoch [30/100], Step [1000/2534], Loss: 0.1940\n",
            "Epoch [30/100], Step [1100/2534], Loss: 0.2235\n",
            "Epoch [30/100], Step [1200/2534], Loss: 0.3608\n",
            "Epoch [30/100], Step [1300/2534], Loss: 0.2790\n",
            "Epoch [30/100], Step [1400/2534], Loss: 0.3858\n",
            "Epoch [30/100], Step [1500/2534], Loss: 0.1998\n",
            "Epoch [30/100], Step [1600/2534], Loss: 0.4742\n",
            "Epoch [30/100], Step [1700/2534], Loss: 0.4674\n",
            "Epoch [30/100], Step [1800/2534], Loss: 0.6090\n",
            "Epoch [30/100], Step [1900/2534], Loss: 0.3825\n",
            "Epoch [30/100], Step [2000/2534], Loss: 0.4150\n",
            "Epoch [30/100], Step [2100/2534], Loss: 0.4184\n",
            "Epoch [30/100], Step [2200/2534], Loss: 0.5832\n",
            "Epoch [30/100], Step [2300/2534], Loss: 0.3418\n",
            "Epoch [30/100], Step [2400/2534], Loss: 0.5812\n",
            "Epoch [30/100], Step [2500/2534], Loss: 0.5998\n",
            "Epoch [30/100] Average Training Loss: 0.3763\n",
            "Epoch [31/100], Step [100/2534], Loss: 0.1718\n",
            "Epoch [31/100], Step [200/2534], Loss: 0.2135\n",
            "Epoch [31/100], Step [300/2534], Loss: 0.4511\n",
            "Epoch [31/100], Step [400/2534], Loss: 0.1795\n",
            "Epoch [31/100], Step [500/2534], Loss: 0.3673\n",
            "Epoch [31/100], Step [600/2534], Loss: 0.3004\n",
            "Epoch [31/100], Step [700/2534], Loss: 0.1478\n",
            "Epoch [31/100], Step [800/2534], Loss: 0.2036\n",
            "Epoch [31/100], Step [900/2534], Loss: 0.4606\n",
            "Epoch [31/100], Step [1000/2534], Loss: 0.2554\n",
            "Epoch [31/100], Step [1100/2534], Loss: 0.3525\n",
            "Epoch [31/100], Step [1200/2534], Loss: 0.3359\n",
            "Epoch [31/100], Step [1300/2534], Loss: 0.3905\n",
            "Epoch [31/100], Step [1400/2534], Loss: 0.3743\n",
            "Epoch [31/100], Step [1500/2534], Loss: 0.3726\n",
            "Epoch [31/100], Step [1600/2534], Loss: 0.4065\n",
            "Epoch [31/100], Step [1700/2534], Loss: 0.3850\n",
            "Epoch [31/100], Step [1800/2534], Loss: 0.2884\n",
            "Epoch [31/100], Step [1900/2534], Loss: 0.2122\n",
            "Epoch [31/100], Step [2000/2534], Loss: 0.1617\n",
            "Epoch [31/100], Step [2100/2534], Loss: 0.8872\n",
            "Epoch [31/100], Step [2200/2534], Loss: 0.4720\n",
            "Epoch [31/100], Step [2300/2534], Loss: 0.4544\n",
            "Epoch [31/100], Step [2400/2534], Loss: 0.4266\n",
            "Epoch [31/100], Step [2500/2534], Loss: 0.3555\n",
            "Epoch [31/100] Average Training Loss: 0.3702\n",
            "Epoch [32/100], Step [100/2534], Loss: 0.2963\n",
            "Epoch [32/100], Step [200/2534], Loss: 0.1786\n",
            "Epoch [32/100], Step [300/2534], Loss: 0.2546\n",
            "Epoch [32/100], Step [400/2534], Loss: 0.2444\n",
            "Epoch [32/100], Step [500/2534], Loss: 0.1633\n",
            "Epoch [32/100], Step [600/2534], Loss: 0.2232\n",
            "Epoch [32/100], Step [700/2534], Loss: 0.1978\n",
            "Epoch [32/100], Step [800/2534], Loss: 0.3988\n",
            "Epoch [32/100], Step [900/2534], Loss: 0.2374\n",
            "Epoch [32/100], Step [1000/2534], Loss: 0.2286\n",
            "Epoch [32/100], Step [1100/2534], Loss: 0.3913\n",
            "Epoch [32/100], Step [1200/2534], Loss: 0.3419\n",
            "Epoch [32/100], Step [1300/2534], Loss: 0.3930\n",
            "Epoch [32/100], Step [1400/2534], Loss: 0.1688\n",
            "Epoch [32/100], Step [1500/2534], Loss: 0.3180\n",
            "Epoch [32/100], Step [1600/2534], Loss: 0.4856\n",
            "Epoch [32/100], Step [1700/2534], Loss: 0.5419\n",
            "Epoch [32/100], Step [1800/2534], Loss: 0.4762\n",
            "Epoch [32/100], Step [1900/2534], Loss: 0.2506\n",
            "Epoch [32/100], Step [2000/2534], Loss: 0.5309\n",
            "Epoch [32/100], Step [2100/2534], Loss: 0.4609\n",
            "Epoch [32/100], Step [2200/2534], Loss: 0.4257\n",
            "Epoch [32/100], Step [2300/2534], Loss: 0.4303\n",
            "Epoch [32/100], Step [2400/2534], Loss: 0.4660\n",
            "Epoch [32/100], Step [2500/2534], Loss: 0.6730\n",
            "Epoch [32/100] Average Training Loss: 0.3639\n",
            "Epoch [33/100], Step [100/2534], Loss: 0.1770\n",
            "Epoch [33/100], Step [200/2534], Loss: 0.2977\n",
            "Epoch [33/100], Step [300/2534], Loss: 0.1616\n",
            "Epoch [33/100], Step [400/2534], Loss: 0.2938\n",
            "Epoch [33/100], Step [500/2534], Loss: 0.3136\n",
            "Epoch [33/100], Step [600/2534], Loss: 0.2264\n",
            "Epoch [33/100], Step [700/2534], Loss: 0.3578\n",
            "Epoch [33/100], Step [800/2534], Loss: 0.1656\n",
            "Epoch [33/100], Step [900/2534], Loss: 0.2278\n",
            "Epoch [33/100], Step [1000/2534], Loss: 0.3237\n",
            "Epoch [33/100], Step [1100/2534], Loss: 0.3985\n",
            "Epoch [33/100], Step [1200/2534], Loss: 0.3716\n",
            "Epoch [33/100], Step [1300/2534], Loss: 0.4284\n",
            "Epoch [33/100], Step [1400/2534], Loss: 0.2485\n",
            "Epoch [33/100], Step [1500/2534], Loss: 0.4930\n",
            "Epoch [33/100], Step [1600/2534], Loss: 0.2849\n",
            "Epoch [33/100], Step [1700/2534], Loss: 0.3768\n",
            "Epoch [33/100], Step [1800/2534], Loss: 0.4911\n",
            "Epoch [33/100], Step [1900/2534], Loss: 0.5979\n",
            "Epoch [33/100], Step [2000/2534], Loss: 0.5487\n",
            "Epoch [33/100], Step [2100/2534], Loss: 0.2368\n",
            "Epoch [33/100], Step [2200/2534], Loss: 0.5390\n",
            "Epoch [33/100], Step [2300/2534], Loss: 0.4769\n",
            "Epoch [33/100], Step [2400/2534], Loss: 0.5842\n",
            "Epoch [33/100], Step [2500/2534], Loss: 0.3627\n",
            "Epoch [33/100] Average Training Loss: 0.3584\n",
            "Epoch [34/100], Step [100/2534], Loss: 0.2584\n",
            "Epoch [34/100], Step [200/2534], Loss: 0.3859\n",
            "Epoch [34/100], Step [300/2534], Loss: 0.2809\n",
            "Epoch [34/100], Step [400/2534], Loss: 0.2143\n",
            "Epoch [34/100], Step [500/2534], Loss: 0.3680\n",
            "Epoch [34/100], Step [600/2534], Loss: 0.2775\n",
            "Epoch [34/100], Step [700/2534], Loss: 0.4667\n",
            "Epoch [34/100], Step [800/2534], Loss: 0.1924\n",
            "Epoch [34/100], Step [900/2534], Loss: 0.2879\n",
            "Epoch [34/100], Step [1000/2534], Loss: 0.5523\n",
            "Epoch [34/100], Step [1100/2534], Loss: 0.5255\n",
            "Epoch [34/100], Step [1200/2534], Loss: 0.3014\n",
            "Epoch [34/100], Step [1300/2534], Loss: 0.4801\n",
            "Epoch [34/100], Step [1400/2534], Loss: 0.3993\n",
            "Epoch [34/100], Step [1500/2534], Loss: 0.2320\n",
            "Epoch [34/100], Step [1600/2534], Loss: 0.3033\n",
            "Epoch [34/100], Step [1700/2534], Loss: 0.5129\n",
            "Epoch [34/100], Step [1800/2534], Loss: 0.4057\n",
            "Epoch [34/100], Step [1900/2534], Loss: 0.3317\n",
            "Epoch [34/100], Step [2000/2534], Loss: 0.3436\n",
            "Epoch [34/100], Step [2100/2534], Loss: 0.4054\n",
            "Epoch [34/100], Step [2200/2534], Loss: 0.5616\n",
            "Epoch [34/100], Step [2300/2534], Loss: 0.4633\n",
            "Epoch [34/100], Step [2400/2534], Loss: 0.5148\n",
            "Epoch [34/100], Step [2500/2534], Loss: 0.5826\n",
            "Epoch [34/100] Average Training Loss: 0.3546\n",
            "Epoch [35/100], Step [100/2534], Loss: 0.3160\n",
            "Epoch [35/100], Step [200/2534], Loss: 0.3930\n",
            "Epoch [35/100], Step [300/2534], Loss: 0.0774\n",
            "Epoch [35/100], Step [400/2534], Loss: 0.2078\n",
            "Epoch [35/100], Step [500/2534], Loss: 0.3580\n",
            "Epoch [35/100], Step [600/2534], Loss: 0.3313\n",
            "Epoch [35/100], Step [700/2534], Loss: 0.1833\n",
            "Epoch [35/100], Step [800/2534], Loss: 0.4149\n",
            "Epoch [35/100], Step [900/2534], Loss: 0.3746\n",
            "Epoch [35/100], Step [1000/2534], Loss: 0.3188\n",
            "Epoch [35/100], Step [1100/2534], Loss: 0.2372\n",
            "Epoch [35/100], Step [1200/2534], Loss: 0.4241\n",
            "Epoch [35/100], Step [1300/2534], Loss: 0.1991\n",
            "Epoch [35/100], Step [1400/2534], Loss: 0.5672\n",
            "Epoch [35/100], Step [1500/2534], Loss: 0.2002\n",
            "Epoch [35/100], Step [1600/2534], Loss: 0.6629\n",
            "Epoch [35/100], Step [1700/2534], Loss: 0.3289\n",
            "Epoch [35/100], Step [1800/2534], Loss: 0.3108\n",
            "Epoch [35/100], Step [1900/2534], Loss: 0.4793\n",
            "Epoch [35/100], Step [2000/2534], Loss: 0.2508\n",
            "Epoch [35/100], Step [2100/2534], Loss: 0.5310\n",
            "Epoch [35/100], Step [2200/2534], Loss: 0.4562\n",
            "Epoch [35/100], Step [2300/2534], Loss: 0.5497\n",
            "Epoch [35/100], Step [2400/2534], Loss: 0.7743\n",
            "Epoch [35/100], Step [2500/2534], Loss: 0.5661\n",
            "Epoch [35/100] Average Training Loss: 0.3504\n",
            "Epoch [36/100], Step [100/2534], Loss: 0.2530\n",
            "Epoch [36/100], Step [200/2534], Loss: 0.1831\n",
            "Epoch [36/100], Step [300/2534], Loss: 0.4148\n",
            "Epoch [36/100], Step [400/2534], Loss: 0.2329\n",
            "Epoch [36/100], Step [500/2534], Loss: 0.2538\n",
            "Epoch [36/100], Step [600/2534], Loss: 0.2690\n",
            "Epoch [36/100], Step [700/2534], Loss: 0.2878\n",
            "Epoch [36/100], Step [800/2534], Loss: 0.1118\n",
            "Epoch [36/100], Step [900/2534], Loss: 0.6769\n",
            "Epoch [36/100], Step [1000/2534], Loss: 0.3681\n",
            "Epoch [36/100], Step [1100/2534], Loss: 0.1457\n",
            "Epoch [36/100], Step [1200/2534], Loss: 0.3230\n",
            "Epoch [36/100], Step [1300/2534], Loss: 0.2821\n",
            "Epoch [36/100], Step [1400/2534], Loss: 0.5394\n",
            "Epoch [36/100], Step [1500/2534], Loss: 0.2813\n",
            "Epoch [36/100], Step [1600/2534], Loss: 0.2734\n",
            "Epoch [36/100], Step [1700/2534], Loss: 0.4681\n",
            "Epoch [36/100], Step [1800/2534], Loss: 0.6098\n",
            "Epoch [36/100], Step [1900/2534], Loss: 0.2889\n",
            "Epoch [36/100], Step [2000/2534], Loss: 0.4677\n",
            "Epoch [36/100], Step [2100/2534], Loss: 0.5339\n",
            "Epoch [36/100], Step [2200/2534], Loss: 0.2672\n",
            "Epoch [36/100], Step [2300/2534], Loss: 0.3360\n",
            "Epoch [36/100], Step [2400/2534], Loss: 0.3688\n",
            "Epoch [36/100], Step [2500/2534], Loss: 0.5418\n",
            "Epoch [36/100] Average Training Loss: 0.3457\n",
            "Epoch [37/100], Step [100/2534], Loss: 0.1724\n",
            "Epoch [37/100], Step [200/2534], Loss: 0.3112\n",
            "Epoch [37/100], Step [300/2534], Loss: 0.2100\n",
            "Epoch [37/100], Step [400/2534], Loss: 0.4657\n",
            "Epoch [37/100], Step [500/2534], Loss: 0.1905\n",
            "Epoch [37/100], Step [600/2534], Loss: 0.2084\n",
            "Epoch [37/100], Step [700/2534], Loss: 0.2971\n",
            "Epoch [37/100], Step [800/2534], Loss: 0.5236\n",
            "Epoch [37/100], Step [900/2534], Loss: 0.4115\n",
            "Epoch [37/100], Step [1000/2534], Loss: 0.1168\n",
            "Epoch [37/100], Step [1100/2534], Loss: 0.3716\n",
            "Epoch [37/100], Step [1200/2534], Loss: 0.2560\n",
            "Epoch [37/100], Step [1300/2534], Loss: 0.2740\n",
            "Epoch [37/100], Step [1400/2534], Loss: 0.5089\n",
            "Epoch [37/100], Step [1500/2534], Loss: 0.3298\n",
            "Epoch [37/100], Step [1600/2534], Loss: 0.2928\n",
            "Epoch [37/100], Step [1700/2534], Loss: 0.1238\n",
            "Epoch [37/100], Step [1800/2534], Loss: 0.2624\n",
            "Epoch [37/100], Step [1900/2534], Loss: 0.2017\n",
            "Epoch [37/100], Step [2000/2534], Loss: 0.4877\n",
            "Epoch [37/100], Step [2100/2534], Loss: 0.3085\n",
            "Epoch [37/100], Step [2200/2534], Loss: 0.1823\n",
            "Epoch [37/100], Step [2300/2534], Loss: 0.3562\n",
            "Epoch [37/100], Step [2400/2534], Loss: 0.4415\n",
            "Epoch [37/100], Step [2500/2534], Loss: 0.6307\n",
            "Epoch [37/100] Average Training Loss: 0.3392\n",
            "Epoch [38/100], Step [100/2534], Loss: 0.1695\n",
            "Epoch [38/100], Step [200/2534], Loss: 0.4021\n",
            "Epoch [38/100], Step [300/2534], Loss: 0.2720\n",
            "Epoch [38/100], Step [400/2534], Loss: 0.2156\n",
            "Epoch [38/100], Step [500/2534], Loss: 0.3808\n",
            "Epoch [38/100], Step [600/2534], Loss: 0.2315\n",
            "Epoch [38/100], Step [700/2534], Loss: 0.3377\n",
            "Epoch [38/100], Step [800/2534], Loss: 0.4480\n",
            "Epoch [38/100], Step [900/2534], Loss: 0.1304\n",
            "Epoch [38/100], Step [1000/2534], Loss: 0.3708\n",
            "Epoch [38/100], Step [1100/2534], Loss: 0.3756\n",
            "Epoch [38/100], Step [1200/2534], Loss: 0.2347\n",
            "Epoch [38/100], Step [1300/2534], Loss: 0.4024\n",
            "Epoch [38/100], Step [1400/2534], Loss: 0.1054\n",
            "Epoch [38/100], Step [1500/2534], Loss: 0.3010\n",
            "Epoch [38/100], Step [1600/2534], Loss: 0.5537\n",
            "Epoch [38/100], Step [1700/2534], Loss: 0.6414\n",
            "Epoch [38/100], Step [1800/2534], Loss: 0.4624\n",
            "Epoch [38/100], Step [1900/2534], Loss: 0.2823\n",
            "Epoch [38/100], Step [2000/2534], Loss: 0.3565\n",
            "Epoch [38/100], Step [2100/2534], Loss: 0.4084\n",
            "Epoch [38/100], Step [2200/2534], Loss: 0.3462\n",
            "Epoch [38/100], Step [2300/2534], Loss: 0.4108\n",
            "Epoch [38/100], Step [2400/2534], Loss: 0.2199\n",
            "Epoch [38/100], Step [2500/2534], Loss: 0.4489\n",
            "Epoch [38/100] Average Training Loss: 0.3378\n",
            "Epoch [39/100], Step [100/2534], Loss: 0.2864\n",
            "Epoch [39/100], Step [200/2534], Loss: 0.3401\n",
            "Epoch [39/100], Step [300/2534], Loss: 0.3311\n",
            "Epoch [39/100], Step [400/2534], Loss: 0.3486\n",
            "Epoch [39/100], Step [500/2534], Loss: 0.2662\n",
            "Epoch [39/100], Step [600/2534], Loss: 0.2515\n",
            "Epoch [39/100], Step [700/2534], Loss: 0.3126\n",
            "Epoch [39/100], Step [800/2534], Loss: 0.2296\n",
            "Epoch [39/100], Step [900/2534], Loss: 0.1919\n",
            "Epoch [39/100], Step [1000/2534], Loss: 0.2447\n",
            "Epoch [39/100], Step [1100/2534], Loss: 0.2895\n",
            "Epoch [39/100], Step [1200/2534], Loss: 0.5027\n",
            "Epoch [39/100], Step [1300/2534], Loss: 0.3994\n",
            "Epoch [39/100], Step [1400/2534], Loss: 0.2966\n",
            "Epoch [39/100], Step [1500/2534], Loss: 0.3174\n",
            "Epoch [39/100], Step [1600/2534], Loss: 0.2993\n",
            "Epoch [39/100], Step [1700/2534], Loss: 0.5712\n",
            "Epoch [39/100], Step [1800/2534], Loss: 0.3688\n",
            "Epoch [39/100], Step [1900/2534], Loss: 0.4070\n",
            "Epoch [39/100], Step [2000/2534], Loss: 0.6225\n",
            "Epoch [39/100], Step [2100/2534], Loss: 0.3584\n",
            "Epoch [39/100], Step [2200/2534], Loss: 0.3875\n",
            "Epoch [39/100], Step [2300/2534], Loss: 0.3119\n",
            "Epoch [39/100], Step [2400/2534], Loss: 0.2404\n",
            "Epoch [39/100], Step [2500/2534], Loss: 0.1679\n",
            "Epoch [39/100] Average Training Loss: 0.3355\n",
            "Epoch [40/100], Step [100/2534], Loss: 0.1770\n",
            "Epoch [40/100], Step [200/2534], Loss: 0.3083\n",
            "Epoch [40/100], Step [300/2534], Loss: 0.2641\n",
            "Epoch [40/100], Step [400/2534], Loss: 0.3089\n",
            "Epoch [40/100], Step [500/2534], Loss: 0.3644\n",
            "Epoch [40/100], Step [600/2534], Loss: 0.5002\n",
            "Epoch [40/100], Step [700/2534], Loss: 0.4560\n",
            "Epoch [40/100], Step [800/2534], Loss: 0.3323\n",
            "Epoch [40/100], Step [900/2534], Loss: 0.2383\n",
            "Epoch [40/100], Step [1000/2534], Loss: 0.2043\n",
            "Epoch [40/100], Step [1100/2534], Loss: 0.3353\n",
            "Epoch [40/100], Step [1200/2534], Loss: 0.3726\n",
            "Epoch [40/100], Step [1300/2534], Loss: 0.2953\n",
            "Epoch [40/100], Step [1400/2534], Loss: 0.2732\n",
            "Epoch [40/100], Step [1500/2534], Loss: 0.2750\n",
            "Epoch [40/100], Step [1600/2534], Loss: 0.2438\n",
            "Epoch [40/100], Step [1700/2534], Loss: 0.3618\n",
            "Epoch [40/100], Step [1800/2534], Loss: 0.5226\n",
            "Epoch [40/100], Step [1900/2534], Loss: 0.3607\n",
            "Epoch [40/100], Step [2000/2534], Loss: 0.2542\n",
            "Epoch [40/100], Step [2100/2534], Loss: 0.3197\n",
            "Epoch [40/100], Step [2200/2534], Loss: 0.5174\n",
            "Epoch [40/100], Step [2300/2534], Loss: 0.4605\n",
            "Epoch [40/100], Step [2400/2534], Loss: 0.2830\n",
            "Epoch [40/100], Step [2500/2534], Loss: 0.3928\n",
            "Epoch [40/100] Average Training Loss: 0.3323\n",
            "Epoch [41/100], Step [100/2534], Loss: 0.3950\n",
            "Epoch [41/100], Step [200/2534], Loss: 0.1698\n",
            "Epoch [41/100], Step [300/2534], Loss: 0.3848\n",
            "Epoch [41/100], Step [400/2534], Loss: 0.2583\n",
            "Epoch [41/100], Step [500/2534], Loss: 0.2258\n",
            "Epoch [41/100], Step [600/2534], Loss: 0.1862\n",
            "Epoch [41/100], Step [700/2534], Loss: 0.2132\n",
            "Epoch [41/100], Step [800/2534], Loss: 0.3214\n",
            "Epoch [41/100], Step [900/2534], Loss: 0.4300\n",
            "Epoch [41/100], Step [1000/2534], Loss: 0.2413\n",
            "Epoch [41/100], Step [1100/2534], Loss: 0.2071\n",
            "Epoch [41/100], Step [1200/2534], Loss: 0.3833\n",
            "Epoch [41/100], Step [1300/2534], Loss: 0.1876\n",
            "Epoch [41/100], Step [1400/2534], Loss: 0.3885\n",
            "Epoch [41/100], Step [1500/2534], Loss: 0.2609\n",
            "Epoch [41/100], Step [1600/2534], Loss: 0.2949\n",
            "Epoch [41/100], Step [1700/2534], Loss: 0.4474\n",
            "Epoch [41/100], Step [1800/2534], Loss: 0.4116\n",
            "Epoch [41/100], Step [1900/2534], Loss: 0.3253\n",
            "Epoch [41/100], Step [2000/2534], Loss: 0.3762\n",
            "Epoch [41/100], Step [2100/2534], Loss: 0.3020\n",
            "Epoch [41/100], Step [2200/2534], Loss: 0.5962\n",
            "Epoch [41/100], Step [2300/2534], Loss: 0.4991\n",
            "Epoch [41/100], Step [2400/2534], Loss: 0.4873\n",
            "Epoch [41/100], Step [2500/2534], Loss: 0.3559\n",
            "Epoch [41/100] Average Training Loss: 0.3285\n",
            "Epoch [42/100], Step [100/2534], Loss: 0.1082\n",
            "Epoch [42/100], Step [200/2534], Loss: 0.3395\n",
            "Epoch [42/100], Step [300/2534], Loss: 0.2728\n",
            "Epoch [42/100], Step [400/2534], Loss: 0.3545\n",
            "Epoch [42/100], Step [500/2534], Loss: 0.2756\n",
            "Epoch [42/100], Step [600/2534], Loss: 0.2892\n",
            "Epoch [42/100], Step [700/2534], Loss: 0.2526\n",
            "Epoch [42/100], Step [800/2534], Loss: 0.2904\n",
            "Epoch [42/100], Step [900/2534], Loss: 0.2547\n",
            "Epoch [42/100], Step [1000/2534], Loss: 0.4325\n",
            "Epoch [42/100], Step [1100/2534], Loss: 0.3036\n",
            "Epoch [42/100], Step [1200/2534], Loss: 0.3380\n",
            "Epoch [42/100], Step [1300/2534], Loss: 0.2691\n",
            "Epoch [42/100], Step [1400/2534], Loss: 0.2764\n",
            "Epoch [42/100], Step [1500/2534], Loss: 0.1450\n",
            "Epoch [42/100], Step [1600/2534], Loss: 0.4047\n",
            "Epoch [42/100], Step [1700/2534], Loss: 0.4607\n",
            "Epoch [42/100], Step [1800/2534], Loss: 0.4569\n",
            "Epoch [42/100], Step [1900/2534], Loss: 0.2335\n",
            "Epoch [42/100], Step [2000/2534], Loss: 0.4391\n",
            "Epoch [42/100], Step [2100/2534], Loss: 0.4787\n",
            "Epoch [42/100], Step [2200/2534], Loss: 0.3486\n",
            "Epoch [42/100], Step [2300/2534], Loss: 0.3482\n",
            "Epoch [42/100], Step [2400/2534], Loss: 0.4401\n",
            "Epoch [42/100], Step [2500/2534], Loss: 0.5495\n",
            "Epoch [42/100] Average Training Loss: 0.3276\n",
            "Epoch [43/100], Step [100/2534], Loss: 0.1064\n",
            "Epoch [43/100], Step [200/2534], Loss: 0.2707\n",
            "Epoch [43/100], Step [300/2534], Loss: 0.2144\n",
            "Epoch [43/100], Step [400/2534], Loss: 0.3588\n",
            "Epoch [43/100], Step [500/2534], Loss: 0.1354\n",
            "Epoch [43/100], Step [600/2534], Loss: 0.3398\n",
            "Epoch [43/100], Step [700/2534], Loss: 0.4233\n",
            "Epoch [43/100], Step [800/2534], Loss: 0.2004\n",
            "Epoch [43/100], Step [900/2534], Loss: 0.3124\n",
            "Epoch [43/100], Step [1000/2534], Loss: 0.3330\n",
            "Epoch [43/100], Step [1100/2534], Loss: 0.2633\n",
            "Epoch [43/100], Step [1200/2534], Loss: 0.6434\n",
            "Epoch [43/100], Step [1300/2534], Loss: 0.6266\n",
            "Epoch [43/100], Step [1400/2534], Loss: 0.4404\n",
            "Epoch [43/100], Step [1500/2534], Loss: 0.4610\n",
            "Epoch [43/100], Step [1600/2534], Loss: 0.4330\n",
            "Epoch [43/100], Step [1700/2534], Loss: 0.2500\n",
            "Epoch [43/100], Step [1800/2534], Loss: 0.6585\n",
            "Epoch [43/100], Step [1900/2534], Loss: 0.3943\n",
            "Epoch [43/100], Step [2000/2534], Loss: 0.3837\n",
            "Epoch [43/100], Step [2100/2534], Loss: 0.4319\n",
            "Epoch [43/100], Step [2200/2534], Loss: 0.2721\n",
            "Epoch [43/100], Step [2300/2534], Loss: 0.4897\n",
            "Epoch [43/100], Step [2400/2534], Loss: 0.3924\n",
            "Epoch [43/100], Step [2500/2534], Loss: 0.2726\n",
            "Epoch [43/100] Average Training Loss: 0.3235\n",
            "Epoch [44/100], Step [100/2534], Loss: 0.1448\n",
            "Epoch [44/100], Step [200/2534], Loss: 0.2197\n",
            "Epoch [44/100], Step [300/2534], Loss: 0.1770\n",
            "Epoch [44/100], Step [400/2534], Loss: 0.2015\n",
            "Epoch [44/100], Step [500/2534], Loss: 0.3657\n",
            "Epoch [44/100], Step [600/2534], Loss: 0.2893\n",
            "Epoch [44/100], Step [700/2534], Loss: 0.4216\n",
            "Epoch [44/100], Step [800/2534], Loss: 0.1977\n",
            "Epoch [44/100], Step [900/2534], Loss: 0.1419\n",
            "Epoch [44/100], Step [1000/2534], Loss: 0.2717\n",
            "Epoch [44/100], Step [1100/2534], Loss: 0.6636\n",
            "Epoch [44/100], Step [1200/2534], Loss: 0.3796\n",
            "Epoch [44/100], Step [1300/2534], Loss: 0.2629\n",
            "Epoch [44/100], Step [1400/2534], Loss: 0.4866\n",
            "Epoch [44/100], Step [1500/2534], Loss: 0.2235\n",
            "Epoch [44/100], Step [1600/2534], Loss: 0.3996\n",
            "Epoch [44/100], Step [1700/2534], Loss: 0.2724\n",
            "Epoch [44/100], Step [1800/2534], Loss: 0.2760\n",
            "Epoch [44/100], Step [1900/2534], Loss: 0.3944\n",
            "Epoch [44/100], Step [2000/2534], Loss: 0.3621\n",
            "Epoch [44/100], Step [2100/2534], Loss: 0.3178\n",
            "Epoch [44/100], Step [2200/2534], Loss: 0.5186\n",
            "Epoch [44/100], Step [2300/2534], Loss: 0.2100\n",
            "Epoch [44/100], Step [2400/2534], Loss: 0.4183\n",
            "Epoch [44/100], Step [2500/2534], Loss: 0.3194\n",
            "Epoch [44/100] Average Training Loss: 0.3229\n",
            "Epoch [45/100], Step [100/2534], Loss: 0.2042\n",
            "Epoch [45/100], Step [200/2534], Loss: 0.3555\n",
            "Epoch [45/100], Step [300/2534], Loss: 0.6781\n",
            "Epoch [45/100], Step [400/2534], Loss: 0.1549\n",
            "Epoch [45/100], Step [500/2534], Loss: 0.1898\n",
            "Epoch [45/100], Step [600/2534], Loss: 0.1932\n",
            "Epoch [45/100], Step [700/2534], Loss: 0.0724\n",
            "Epoch [45/100], Step [800/2534], Loss: 0.1875\n",
            "Epoch [45/100], Step [900/2534], Loss: 0.2251\n",
            "Epoch [45/100], Step [1000/2534], Loss: 0.2036\n",
            "Epoch [45/100], Step [1100/2534], Loss: 0.2707\n",
            "Epoch [45/100], Step [1200/2534], Loss: 0.2506\n",
            "Epoch [45/100], Step [1300/2534], Loss: 0.4089\n",
            "Epoch [45/100], Step [1400/2534], Loss: 0.4683\n",
            "Epoch [45/100], Step [1500/2534], Loss: 0.1581\n",
            "Epoch [45/100], Step [1600/2534], Loss: 0.1455\n",
            "Epoch [45/100], Step [1700/2534], Loss: 0.3729\n",
            "Epoch [45/100], Step [1800/2534], Loss: 0.3916\n",
            "Epoch [45/100], Step [1900/2534], Loss: 0.4110\n",
            "Epoch [45/100], Step [2000/2534], Loss: 0.4326\n",
            "Epoch [45/100], Step [2100/2534], Loss: 0.4001\n",
            "Epoch [45/100], Step [2200/2534], Loss: 0.5727\n",
            "Epoch [45/100], Step [2300/2534], Loss: 0.3892\n",
            "Epoch [45/100], Step [2400/2534], Loss: 0.3617\n",
            "Epoch [45/100], Step [2500/2534], Loss: 0.3545\n",
            "Epoch [45/100] Average Training Loss: 0.3206\n",
            "Epoch [46/100], Step [100/2534], Loss: 0.2148\n",
            "Epoch [46/100], Step [200/2534], Loss: 0.1693\n",
            "Epoch [46/100], Step [300/2534], Loss: 0.4297\n",
            "Epoch [46/100], Step [400/2534], Loss: 0.3005\n",
            "Epoch [46/100], Step [500/2534], Loss: 0.3643\n",
            "Epoch [46/100], Step [600/2534], Loss: 0.3364\n",
            "Epoch [46/100], Step [700/2534], Loss: 0.4433\n",
            "Epoch [46/100], Step [800/2534], Loss: 0.2884\n",
            "Epoch [46/100], Step [900/2534], Loss: 0.2256\n",
            "Epoch [46/100], Step [1000/2534], Loss: 0.2852\n",
            "Epoch [46/100], Step [1100/2534], Loss: 0.2449\n",
            "Epoch [46/100], Step [1200/2534], Loss: 0.5006\n",
            "Epoch [46/100], Step [1300/2534], Loss: 0.2957\n",
            "Epoch [46/100], Step [1400/2534], Loss: 0.2227\n",
            "Epoch [46/100], Step [1500/2534], Loss: 0.5125\n",
            "Epoch [46/100], Step [1600/2534], Loss: 0.2157\n",
            "Epoch [46/100], Step [1700/2534], Loss: 0.1673\n",
            "Epoch [46/100], Step [1800/2534], Loss: 0.1791\n",
            "Epoch [46/100], Step [1900/2534], Loss: 0.4107\n",
            "Epoch [46/100], Step [2000/2534], Loss: 0.1354\n",
            "Epoch [46/100], Step [2100/2534], Loss: 0.4001\n",
            "Epoch [46/100], Step [2200/2534], Loss: 0.2635\n",
            "Epoch [46/100], Step [2300/2534], Loss: 0.2840\n",
            "Epoch [46/100], Step [2400/2534], Loss: 0.3129\n",
            "Epoch [46/100], Step [2500/2534], Loss: 0.3885\n",
            "Epoch [46/100] Average Training Loss: 0.3192\n",
            "Epoch [47/100], Step [100/2534], Loss: 0.2968\n",
            "Epoch [47/100], Step [200/2534], Loss: 0.3117\n",
            "Epoch [47/100], Step [300/2534], Loss: 0.2782\n",
            "Epoch [47/100], Step [400/2534], Loss: 0.3394\n",
            "Epoch [47/100], Step [500/2534], Loss: 0.1713\n",
            "Epoch [47/100], Step [600/2534], Loss: 0.1928\n",
            "Epoch [47/100], Step [700/2534], Loss: 0.4082\n",
            "Epoch [47/100], Step [800/2534], Loss: 0.3216\n",
            "Epoch [47/100], Step [900/2534], Loss: 0.2897\n",
            "Epoch [47/100], Step [1000/2534], Loss: 0.2313\n",
            "Epoch [47/100], Step [1100/2534], Loss: 0.2933\n",
            "Epoch [47/100], Step [1200/2534], Loss: 0.1924\n",
            "Epoch [47/100], Step [1300/2534], Loss: 0.2903\n",
            "Epoch [47/100], Step [1400/2534], Loss: 0.2640\n",
            "Epoch [47/100], Step [1500/2534], Loss: 0.3268\n",
            "Epoch [47/100], Step [1600/2534], Loss: 0.2274\n",
            "Epoch [47/100], Step [1700/2534], Loss: 0.5451\n",
            "Epoch [47/100], Step [1800/2534], Loss: 0.3437\n",
            "Epoch [47/100], Step [1900/2534], Loss: 0.3370\n",
            "Epoch [47/100], Step [2000/2534], Loss: 0.3917\n",
            "Epoch [47/100], Step [2100/2534], Loss: 0.2402\n",
            "Epoch [47/100], Step [2200/2534], Loss: 0.4828\n",
            "Epoch [47/100], Step [2300/2534], Loss: 0.4238\n",
            "Epoch [47/100], Step [2400/2534], Loss: 0.3706\n",
            "Epoch [47/100], Step [2500/2534], Loss: 0.2159\n",
            "Epoch [47/100] Average Training Loss: 0.3182\n",
            "Epoch [48/100], Step [100/2534], Loss: 0.5595\n",
            "Epoch [48/100], Step [200/2534], Loss: 0.2308\n",
            "Epoch [48/100], Step [300/2534], Loss: 0.1276\n",
            "Epoch [48/100], Step [400/2534], Loss: 0.4117\n",
            "Epoch [48/100], Step [500/2534], Loss: 0.2133\n",
            "Epoch [48/100], Step [600/2534], Loss: 0.2716\n",
            "Epoch [48/100], Step [700/2534], Loss: 0.4555\n",
            "Epoch [48/100], Step [800/2534], Loss: 0.2779\n",
            "Epoch [48/100], Step [900/2534], Loss: 0.3242\n",
            "Epoch [48/100], Step [1000/2534], Loss: 0.3092\n",
            "Epoch [48/100], Step [1100/2534], Loss: 0.2481\n",
            "Epoch [48/100], Step [1200/2534], Loss: 0.2310\n",
            "Epoch [48/100], Step [1300/2534], Loss: 0.2249\n",
            "Epoch [48/100], Step [1400/2534], Loss: 0.3109\n",
            "Epoch [48/100], Step [1500/2534], Loss: 0.4769\n",
            "Epoch [48/100], Step [1600/2534], Loss: 0.3052\n",
            "Epoch [48/100], Step [1700/2534], Loss: 0.5185\n",
            "Epoch [48/100], Step [1800/2534], Loss: 0.5800\n",
            "Epoch [48/100], Step [1900/2534], Loss: 0.4101\n",
            "Epoch [48/100], Step [2000/2534], Loss: 0.3201\n",
            "Epoch [48/100], Step [2100/2534], Loss: 0.4848\n",
            "Epoch [48/100], Step [2200/2534], Loss: 0.4150\n",
            "Epoch [48/100], Step [2300/2534], Loss: 0.3099\n",
            "Epoch [48/100], Step [2400/2534], Loss: 0.7645\n",
            "Epoch [48/100], Step [2500/2534], Loss: 0.3694\n",
            "Epoch [48/100] Average Training Loss: 0.3161\n",
            "Epoch [49/100], Step [100/2534], Loss: 0.1350\n",
            "Epoch [49/100], Step [200/2534], Loss: 0.2518\n",
            "Epoch [49/100], Step [300/2534], Loss: 0.3694\n",
            "Epoch [49/100], Step [400/2534], Loss: 0.2372\n",
            "Epoch [49/100], Step [500/2534], Loss: 0.0911\n",
            "Epoch [49/100], Step [600/2534], Loss: 0.3691\n",
            "Epoch [49/100], Step [700/2534], Loss: 0.3450\n",
            "Epoch [49/100], Step [800/2534], Loss: 0.0807\n",
            "Epoch [49/100], Step [900/2534], Loss: 0.3322\n",
            "Epoch [49/100], Step [1000/2534], Loss: 0.4851\n",
            "Epoch [49/100], Step [1100/2534], Loss: 0.1736\n",
            "Epoch [49/100], Step [1200/2534], Loss: 0.5025\n",
            "Epoch [49/100], Step [1300/2534], Loss: 0.2946\n",
            "Epoch [49/100], Step [1400/2534], Loss: 0.3056\n",
            "Epoch [49/100], Step [1500/2534], Loss: 0.1535\n",
            "Epoch [49/100], Step [1600/2534], Loss: 0.4999\n",
            "Epoch [49/100], Step [1700/2534], Loss: 0.5016\n",
            "Epoch [49/100], Step [1800/2534], Loss: 0.2808\n",
            "Epoch [49/100], Step [1900/2534], Loss: 0.4714\n",
            "Epoch [49/100], Step [2000/2534], Loss: 0.5528\n",
            "Epoch [49/100], Step [2100/2534], Loss: 0.2156\n",
            "Epoch [49/100], Step [2200/2534], Loss: 0.3750\n",
            "Epoch [49/100], Step [2300/2534], Loss: 0.4061\n",
            "Epoch [49/100], Step [2400/2534], Loss: 0.3234\n",
            "Epoch [49/100], Step [2500/2534], Loss: 0.1504\n",
            "Epoch [49/100] Average Training Loss: 0.3129\n",
            "Epoch [50/100], Step [100/2534], Loss: 0.3131\n",
            "Epoch [50/100], Step [200/2534], Loss: 0.4060\n",
            "Epoch [50/100], Step [300/2534], Loss: 0.2795\n",
            "Epoch [50/100], Step [400/2534], Loss: 0.3798\n",
            "Epoch [50/100], Step [500/2534], Loss: 0.2877\n",
            "Epoch [50/100], Step [600/2534], Loss: 0.0891\n",
            "Epoch [50/100], Step [700/2534], Loss: 0.1446\n",
            "Epoch [50/100], Step [800/2534], Loss: 0.3187\n",
            "Epoch [50/100], Step [900/2534], Loss: 0.2075\n",
            "Epoch [50/100], Step [1000/2534], Loss: 0.3353\n",
            "Epoch [50/100], Step [1100/2534], Loss: 0.2588\n",
            "Epoch [50/100], Step [1200/2534], Loss: 0.3737\n",
            "Epoch [50/100], Step [1300/2534], Loss: 0.2327\n",
            "Epoch [50/100], Step [1400/2534], Loss: 0.3396\n",
            "Epoch [50/100], Step [1500/2534], Loss: 0.6210\n",
            "Epoch [50/100], Step [1600/2534], Loss: 0.2662\n",
            "Epoch [50/100], Step [1700/2534], Loss: 0.3900\n",
            "Epoch [50/100], Step [1800/2534], Loss: 0.3479\n",
            "Epoch [50/100], Step [1900/2534], Loss: 0.3412\n",
            "Epoch [50/100], Step [2000/2534], Loss: 0.2907\n",
            "Epoch [50/100], Step [2100/2534], Loss: 0.3445\n",
            "Epoch [50/100], Step [2200/2534], Loss: 0.2532\n",
            "Epoch [50/100], Step [2300/2534], Loss: 0.2673\n",
            "Epoch [50/100], Step [2400/2534], Loss: 0.4940\n",
            "Epoch [50/100], Step [2500/2534], Loss: 0.2355\n",
            "Epoch [50/100] Average Training Loss: 0.3126\n",
            "Epoch [51/100], Step [100/2534], Loss: 0.2959\n",
            "Epoch [51/100], Step [200/2534], Loss: 0.2014\n",
            "Epoch [51/100], Step [300/2534], Loss: 0.4687\n",
            "Epoch [51/100], Step [400/2534], Loss: 0.3407\n",
            "Epoch [51/100], Step [500/2534], Loss: 0.3236\n",
            "Epoch [51/100], Step [600/2534], Loss: 0.1738\n",
            "Epoch [51/100], Step [700/2534], Loss: 0.1859\n",
            "Epoch [51/100], Step [800/2534], Loss: 0.1959\n",
            "Epoch [51/100], Step [900/2534], Loss: 0.4341\n",
            "Epoch [51/100], Step [1000/2534], Loss: 0.1589\n",
            "Epoch [51/100], Step [1100/2534], Loss: 0.1783\n",
            "Epoch [51/100], Step [1200/2534], Loss: 0.2425\n",
            "Epoch [51/100], Step [1300/2534], Loss: 0.4107\n",
            "Epoch [51/100], Step [1400/2534], Loss: 0.2816\n",
            "Epoch [51/100], Step [1500/2534], Loss: 0.4493\n",
            "Epoch [51/100], Step [1600/2534], Loss: 0.2923\n",
            "Epoch [51/100], Step [1700/2534], Loss: 0.2919\n",
            "Epoch [51/100], Step [1800/2534], Loss: 0.4303\n",
            "Epoch [51/100], Step [1900/2534], Loss: 0.2455\n",
            "Epoch [51/100], Step [2000/2534], Loss: 0.3497\n",
            "Epoch [51/100], Step [2100/2534], Loss: 0.4698\n",
            "Epoch [51/100], Step [2200/2534], Loss: 0.3948\n",
            "Epoch [51/100], Step [2300/2534], Loss: 0.4553\n",
            "Epoch [51/100], Step [2400/2534], Loss: 0.5028\n",
            "Epoch [51/100], Step [2500/2534], Loss: 0.2233\n",
            "Epoch [51/100] Average Training Loss: 0.3116\n",
            "Epoch [52/100], Step [100/2534], Loss: 0.2834\n",
            "Epoch [52/100], Step [200/2534], Loss: 0.1691\n",
            "Epoch [52/100], Step [300/2534], Loss: 0.2521\n",
            "Epoch [52/100], Step [400/2534], Loss: 0.3310\n",
            "Epoch [52/100], Step [500/2534], Loss: 0.1335\n",
            "Epoch [52/100], Step [600/2534], Loss: 0.3812\n",
            "Epoch [52/100], Step [700/2534], Loss: 0.2474\n",
            "Epoch [52/100], Step [800/2534], Loss: 0.4069\n",
            "Epoch [52/100], Step [900/2534], Loss: 0.1970\n",
            "Epoch [52/100], Step [1000/2534], Loss: 0.2660\n",
            "Epoch [52/100], Step [1100/2534], Loss: 0.4192\n",
            "Epoch [52/100], Step [1200/2534], Loss: 0.2831\n",
            "Epoch [52/100], Step [1300/2534], Loss: 0.2802\n",
            "Epoch [52/100], Step [1400/2534], Loss: 0.2141\n",
            "Epoch [52/100], Step [1500/2534], Loss: 0.2115\n",
            "Epoch [52/100], Step [1600/2534], Loss: 0.3798\n",
            "Epoch [52/100], Step [1700/2534], Loss: 0.2011\n",
            "Epoch [52/100], Step [1800/2534], Loss: 0.2414\n",
            "Epoch [52/100], Step [1900/2534], Loss: 0.3446\n",
            "Epoch [52/100], Step [2000/2534], Loss: 0.4325\n",
            "Epoch [52/100], Step [2100/2534], Loss: 0.4470\n",
            "Epoch [52/100], Step [2200/2534], Loss: 0.3217\n",
            "Epoch [52/100], Step [2300/2534], Loss: 0.2256\n",
            "Epoch [52/100], Step [2400/2534], Loss: 0.1817\n",
            "Epoch [52/100], Step [2500/2534], Loss: 0.2221\n",
            "Epoch [52/100] Average Training Loss: 0.3110\n",
            "Epoch [53/100], Step [100/2534], Loss: 0.2137\n",
            "Epoch [53/100], Step [200/2534], Loss: 0.2559\n",
            "Epoch [53/100], Step [300/2534], Loss: 0.3590\n",
            "Epoch [53/100], Step [400/2534], Loss: 0.3606\n",
            "Epoch [53/100], Step [500/2534], Loss: 0.3556\n",
            "Epoch [53/100], Step [600/2534], Loss: 0.2340\n",
            "Epoch [53/100], Step [700/2534], Loss: 0.2489\n",
            "Epoch [53/100], Step [800/2534], Loss: 0.3808\n",
            "Epoch [53/100], Step [900/2534], Loss: 0.2838\n",
            "Epoch [53/100], Step [1000/2534], Loss: 0.2381\n",
            "Epoch [53/100], Step [1100/2534], Loss: 0.3008\n",
            "Epoch [53/100], Step [1200/2534], Loss: 0.4740\n",
            "Epoch [53/100], Step [1300/2534], Loss: 0.2320\n",
            "Epoch [53/100], Step [1400/2534], Loss: 0.2813\n",
            "Epoch [53/100], Step [1500/2534], Loss: 0.3295\n",
            "Epoch [53/100], Step [1600/2534], Loss: 0.4838\n",
            "Epoch [53/100], Step [1700/2534], Loss: 0.2490\n",
            "Epoch [53/100], Step [1800/2534], Loss: 0.3885\n",
            "Epoch [53/100], Step [1900/2534], Loss: 0.2449\n",
            "Epoch [53/100], Step [2000/2534], Loss: 0.4923\n",
            "Epoch [53/100], Step [2100/2534], Loss: 0.3014\n",
            "Epoch [53/100], Step [2200/2534], Loss: 0.4013\n",
            "Epoch [53/100], Step [2300/2534], Loss: 0.2541\n",
            "Epoch [53/100], Step [2400/2534], Loss: 0.2677\n",
            "Epoch [53/100], Step [2500/2534], Loss: 0.1473\n",
            "Epoch [53/100] Average Training Loss: 0.3087\n",
            "Epoch [54/100], Step [100/2534], Loss: 0.2996\n",
            "Epoch [54/100], Step [200/2534], Loss: 0.1376\n",
            "Epoch [54/100], Step [300/2534], Loss: 0.2393\n",
            "Epoch [54/100], Step [400/2534], Loss: 0.1941\n",
            "Epoch [54/100], Step [500/2534], Loss: 0.4108\n",
            "Epoch [54/100], Step [600/2534], Loss: 0.3469\n",
            "Epoch [54/100], Step [700/2534], Loss: 0.2138\n",
            "Epoch [54/100], Step [800/2534], Loss: 0.3158\n",
            "Epoch [54/100], Step [900/2534], Loss: 0.2672\n",
            "Epoch [54/100], Step [1000/2534], Loss: 0.2548\n",
            "Epoch [54/100], Step [1100/2534], Loss: 0.2927\n",
            "Epoch [54/100], Step [1200/2534], Loss: 0.2132\n",
            "Epoch [54/100], Step [1300/2534], Loss: 0.2637\n",
            "Epoch [54/100], Step [1400/2534], Loss: 0.4700\n",
            "Epoch [54/100], Step [1500/2534], Loss: 0.4147\n",
            "Epoch [54/100], Step [1600/2534], Loss: 0.3108\n",
            "Epoch [54/100], Step [1700/2534], Loss: 0.2995\n",
            "Epoch [54/100], Step [1800/2534], Loss: 0.5927\n",
            "Epoch [54/100], Step [1900/2534], Loss: 0.3120\n",
            "Epoch [54/100], Step [2000/2534], Loss: 0.4284\n",
            "Epoch [54/100], Step [2100/2534], Loss: 0.1546\n",
            "Epoch [54/100], Step [2200/2534], Loss: 0.3265\n",
            "Epoch [54/100], Step [2300/2534], Loss: 0.4158\n",
            "Epoch [54/100], Step [2400/2534], Loss: 0.3548\n",
            "Epoch [54/100], Step [2500/2534], Loss: 0.4182\n",
            "Epoch [54/100] Average Training Loss: 0.3078\n",
            "Epoch [55/100], Step [100/2534], Loss: 0.3399\n",
            "Epoch [55/100], Step [200/2534], Loss: 0.2038\n",
            "Epoch [55/100], Step [300/2534], Loss: 0.2940\n",
            "Epoch [55/100], Step [400/2534], Loss: 0.2432\n",
            "Epoch [55/100], Step [500/2534], Loss: 0.1835\n",
            "Epoch [55/100], Step [600/2534], Loss: 0.6157\n",
            "Epoch [55/100], Step [700/2534], Loss: 0.3106\n",
            "Epoch [55/100], Step [800/2534], Loss: 0.3897\n",
            "Epoch [55/100], Step [900/2534], Loss: 0.4136\n",
            "Epoch [55/100], Step [1000/2534], Loss: 0.2272\n",
            "Epoch [55/100], Step [1100/2534], Loss: 0.5375\n",
            "Epoch [55/100], Step [1200/2534], Loss: 0.4675\n",
            "Epoch [55/100], Step [1300/2534], Loss: 0.3168\n",
            "Epoch [55/100], Step [1400/2534], Loss: 0.4288\n",
            "Epoch [55/100], Step [1500/2534], Loss: 0.3113\n",
            "Epoch [55/100], Step [1600/2534], Loss: 0.4068\n",
            "Epoch [55/100], Step [1700/2534], Loss: 0.2424\n",
            "Epoch [55/100], Step [1800/2534], Loss: 0.4270\n",
            "Epoch [55/100], Step [1900/2534], Loss: 0.3701\n",
            "Epoch [55/100], Step [2000/2534], Loss: 0.2585\n",
            "Epoch [55/100], Step [2100/2534], Loss: 0.4972\n",
            "Epoch [55/100], Step [2200/2534], Loss: 0.4417\n",
            "Epoch [55/100], Step [2300/2534], Loss: 0.4989\n",
            "Epoch [55/100], Step [2400/2534], Loss: 0.2138\n",
            "Epoch [55/100], Step [2500/2534], Loss: 0.2284\n",
            "Epoch [55/100] Average Training Loss: 0.3089\n",
            "Epoch [56/100], Step [100/2534], Loss: 0.1107\n",
            "Epoch [56/100], Step [200/2534], Loss: 0.2506\n",
            "Epoch [56/100], Step [300/2534], Loss: 0.2601\n",
            "Epoch [56/100], Step [400/2534], Loss: 0.1027\n",
            "Epoch [56/100], Step [500/2534], Loss: 0.1302\n",
            "Epoch [56/100], Step [600/2534], Loss: 0.1725\n",
            "Epoch [56/100], Step [700/2534], Loss: 0.3194\n",
            "Epoch [56/100], Step [800/2534], Loss: 0.1347\n",
            "Epoch [56/100], Step [900/2534], Loss: 0.2831\n",
            "Epoch [56/100], Step [1000/2534], Loss: 0.3607\n",
            "Epoch [56/100], Step [1100/2534], Loss: 0.1164\n",
            "Epoch [56/100], Step [1200/2534], Loss: 0.3366\n",
            "Epoch [56/100], Step [1300/2534], Loss: 0.3277\n",
            "Epoch [56/100], Step [1400/2534], Loss: 0.3625\n",
            "Epoch [56/100], Step [1500/2534], Loss: 0.2122\n",
            "Epoch [56/100], Step [1600/2534], Loss: 0.2514\n",
            "Epoch [56/100], Step [1700/2534], Loss: 0.3977\n",
            "Epoch [56/100], Step [1800/2534], Loss: 0.3532\n",
            "Epoch [56/100], Step [1900/2534], Loss: 0.4306\n",
            "Epoch [56/100], Step [2000/2534], Loss: 0.2829\n",
            "Epoch [56/100], Step [2100/2534], Loss: 0.3834\n",
            "Epoch [56/100], Step [2200/2534], Loss: 0.1831\n",
            "Epoch [56/100], Step [2300/2534], Loss: 0.3810\n",
            "Epoch [56/100], Step [2400/2534], Loss: 0.4651\n",
            "Epoch [56/100], Step [2500/2534], Loss: 0.2702\n",
            "Epoch [56/100] Average Training Loss: 0.3051\n",
            "Epoch [57/100], Step [100/2534], Loss: 0.4477\n",
            "Epoch [57/100], Step [200/2534], Loss: 0.2580\n",
            "Epoch [57/100], Step [300/2534], Loss: 0.2115\n",
            "Epoch [57/100], Step [400/2534], Loss: 0.3536\n",
            "Epoch [57/100], Step [500/2534], Loss: 0.1959\n",
            "Epoch [57/100], Step [600/2534], Loss: 0.3116\n",
            "Epoch [57/100], Step [700/2534], Loss: 0.4043\n",
            "Epoch [57/100], Step [800/2534], Loss: 0.2091\n",
            "Epoch [57/100], Step [900/2534], Loss: 0.2890\n",
            "Epoch [57/100], Step [1000/2534], Loss: 0.1849\n",
            "Epoch [57/100], Step [1100/2534], Loss: 0.0893\n",
            "Epoch [57/100], Step [1200/2534], Loss: 0.3220\n",
            "Epoch [57/100], Step [1300/2534], Loss: 0.4028\n",
            "Epoch [57/100], Step [1400/2534], Loss: 0.3019\n",
            "Epoch [57/100], Step [1500/2534], Loss: 0.2000\n",
            "Epoch [57/100], Step [1600/2534], Loss: 0.3915\n",
            "Epoch [57/100], Step [1700/2534], Loss: 0.2439\n",
            "Epoch [57/100], Step [1800/2534], Loss: 0.2686\n",
            "Epoch [57/100], Step [1900/2534], Loss: 0.4580\n",
            "Epoch [57/100], Step [2000/2534], Loss: 0.3793\n",
            "Epoch [57/100], Step [2100/2534], Loss: 0.4490\n",
            "Epoch [57/100], Step [2200/2534], Loss: 0.4239\n",
            "Epoch [57/100], Step [2300/2534], Loss: 0.2766\n",
            "Epoch [57/100], Step [2400/2534], Loss: 0.3937\n",
            "Epoch [57/100], Step [2500/2534], Loss: 0.2312\n",
            "Epoch [57/100] Average Training Loss: 0.3074\n",
            "Epoch [58/100], Step [100/2534], Loss: 0.1314\n",
            "Epoch [58/100], Step [200/2534], Loss: 0.2167\n",
            "Epoch [58/100], Step [300/2534], Loss: 0.0900\n",
            "Epoch [58/100], Step [400/2534], Loss: 0.1483\n",
            "Epoch [58/100], Step [500/2534], Loss: 0.2220\n",
            "Epoch [58/100], Step [600/2534], Loss: 0.2215\n",
            "Epoch [58/100], Step [700/2534], Loss: 0.3046\n",
            "Epoch [58/100], Step [800/2534], Loss: 0.2754\n",
            "Epoch [58/100], Step [900/2534], Loss: 0.2205\n",
            "Epoch [58/100], Step [1000/2534], Loss: 0.4097\n",
            "Epoch [58/100], Step [1100/2534], Loss: 0.4100\n",
            "Epoch [58/100], Step [1200/2534], Loss: 0.3533\n",
            "Epoch [58/100], Step [1300/2534], Loss: 0.2801\n",
            "Epoch [58/100], Step [1400/2534], Loss: 0.5381\n",
            "Epoch [58/100], Step [1500/2534], Loss: 0.2639\n",
            "Epoch [58/100], Step [1600/2534], Loss: 0.3533\n",
            "Epoch [58/100], Step [1700/2534], Loss: 0.3710\n",
            "Epoch [58/100], Step [1800/2534], Loss: 0.4495\n",
            "Epoch [58/100], Step [1900/2534], Loss: 0.1175\n",
            "Epoch [58/100], Step [2000/2534], Loss: 0.5679\n",
            "Epoch [58/100], Step [2100/2534], Loss: 0.4438\n",
            "Epoch [58/100], Step [2200/2534], Loss: 0.3536\n",
            "Epoch [58/100], Step [2300/2534], Loss: 0.3092\n",
            "Epoch [58/100], Step [2400/2534], Loss: 0.2499\n",
            "Epoch [58/100], Step [2500/2534], Loss: 0.3209\n",
            "Epoch [58/100] Average Training Loss: 0.3052\n",
            "Epoch [59/100], Step [100/2534], Loss: 0.1129\n",
            "Epoch [59/100], Step [200/2534], Loss: 0.3585\n",
            "Epoch [59/100], Step [300/2534], Loss: 0.3072\n",
            "Epoch [59/100], Step [400/2534], Loss: 0.1758\n",
            "Epoch [59/100], Step [500/2534], Loss: 0.3442\n",
            "Epoch [59/100], Step [600/2534], Loss: 0.2128\n",
            "Epoch [59/100], Step [700/2534], Loss: 0.2895\n",
            "Epoch [59/100], Step [800/2534], Loss: 0.1988\n",
            "Epoch [59/100], Step [900/2534], Loss: 0.3488\n",
            "Epoch [59/100], Step [1000/2534], Loss: 0.2434\n",
            "Epoch [59/100], Step [1100/2534], Loss: 0.3627\n",
            "Epoch [59/100], Step [1200/2534], Loss: 0.1641\n",
            "Epoch [59/100], Step [1300/2534], Loss: 0.2056\n",
            "Epoch [59/100], Step [1400/2534], Loss: 0.3793\n",
            "Epoch [59/100], Step [1500/2534], Loss: 0.1068\n",
            "Epoch [59/100], Step [1600/2534], Loss: 0.5393\n",
            "Epoch [59/100], Step [1700/2534], Loss: 0.3026\n",
            "Epoch [59/100], Step [1800/2534], Loss: 0.4684\n",
            "Epoch [59/100], Step [1900/2534], Loss: 0.4605\n",
            "Epoch [59/100], Step [2000/2534], Loss: 0.3661\n",
            "Epoch [59/100], Step [2100/2534], Loss: 0.3684\n",
            "Epoch [59/100], Step [2200/2534], Loss: 0.2827\n",
            "Epoch [59/100], Step [2300/2534], Loss: 0.1314\n",
            "Epoch [59/100], Step [2400/2534], Loss: 0.3053\n",
            "Epoch [59/100], Step [2500/2534], Loss: 0.2704\n",
            "Epoch [59/100] Average Training Loss: 0.3042\n",
            "Epoch [60/100], Step [100/2534], Loss: 0.2091\n",
            "Epoch [60/100], Step [200/2534], Loss: 0.2471\n",
            "Epoch [60/100], Step [300/2534], Loss: 0.2513\n",
            "Epoch [60/100], Step [400/2534], Loss: 0.6137\n",
            "Epoch [60/100], Step [500/2534], Loss: 0.4840\n",
            "Epoch [60/100], Step [600/2534], Loss: 0.1654\n",
            "Epoch [60/100], Step [700/2534], Loss: 0.2279\n",
            "Epoch [60/100], Step [800/2534], Loss: 0.3852\n",
            "Epoch [60/100], Step [900/2534], Loss: 0.3725\n",
            "Epoch [60/100], Step [1000/2534], Loss: 0.2872\n",
            "Epoch [60/100], Step [1100/2534], Loss: 0.3093\n",
            "Epoch [60/100], Step [1200/2534], Loss: 0.2706\n",
            "Epoch [60/100], Step [1300/2534], Loss: 0.2249\n",
            "Epoch [60/100], Step [1400/2534], Loss: 0.4742\n",
            "Epoch [60/100], Step [1500/2534], Loss: 0.3749\n",
            "Epoch [60/100], Step [1600/2534], Loss: 0.2819\n",
            "Epoch [60/100], Step [1700/2534], Loss: 0.3409\n",
            "Epoch [60/100], Step [1800/2534], Loss: 0.1404\n",
            "Epoch [60/100], Step [1900/2534], Loss: 0.4099\n",
            "Epoch [60/100], Step [2000/2534], Loss: 0.3897\n",
            "Epoch [60/100], Step [2100/2534], Loss: 0.1741\n",
            "Epoch [60/100], Step [2200/2534], Loss: 0.3798\n",
            "Epoch [60/100], Step [2300/2534], Loss: 0.4022\n",
            "Epoch [60/100], Step [2400/2534], Loss: 0.3991\n",
            "Epoch [60/100], Step [2500/2534], Loss: 0.2529\n",
            "Epoch [60/100] Average Training Loss: 0.3058\n",
            "Epoch [61/100], Step [100/2534], Loss: 0.1757\n",
            "Epoch [61/100], Step [200/2534], Loss: 0.2883\n",
            "Epoch [61/100], Step [300/2534], Loss: 0.2659\n",
            "Epoch [61/100], Step [400/2534], Loss: 0.1753\n",
            "Epoch [61/100], Step [500/2534], Loss: 0.2821\n",
            "Epoch [61/100], Step [600/2534], Loss: 0.1626\n",
            "Epoch [61/100], Step [700/2534], Loss: 0.2663\n",
            "Epoch [61/100], Step [800/2534], Loss: 0.2222\n",
            "Epoch [61/100], Step [900/2534], Loss: 0.2756\n",
            "Epoch [61/100], Step [1000/2534], Loss: 0.2978\n",
            "Epoch [61/100], Step [1100/2534], Loss: 0.3608\n",
            "Epoch [61/100], Step [1200/2534], Loss: 0.3102\n",
            "Epoch [61/100], Step [1300/2534], Loss: 0.4286\n",
            "Epoch [61/100], Step [1400/2534], Loss: 0.2795\n",
            "Epoch [61/100], Step [1500/2534], Loss: 0.2347\n",
            "Epoch [61/100], Step [1600/2534], Loss: 0.5807\n",
            "Epoch [61/100], Step [1700/2534], Loss: 0.3775\n",
            "Epoch [61/100], Step [1800/2534], Loss: 0.3063\n",
            "Epoch [61/100], Step [1900/2534], Loss: 0.3853\n",
            "Epoch [61/100], Step [2000/2534], Loss: 0.2709\n",
            "Epoch [61/100], Step [2100/2534], Loss: 0.2268\n",
            "Epoch [61/100], Step [2200/2534], Loss: 0.3261\n",
            "Epoch [61/100], Step [2300/2534], Loss: 0.4585\n",
            "Epoch [61/100], Step [2400/2534], Loss: 0.2838\n",
            "Epoch [61/100], Step [2500/2534], Loss: 0.2535\n",
            "Epoch [61/100] Average Training Loss: 0.3019\n",
            "Epoch [62/100], Step [100/2534], Loss: 0.1658\n",
            "Epoch [62/100], Step [200/2534], Loss: 0.3983\n",
            "Epoch [62/100], Step [300/2534], Loss: 0.1709\n",
            "Epoch [62/100], Step [400/2534], Loss: 0.2389\n",
            "Epoch [62/100], Step [500/2534], Loss: 0.1925\n",
            "Epoch [62/100], Step [600/2534], Loss: 0.3109\n",
            "Epoch [62/100], Step [700/2534], Loss: 0.2471\n",
            "Epoch [62/100], Step [800/2534], Loss: 0.3155\n",
            "Epoch [62/100], Step [900/2534], Loss: 0.3512\n",
            "Epoch [62/100], Step [1000/2534], Loss: 0.1996\n",
            "Epoch [62/100], Step [1100/2534], Loss: 0.2588\n",
            "Epoch [62/100], Step [1200/2534], Loss: 0.4271\n",
            "Epoch [62/100], Step [1300/2534], Loss: 0.3625\n",
            "Epoch [62/100], Step [1400/2534], Loss: 0.2021\n",
            "Epoch [62/100], Step [1500/2534], Loss: 0.2389\n",
            "Epoch [62/100], Step [1600/2534], Loss: 0.2593\n",
            "Epoch [62/100], Step [1700/2534], Loss: 0.3460\n",
            "Epoch [62/100], Step [1800/2534], Loss: 0.6686\n",
            "Epoch [62/100], Step [1900/2534], Loss: 0.4811\n",
            "Epoch [62/100], Step [2000/2534], Loss: 0.6161\n",
            "Epoch [62/100], Step [2100/2534], Loss: 0.5076\n",
            "Epoch [62/100], Step [2200/2534], Loss: 0.4309\n",
            "Epoch [62/100], Step [2300/2534], Loss: 0.4221\n",
            "Epoch [62/100], Step [2400/2534], Loss: 0.4540\n",
            "Epoch [62/100], Step [2500/2534], Loss: 0.5113\n",
            "Epoch [62/100] Average Training Loss: 0.3041\n",
            "Epoch [63/100], Step [100/2534], Loss: 0.2291\n",
            "Epoch [63/100], Step [200/2534], Loss: 0.0850\n",
            "Epoch [63/100], Step [300/2534], Loss: 0.2578\n",
            "Epoch [63/100], Step [400/2534], Loss: 0.1769\n",
            "Epoch [63/100], Step [500/2534], Loss: 0.2165\n",
            "Epoch [63/100], Step [600/2534], Loss: 0.2395\n",
            "Epoch [63/100], Step [700/2534], Loss: 0.1568\n",
            "Epoch [63/100], Step [800/2534], Loss: 0.4419\n",
            "Epoch [63/100], Step [900/2534], Loss: 0.2380\n",
            "Epoch [63/100], Step [1000/2534], Loss: 0.5483\n",
            "Epoch [63/100], Step [1100/2534], Loss: 0.3370\n",
            "Epoch [63/100], Step [1200/2534], Loss: 0.3601\n",
            "Epoch [63/100], Step [1300/2534], Loss: 0.4518\n",
            "Epoch [63/100], Step [1400/2534], Loss: 0.5122\n",
            "Epoch [63/100], Step [1500/2534], Loss: 0.2639\n",
            "Epoch [63/100], Step [1600/2534], Loss: 0.4240\n",
            "Epoch [63/100], Step [1700/2534], Loss: 0.2803\n",
            "Epoch [63/100], Step [1800/2534], Loss: 0.3190\n",
            "Epoch [63/100], Step [1900/2534], Loss: 0.3697\n",
            "Epoch [63/100], Step [2000/2534], Loss: 0.2307\n",
            "Epoch [63/100], Step [2100/2534], Loss: 0.2643\n",
            "Epoch [63/100], Step [2200/2534], Loss: 0.1623\n",
            "Epoch [63/100], Step [2300/2534], Loss: 0.3155\n",
            "Epoch [63/100], Step [2400/2534], Loss: 0.5440\n",
            "Epoch [63/100], Step [2500/2534], Loss: 0.5274\n",
            "Epoch [63/100] Average Training Loss: 0.3008\n",
            "Epoch [64/100], Step [100/2534], Loss: 0.3704\n",
            "Epoch [64/100], Step [200/2534], Loss: 0.2426\n",
            "Epoch [64/100], Step [300/2534], Loss: 0.2103\n",
            "Epoch [64/100], Step [400/2534], Loss: 0.2085\n",
            "Epoch [64/100], Step [500/2534], Loss: 0.1586\n",
            "Epoch [64/100], Step [600/2534], Loss: 0.3796\n",
            "Epoch [64/100], Step [700/2534], Loss: 0.4951\n",
            "Epoch [64/100], Step [800/2534], Loss: 0.2238\n",
            "Epoch [64/100], Step [900/2534], Loss: 0.2421\n",
            "Epoch [64/100], Step [1000/2534], Loss: 0.1816\n",
            "Epoch [64/100], Step [1100/2534], Loss: 0.3189\n",
            "Epoch [64/100], Step [1200/2534], Loss: 0.1769\n",
            "Epoch [64/100], Step [1300/2534], Loss: 0.2953\n",
            "Epoch [64/100], Step [1400/2534], Loss: 0.5090\n",
            "Epoch [64/100], Step [1500/2534], Loss: 0.4797\n",
            "Epoch [64/100], Step [1600/2534], Loss: 0.3096\n",
            "Epoch [64/100], Step [1700/2534], Loss: 0.2165\n",
            "Epoch [64/100], Step [1800/2534], Loss: 0.3138\n",
            "Epoch [64/100], Step [1900/2534], Loss: 0.5252\n",
            "Epoch [64/100], Step [2000/2534], Loss: 0.4861\n",
            "Epoch [64/100], Step [2100/2534], Loss: 0.3025\n",
            "Epoch [64/100], Step [2200/2534], Loss: 0.4653\n",
            "Epoch [64/100], Step [2300/2534], Loss: 0.3049\n",
            "Epoch [64/100], Step [2400/2534], Loss: 0.5062\n",
            "Epoch [64/100], Step [2500/2534], Loss: 0.4265\n",
            "Epoch [64/100] Average Training Loss: 0.3026\n",
            "Epoch [65/100], Step [100/2534], Loss: 0.1741\n",
            "Epoch [65/100], Step [200/2534], Loss: 0.1618\n",
            "Epoch [65/100], Step [300/2534], Loss: 0.1552\n",
            "Epoch [65/100], Step [400/2534], Loss: 0.3981\n",
            "Epoch [65/100], Step [500/2534], Loss: 0.2095\n",
            "Epoch [65/100], Step [600/2534], Loss: 0.2273\n",
            "Epoch [65/100], Step [700/2534], Loss: 0.2031\n",
            "Epoch [65/100], Step [800/2534], Loss: 0.3789\n",
            "Epoch [65/100], Step [900/2534], Loss: 0.2228\n",
            "Epoch [65/100], Step [1000/2534], Loss: 0.2080\n",
            "Epoch [65/100], Step [1100/2534], Loss: 0.2497\n",
            "Epoch [65/100], Step [1200/2534], Loss: 0.5125\n",
            "Epoch [65/100], Step [1300/2534], Loss: 0.1013\n",
            "Epoch [65/100], Step [1400/2534], Loss: 0.1885\n",
            "Epoch [65/100], Step [1500/2534], Loss: 0.2451\n",
            "Epoch [65/100], Step [1600/2534], Loss: 0.3254\n",
            "Epoch [65/100], Step [1700/2534], Loss: 0.2212\n",
            "Epoch [65/100], Step [1800/2534], Loss: 0.2882\n",
            "Epoch [65/100], Step [1900/2534], Loss: 0.2303\n",
            "Epoch [65/100], Step [2000/2534], Loss: 0.3400\n",
            "Epoch [65/100], Step [2100/2534], Loss: 0.3303\n",
            "Epoch [65/100], Step [2200/2534], Loss: 0.2106\n",
            "Epoch [65/100], Step [2300/2534], Loss: 0.3936\n",
            "Epoch [65/100], Step [2400/2534], Loss: 0.5059\n",
            "Epoch [65/100], Step [2500/2534], Loss: 0.3032\n",
            "Epoch [65/100] Average Training Loss: 0.3017\n",
            "Epoch [66/100], Step [100/2534], Loss: 0.2749\n",
            "Epoch [66/100], Step [200/2534], Loss: 0.4374\n",
            "Epoch [66/100], Step [300/2534], Loss: 0.1638\n",
            "Epoch [66/100], Step [400/2534], Loss: 0.1692\n",
            "Epoch [66/100], Step [500/2534], Loss: 0.0783\n",
            "Epoch [66/100], Step [600/2534], Loss: 0.3040\n",
            "Epoch [66/100], Step [700/2534], Loss: 0.2573\n",
            "Epoch [66/100], Step [800/2534], Loss: 0.2971\n",
            "Epoch [66/100], Step [900/2534], Loss: 0.2071\n",
            "Epoch [66/100], Step [1000/2534], Loss: 0.1315\n",
            "Epoch [66/100], Step [1100/2534], Loss: 0.4142\n",
            "Epoch [66/100], Step [1200/2534], Loss: 0.2268\n",
            "Epoch [66/100], Step [1300/2534], Loss: 0.3824\n",
            "Epoch [66/100], Step [1400/2534], Loss: 0.1711\n",
            "Epoch [66/100], Step [1500/2534], Loss: 0.3632\n",
            "Epoch [66/100], Step [1600/2534], Loss: 0.3300\n",
            "Epoch [66/100], Step [1700/2534], Loss: 0.4166\n",
            "Epoch [66/100], Step [1800/2534], Loss: 0.2698\n",
            "Epoch [66/100], Step [1900/2534], Loss: 0.4149\n",
            "Epoch [66/100], Step [2000/2534], Loss: 0.2251\n",
            "Epoch [66/100], Step [2100/2534], Loss: 0.4696\n",
            "Epoch [66/100], Step [2200/2534], Loss: 0.2621\n",
            "Epoch [66/100], Step [2300/2534], Loss: 0.3930\n",
            "Epoch [66/100], Step [2400/2534], Loss: 0.2908\n",
            "Epoch [66/100], Step [2500/2534], Loss: 0.2760\n",
            "Epoch [66/100] Average Training Loss: 0.3010\n",
            "Epoch [67/100], Step [100/2534], Loss: 0.2299\n",
            "Epoch [67/100], Step [200/2534], Loss: 0.0731\n",
            "Epoch [67/100], Step [300/2534], Loss: 0.1293\n",
            "Epoch [67/100], Step [400/2534], Loss: 0.2022\n",
            "Epoch [67/100], Step [500/2534], Loss: 0.1414\n",
            "Epoch [67/100], Step [600/2534], Loss: 0.2369\n",
            "Epoch [67/100], Step [700/2534], Loss: 0.1795\n",
            "Epoch [67/100], Step [800/2534], Loss: 0.3203\n",
            "Epoch [67/100], Step [900/2534], Loss: 0.1702\n",
            "Epoch [67/100], Step [1000/2534], Loss: 0.3726\n",
            "Epoch [67/100], Step [1100/2534], Loss: 0.1424\n",
            "Epoch [67/100], Step [1200/2534], Loss: 0.3311\n",
            "Epoch [67/100], Step [1300/2534], Loss: 0.2048\n",
            "Epoch [67/100], Step [1400/2534], Loss: 0.2317\n",
            "Epoch [67/100], Step [1500/2534], Loss: 0.3735\n",
            "Epoch [67/100], Step [1600/2534], Loss: 0.3149\n",
            "Epoch [67/100], Step [1700/2534], Loss: 0.2705\n",
            "Epoch [67/100], Step [1800/2534], Loss: 0.3052\n",
            "Epoch [67/100], Step [1900/2534], Loss: 0.3681\n",
            "Epoch [67/100], Step [2000/2534], Loss: 0.2995\n",
            "Epoch [67/100], Step [2100/2534], Loss: 0.4619\n",
            "Epoch [67/100], Step [2200/2534], Loss: 0.4176\n",
            "Epoch [67/100], Step [2300/2534], Loss: 0.4064\n",
            "Epoch [67/100], Step [2400/2534], Loss: 0.3180\n",
            "Epoch [67/100], Step [2500/2534], Loss: 0.3894\n",
            "Epoch [67/100] Average Training Loss: 0.2998\n",
            "Epoch [68/100], Step [100/2534], Loss: 0.2272\n",
            "Epoch [68/100], Step [200/2534], Loss: 0.3240\n",
            "Epoch [68/100], Step [300/2534], Loss: 0.2920\n",
            "Epoch [68/100], Step [400/2534], Loss: 0.2146\n",
            "Epoch [68/100], Step [500/2534], Loss: 0.2520\n",
            "Epoch [68/100], Step [600/2534], Loss: 0.2708\n",
            "Epoch [68/100], Step [700/2534], Loss: 0.2828\n",
            "Epoch [68/100], Step [800/2534], Loss: 0.2772\n",
            "Epoch [68/100], Step [900/2534], Loss: 0.0979\n",
            "Epoch [68/100], Step [1000/2534], Loss: 0.3365\n",
            "Epoch [68/100], Step [1100/2534], Loss: 0.1491\n",
            "Epoch [68/100], Step [1200/2534], Loss: 0.1572\n",
            "Epoch [68/100], Step [1300/2534], Loss: 0.2057\n",
            "Epoch [68/100], Step [1400/2534], Loss: 0.3768\n",
            "Epoch [68/100], Step [1500/2534], Loss: 0.3398\n",
            "Epoch [68/100], Step [1600/2534], Loss: 0.2860\n",
            "Epoch [68/100], Step [1700/2534], Loss: 0.2325\n",
            "Epoch [68/100], Step [1800/2534], Loss: 0.3998\n",
            "Epoch [68/100], Step [1900/2534], Loss: 0.3486\n",
            "Epoch [68/100], Step [2000/2534], Loss: 0.3318\n",
            "Epoch [68/100], Step [2100/2534], Loss: 0.2714\n",
            "Epoch [68/100], Step [2200/2534], Loss: 0.4147\n",
            "Epoch [68/100], Step [2300/2534], Loss: 0.2229\n",
            "Epoch [68/100], Step [2400/2534], Loss: 0.2371\n",
            "Epoch [68/100], Step [2500/2534], Loss: 0.4637\n",
            "Epoch [68/100] Average Training Loss: 0.3012\n",
            "Epoch [69/100], Step [100/2534], Loss: 0.4476\n",
            "Epoch [69/100], Step [200/2534], Loss: 0.4048\n",
            "Epoch [69/100], Step [300/2534], Loss: 0.1577\n",
            "Epoch [69/100], Step [400/2534], Loss: 0.1877\n",
            "Epoch [69/100], Step [500/2534], Loss: 0.2982\n",
            "Epoch [69/100], Step [600/2534], Loss: 0.1561\n",
            "Epoch [69/100], Step [700/2534], Loss: 0.1668\n",
            "Epoch [69/100], Step [800/2534], Loss: 0.2529\n",
            "Epoch [69/100], Step [900/2534], Loss: 0.2709\n",
            "Epoch [69/100], Step [1000/2534], Loss: 0.3603\n",
            "Epoch [69/100], Step [1100/2534], Loss: 0.1100\n",
            "Epoch [69/100], Step [1200/2534], Loss: 0.3039\n",
            "Epoch [69/100], Step [1300/2534], Loss: 0.2060\n",
            "Epoch [69/100], Step [1400/2534], Loss: 0.3058\n",
            "Epoch [69/100], Step [1500/2534], Loss: 0.2520\n",
            "Epoch [69/100], Step [1600/2534], Loss: 0.4411\n",
            "Epoch [69/100], Step [1700/2534], Loss: 0.4604\n",
            "Epoch [69/100], Step [1800/2534], Loss: 0.3457\n",
            "Epoch [69/100], Step [1900/2534], Loss: 0.4103\n",
            "Epoch [69/100], Step [2000/2534], Loss: 0.3412\n",
            "Epoch [69/100], Step [2100/2534], Loss: 0.2549\n",
            "Epoch [69/100], Step [2200/2534], Loss: 0.2957\n",
            "Epoch [69/100], Step [2300/2534], Loss: 0.3281\n",
            "Epoch [69/100], Step [2400/2534], Loss: 0.1571\n",
            "Epoch [69/100], Step [2500/2534], Loss: 0.2784\n",
            "Epoch [69/100] Average Training Loss: 0.3002\n",
            "Epoch [70/100], Step [100/2534], Loss: 0.3367\n",
            "Epoch [70/100], Step [200/2534], Loss: 0.3041\n",
            "Epoch [70/100], Step [300/2534], Loss: 0.2564\n",
            "Epoch [70/100], Step [400/2534], Loss: 0.1951\n",
            "Epoch [70/100], Step [500/2534], Loss: 0.2412\n",
            "Epoch [70/100], Step [600/2534], Loss: 0.3628\n",
            "Epoch [70/100], Step [700/2534], Loss: 0.2009\n",
            "Epoch [70/100], Step [800/2534], Loss: 0.1685\n",
            "Epoch [70/100], Step [900/2534], Loss: 0.3104\n",
            "Epoch [70/100], Step [1000/2534], Loss: 0.2477\n",
            "Epoch [70/100], Step [1100/2534], Loss: 0.2985\n",
            "Epoch [70/100], Step [1200/2534], Loss: 0.2717\n",
            "Epoch [70/100], Step [1300/2534], Loss: 0.3714\n",
            "Epoch [70/100], Step [1400/2534], Loss: 0.4262\n",
            "Epoch [70/100], Step [1500/2534], Loss: 0.2409\n",
            "Epoch [70/100], Step [1600/2534], Loss: 0.4876\n",
            "Epoch [70/100], Step [1700/2534], Loss: 0.2072\n",
            "Epoch [70/100], Step [1800/2534], Loss: 0.4375\n",
            "Epoch [70/100], Step [1900/2534], Loss: 0.3948\n",
            "Epoch [70/100], Step [2000/2534], Loss: 0.2209\n",
            "Epoch [70/100], Step [2100/2534], Loss: 0.2108\n",
            "Epoch [70/100], Step [2200/2534], Loss: 0.4275\n",
            "Epoch [70/100], Step [2300/2534], Loss: 0.3182\n",
            "Epoch [70/100], Step [2400/2534], Loss: 0.3778\n",
            "Epoch [70/100], Step [2500/2534], Loss: 0.3990\n",
            "Epoch [70/100] Average Training Loss: 0.2993\n",
            "Epoch [71/100], Step [100/2534], Loss: 0.0569\n",
            "Epoch [71/100], Step [200/2534], Loss: 0.2357\n",
            "Epoch [71/100], Step [300/2534], Loss: 0.1678\n",
            "Epoch [71/100], Step [400/2534], Loss: 0.3289\n",
            "Epoch [71/100], Step [500/2534], Loss: 0.1826\n",
            "Epoch [71/100], Step [600/2534], Loss: 0.2378\n",
            "Epoch [71/100], Step [700/2534], Loss: 0.0670\n",
            "Epoch [71/100], Step [800/2534], Loss: 0.2950\n",
            "Epoch [71/100], Step [900/2534], Loss: 0.3497\n",
            "Epoch [71/100], Step [1000/2534], Loss: 0.2721\n",
            "Epoch [71/100], Step [1100/2534], Loss: 0.2286\n",
            "Epoch [71/100], Step [1200/2534], Loss: 0.1009\n",
            "Epoch [71/100], Step [1300/2534], Loss: 0.4550\n",
            "Epoch [71/100], Step [1400/2534], Loss: 0.2774\n",
            "Epoch [71/100], Step [1500/2534], Loss: 0.3176\n",
            "Epoch [71/100], Step [1600/2534], Loss: 0.2087\n",
            "Epoch [71/100], Step [1700/2534], Loss: 0.2276\n",
            "Epoch [71/100], Step [1800/2534], Loss: 0.4848\n",
            "Epoch [71/100], Step [1900/2534], Loss: 0.2770\n",
            "Epoch [71/100], Step [2000/2534], Loss: 0.5206\n",
            "Epoch [71/100], Step [2100/2534], Loss: 0.3932\n",
            "Epoch [71/100], Step [2200/2534], Loss: 0.2989\n",
            "Epoch [71/100], Step [2300/2534], Loss: 0.5109\n",
            "Epoch [71/100], Step [2400/2534], Loss: 0.1878\n",
            "Epoch [71/100], Step [2500/2534], Loss: 0.3872\n",
            "Epoch [71/100] Average Training Loss: 0.2996\n",
            "Epoch [72/100], Step [100/2534], Loss: 0.3415\n",
            "Epoch [72/100], Step [200/2534], Loss: 0.1534\n",
            "Epoch [72/100], Step [300/2534], Loss: 0.2108\n",
            "Epoch [72/100], Step [400/2534], Loss: 0.1693\n",
            "Epoch [72/100], Step [500/2534], Loss: 0.3048\n",
            "Epoch [72/100], Step [600/2534], Loss: 0.2329\n",
            "Epoch [72/100], Step [700/2534], Loss: 0.5103\n",
            "Epoch [72/100], Step [800/2534], Loss: 0.2605\n",
            "Epoch [72/100], Step [900/2534], Loss: 0.2886\n",
            "Epoch [72/100], Step [1000/2534], Loss: 0.1663\n",
            "Epoch [72/100], Step [1100/2534], Loss: 0.3320\n",
            "Epoch [72/100], Step [1200/2534], Loss: 0.1828\n",
            "Epoch [72/100], Step [1300/2534], Loss: 0.3468\n",
            "Epoch [72/100], Step [1400/2534], Loss: 0.2342\n",
            "Epoch [72/100], Step [1500/2534], Loss: 0.3721\n",
            "Epoch [72/100], Step [1600/2534], Loss: 0.2933\n",
            "Epoch [72/100], Step [1700/2534], Loss: 0.3838\n",
            "Epoch [72/100], Step [1800/2534], Loss: 0.2968\n",
            "Epoch [72/100], Step [1900/2534], Loss: 0.1561\n",
            "Epoch [72/100], Step [2000/2534], Loss: 0.2938\n",
            "Epoch [72/100], Step [2100/2534], Loss: 0.3989\n",
            "Epoch [72/100], Step [2200/2534], Loss: 0.3576\n",
            "Epoch [72/100], Step [2300/2534], Loss: 0.2983\n",
            "Epoch [72/100], Step [2400/2534], Loss: 0.2904\n",
            "Epoch [72/100], Step [2500/2534], Loss: 0.5367\n",
            "Epoch [72/100] Average Training Loss: 0.2975\n",
            "Epoch [73/100], Step [100/2534], Loss: 0.1823\n",
            "Epoch [73/100], Step [200/2534], Loss: 0.3922\n",
            "Epoch [73/100], Step [300/2534], Loss: 0.2258\n",
            "Epoch [73/100], Step [400/2534], Loss: 0.2824\n",
            "Epoch [73/100], Step [500/2534], Loss: 0.2157\n",
            "Epoch [73/100], Step [600/2534], Loss: 0.2255\n",
            "Epoch [73/100], Step [700/2534], Loss: 0.3312\n",
            "Epoch [73/100], Step [800/2534], Loss: 0.3943\n",
            "Epoch [73/100], Step [900/2534], Loss: 0.2315\n",
            "Epoch [73/100], Step [1000/2534], Loss: 0.6135\n",
            "Epoch [73/100], Step [1100/2534], Loss: 0.3672\n",
            "Epoch [73/100], Step [1200/2534], Loss: 0.1518\n",
            "Epoch [73/100], Step [1300/2534], Loss: 0.3059\n",
            "Epoch [73/100], Step [1400/2534], Loss: 0.2500\n",
            "Epoch [73/100], Step [1500/2534], Loss: 0.1679\n",
            "Epoch [73/100], Step [1600/2534], Loss: 0.2578\n",
            "Epoch [73/100], Step [1700/2534], Loss: 0.2775\n",
            "Epoch [73/100], Step [1800/2534], Loss: 0.2526\n",
            "Epoch [73/100], Step [1900/2534], Loss: 0.2342\n",
            "Epoch [73/100], Step [2000/2534], Loss: 0.1957\n",
            "Epoch [73/100], Step [2100/2534], Loss: 0.2066\n",
            "Epoch [73/100], Step [2200/2534], Loss: 0.3449\n",
            "Epoch [73/100], Step [2300/2534], Loss: 0.4133\n",
            "Epoch [73/100], Step [2400/2534], Loss: 0.1698\n",
            "Epoch [73/100], Step [2500/2534], Loss: 0.2867\n",
            "Epoch [73/100] Average Training Loss: 0.2979\n",
            "Epoch [74/100], Step [100/2534], Loss: 0.2259\n",
            "Epoch [74/100], Step [200/2534], Loss: 0.0566\n",
            "Epoch [74/100], Step [300/2534], Loss: 0.1111\n",
            "Epoch [74/100], Step [400/2534], Loss: 0.3581\n",
            "Epoch [74/100], Step [500/2534], Loss: 0.2483\n",
            "Epoch [74/100], Step [600/2534], Loss: 0.2787\n",
            "Epoch [74/100], Step [700/2534], Loss: 0.2631\n",
            "Epoch [74/100], Step [800/2534], Loss: 0.2755\n",
            "Epoch [74/100], Step [900/2534], Loss: 0.1743\n",
            "Epoch [74/100], Step [1000/2534], Loss: 0.3594\n",
            "Epoch [74/100], Step [1100/2534], Loss: 0.1864\n",
            "Epoch [74/100], Step [1200/2534], Loss: 0.2505\n",
            "Epoch [74/100], Step [1300/2534], Loss: 0.3737\n",
            "Epoch [74/100], Step [1400/2534], Loss: 0.1598\n",
            "Epoch [74/100], Step [1500/2534], Loss: 0.3109\n",
            "Epoch [74/100], Step [1600/2534], Loss: 0.7876\n",
            "Epoch [74/100], Step [1700/2534], Loss: 0.4214\n",
            "Epoch [74/100], Step [1800/2534], Loss: 0.2202\n",
            "Epoch [74/100], Step [1900/2534], Loss: 0.2776\n",
            "Epoch [74/100], Step [2000/2534], Loss: 0.5809\n",
            "Epoch [74/100], Step [2100/2534], Loss: 0.3940\n",
            "Epoch [74/100], Step [2200/2534], Loss: 0.4344\n",
            "Epoch [74/100], Step [2300/2534], Loss: 0.4060\n",
            "Epoch [74/100], Step [2400/2534], Loss: 0.5176\n",
            "Epoch [74/100], Step [2500/2534], Loss: 0.3991\n",
            "Epoch [74/100] Average Training Loss: 0.2980\n",
            "Epoch [75/100], Step [100/2534], Loss: 0.2496\n",
            "Epoch [75/100], Step [200/2534], Loss: 0.2234\n",
            "Epoch [75/100], Step [300/2534], Loss: 0.1393\n",
            "Epoch [75/100], Step [400/2534], Loss: 0.1949\n",
            "Epoch [75/100], Step [500/2534], Loss: 0.3795\n",
            "Epoch [75/100], Step [600/2534], Loss: 0.2972\n",
            "Epoch [75/100], Step [700/2534], Loss: 0.6022\n",
            "Epoch [75/100], Step [800/2534], Loss: 0.2702\n",
            "Epoch [75/100], Step [900/2534], Loss: 0.1143\n",
            "Epoch [75/100], Step [1000/2534], Loss: 0.3212\n",
            "Epoch [75/100], Step [1100/2534], Loss: 0.2339\n",
            "Epoch [75/100], Step [1200/2534], Loss: 0.1202\n",
            "Epoch [75/100], Step [1300/2534], Loss: 0.2708\n",
            "Epoch [75/100], Step [1400/2534], Loss: 0.2532\n",
            "Epoch [75/100], Step [1500/2534], Loss: 0.1274\n",
            "Epoch [75/100], Step [1600/2534], Loss: 0.2671\n",
            "Epoch [75/100], Step [1700/2534], Loss: 0.2757\n",
            "Epoch [75/100], Step [1800/2534], Loss: 0.2596\n",
            "Epoch [75/100], Step [1900/2534], Loss: 0.3716\n",
            "Epoch [75/100], Step [2000/2534], Loss: 0.2051\n",
            "Epoch [75/100], Step [2100/2534], Loss: 0.2404\n",
            "Epoch [75/100], Step [2200/2534], Loss: 0.3377\n",
            "Epoch [75/100], Step [2300/2534], Loss: 0.5781\n",
            "Epoch [75/100], Step [2400/2534], Loss: 0.3240\n",
            "Epoch [75/100], Step [2500/2534], Loss: 0.2606\n",
            "Epoch [75/100] Average Training Loss: 0.2969\n",
            "Epoch [76/100], Step [100/2534], Loss: 0.2667\n",
            "Epoch [76/100], Step [200/2534], Loss: 0.3389\n",
            "Epoch [76/100], Step [300/2534], Loss: 0.1810\n",
            "Epoch [76/100], Step [400/2534], Loss: 0.2015\n",
            "Epoch [76/100], Step [500/2534], Loss: 0.2020\n",
            "Epoch [76/100], Step [600/2534], Loss: 0.3642\n",
            "Epoch [76/100], Step [700/2534], Loss: 0.2606\n",
            "Epoch [76/100], Step [800/2534], Loss: 0.3368\n",
            "Epoch [76/100], Step [900/2534], Loss: 0.2952\n",
            "Epoch [76/100], Step [1000/2534], Loss: 0.3736\n",
            "Epoch [76/100], Step [1100/2534], Loss: 0.2857\n",
            "Epoch [76/100], Step [1200/2534], Loss: 0.3216\n",
            "Epoch [76/100], Step [1300/2534], Loss: 0.3739\n",
            "Epoch [76/100], Step [1400/2534], Loss: 0.4163\n",
            "Epoch [76/100], Step [1500/2534], Loss: 0.2243\n",
            "Epoch [76/100], Step [1600/2534], Loss: 0.1004\n",
            "Epoch [76/100], Step [1700/2534], Loss: 0.1760\n",
            "Epoch [76/100], Step [1800/2534], Loss: 0.3001\n",
            "Epoch [76/100], Step [1900/2534], Loss: 0.1859\n",
            "Epoch [76/100], Step [2000/2534], Loss: 0.3016\n",
            "Epoch [76/100], Step [2100/2534], Loss: 0.1935\n",
            "Epoch [76/100], Step [2200/2534], Loss: 0.1996\n",
            "Epoch [76/100], Step [2300/2534], Loss: 0.4712\n",
            "Epoch [76/100], Step [2400/2534], Loss: 0.2605\n",
            "Epoch [76/100], Step [2500/2534], Loss: 0.3102\n",
            "Epoch [76/100] Average Training Loss: 0.2966\n",
            "Epoch [77/100], Step [100/2534], Loss: 0.1992\n",
            "Epoch [77/100], Step [200/2534], Loss: 0.1355\n",
            "Epoch [77/100], Step [300/2534], Loss: 0.2945\n",
            "Epoch [77/100], Step [400/2534], Loss: 0.2808\n",
            "Epoch [77/100], Step [500/2534], Loss: 0.1712\n",
            "Epoch [77/100], Step [600/2534], Loss: 0.1775\n",
            "Epoch [77/100], Step [700/2534], Loss: 0.3334\n",
            "Epoch [77/100], Step [800/2534], Loss: 0.2165\n",
            "Epoch [77/100], Step [900/2534], Loss: 0.3053\n",
            "Epoch [77/100], Step [1000/2534], Loss: 0.1583\n",
            "Epoch [77/100], Step [1100/2534], Loss: 0.0741\n",
            "Epoch [77/100], Step [1200/2534], Loss: 0.4320\n",
            "Epoch [77/100], Step [1300/2534], Loss: 0.2386\n",
            "Epoch [77/100], Step [1400/2534], Loss: 0.3094\n",
            "Epoch [77/100], Step [1500/2534], Loss: 0.2369\n",
            "Epoch [77/100], Step [1600/2534], Loss: 0.4148\n",
            "Epoch [77/100], Step [1700/2534], Loss: 0.3730\n",
            "Epoch [77/100], Step [1800/2534], Loss: 0.1812\n",
            "Epoch [77/100], Step [1900/2534], Loss: 0.2906\n",
            "Epoch [77/100], Step [2000/2534], Loss: 0.3303\n",
            "Epoch [77/100], Step [2100/2534], Loss: 0.4415\n",
            "Epoch [77/100], Step [2200/2534], Loss: 0.7306\n",
            "Epoch [77/100], Step [2300/2534], Loss: 0.3269\n",
            "Epoch [77/100], Step [2400/2534], Loss: 0.4898\n",
            "Epoch [77/100], Step [2500/2534], Loss: 0.4172\n",
            "Epoch [77/100] Average Training Loss: 0.2985\n",
            "Epoch [78/100], Step [100/2534], Loss: 0.0886\n",
            "Epoch [78/100], Step [200/2534], Loss: 0.1281\n",
            "Epoch [78/100], Step [300/2534], Loss: 0.1139\n",
            "Epoch [78/100], Step [400/2534], Loss: 0.2463\n",
            "Epoch [78/100], Step [500/2534], Loss: 0.2715\n",
            "Epoch [78/100], Step [600/2534], Loss: 0.2675\n",
            "Epoch [78/100], Step [700/2534], Loss: 0.1937\n",
            "Epoch [78/100], Step [800/2534], Loss: 0.2380\n",
            "Epoch [78/100], Step [900/2534], Loss: 0.2756\n",
            "Epoch [78/100], Step [1000/2534], Loss: 0.3164\n",
            "Epoch [78/100], Step [1100/2534], Loss: 0.1197\n",
            "Epoch [78/100], Step [1200/2534], Loss: 0.1886\n",
            "Epoch [78/100], Step [1300/2534], Loss: 0.3737\n",
            "Epoch [78/100], Step [1400/2534], Loss: 0.3737\n",
            "Epoch [78/100], Step [1500/2534], Loss: 0.3518\n",
            "Epoch [78/100], Step [1600/2534], Loss: 0.3866\n",
            "Epoch [78/100], Step [1700/2534], Loss: 0.5489\n",
            "Epoch [78/100], Step [1800/2534], Loss: 0.1259\n",
            "Epoch [78/100], Step [1900/2534], Loss: 0.5378\n",
            "Epoch [78/100], Step [2000/2534], Loss: 0.1815\n",
            "Epoch [78/100], Step [2100/2534], Loss: 0.4625\n",
            "Epoch [78/100], Step [2200/2534], Loss: 0.0961\n",
            "Epoch [78/100], Step [2300/2534], Loss: 0.2480\n",
            "Epoch [78/100], Step [2400/2534], Loss: 0.5061\n",
            "Epoch [78/100], Step [2500/2534], Loss: 0.2559\n",
            "Epoch [78/100] Average Training Loss: 0.2950\n",
            "Epoch [79/100], Step [100/2534], Loss: 0.0976\n",
            "Epoch [79/100], Step [200/2534], Loss: 0.2465\n",
            "Epoch [79/100], Step [300/2534], Loss: 0.1769\n",
            "Epoch [79/100], Step [400/2534], Loss: 0.3862\n",
            "Epoch [79/100], Step [500/2534], Loss: 0.3141\n",
            "Epoch [79/100], Step [600/2534], Loss: 0.2417\n",
            "Epoch [79/100], Step [700/2534], Loss: 0.3594\n",
            "Epoch [79/100], Step [800/2534], Loss: 0.1836\n",
            "Epoch [79/100], Step [900/2534], Loss: 0.2238\n",
            "Epoch [79/100], Step [1000/2534], Loss: 0.4250\n",
            "Epoch [79/100], Step [1100/2534], Loss: 0.4210\n",
            "Epoch [79/100], Step [1200/2534], Loss: 0.4606\n",
            "Epoch [79/100], Step [1300/2534], Loss: 0.1870\n",
            "Epoch [79/100], Step [1400/2534], Loss: 0.2705\n",
            "Epoch [79/100], Step [1500/2534], Loss: 0.3024\n",
            "Epoch [79/100], Step [1600/2534], Loss: 0.2573\n",
            "Epoch [79/100], Step [1700/2534], Loss: 0.3302\n",
            "Epoch [79/100], Step [1800/2534], Loss: 0.0896\n",
            "Epoch [79/100], Step [1900/2534], Loss: 0.5924\n",
            "Epoch [79/100], Step [2000/2534], Loss: 0.2383\n",
            "Epoch [79/100], Step [2100/2534], Loss: 0.4908\n",
            "Epoch [79/100], Step [2200/2534], Loss: 0.2776\n",
            "Epoch [79/100], Step [2300/2534], Loss: 0.2859\n",
            "Epoch [79/100], Step [2400/2534], Loss: 0.2624\n",
            "Epoch [79/100], Step [2500/2534], Loss: 0.3230\n",
            "Epoch [79/100] Average Training Loss: 0.2967\n",
            "Epoch [80/100], Step [100/2534], Loss: 0.2031\n",
            "Epoch [80/100], Step [200/2534], Loss: 0.3318\n",
            "Epoch [80/100], Step [300/2534], Loss: 0.1230\n",
            "Epoch [80/100], Step [400/2534], Loss: 0.2467\n",
            "Epoch [80/100], Step [500/2534], Loss: 0.1396\n",
            "Epoch [80/100], Step [600/2534], Loss: 0.1268\n",
            "Epoch [80/100], Step [700/2534], Loss: 0.3036\n",
            "Epoch [80/100], Step [800/2534], Loss: 0.0989\n",
            "Epoch [80/100], Step [900/2534], Loss: 0.0799\n",
            "Epoch [80/100], Step [1000/2534], Loss: 0.2034\n",
            "Epoch [80/100], Step [1100/2534], Loss: 0.3072\n",
            "Epoch [80/100], Step [1200/2534], Loss: 0.2833\n",
            "Epoch [80/100], Step [1300/2534], Loss: 0.2843\n",
            "Epoch [80/100], Step [1400/2534], Loss: 0.2699\n",
            "Epoch [80/100], Step [1500/2534], Loss: 0.2120\n",
            "Epoch [80/100], Step [1600/2534], Loss: 0.3003\n",
            "Epoch [80/100], Step [1700/2534], Loss: 0.4579\n",
            "Epoch [80/100], Step [1800/2534], Loss: 0.3953\n",
            "Epoch [80/100], Step [1900/2534], Loss: 0.3286\n",
            "Epoch [80/100], Step [2000/2534], Loss: 0.7243\n",
            "Epoch [80/100], Step [2100/2534], Loss: 0.3243\n",
            "Epoch [80/100], Step [2200/2534], Loss: 0.3116\n",
            "Epoch [80/100], Step [2300/2534], Loss: 0.4882\n",
            "Epoch [80/100], Step [2400/2534], Loss: 0.1197\n",
            "Epoch [80/100], Step [2500/2534], Loss: 0.4272\n",
            "Epoch [80/100] Average Training Loss: 0.2966\n",
            "Epoch [81/100], Step [100/2534], Loss: 0.2213\n",
            "Epoch [81/100], Step [200/2534], Loss: 0.3990\n",
            "Epoch [81/100], Step [300/2534], Loss: 0.2343\n",
            "Epoch [81/100], Step [400/2534], Loss: 0.2702\n",
            "Epoch [81/100], Step [500/2534], Loss: 0.1880\n",
            "Epoch [81/100], Step [600/2534], Loss: 0.1776\n",
            "Epoch [81/100], Step [700/2534], Loss: 0.1446\n",
            "Epoch [81/100], Step [800/2534], Loss: 0.1820\n",
            "Epoch [81/100], Step [900/2534], Loss: 0.1437\n",
            "Epoch [81/100], Step [1000/2534], Loss: 0.3882\n",
            "Epoch [81/100], Step [1100/2534], Loss: 0.6761\n",
            "Epoch [81/100], Step [1200/2534], Loss: 0.3962\n",
            "Epoch [81/100], Step [1300/2534], Loss: 0.3020\n",
            "Epoch [81/100], Step [1400/2534], Loss: 0.3516\n",
            "Epoch [81/100], Step [1500/2534], Loss: 0.3765\n",
            "Epoch [81/100], Step [1600/2534], Loss: 0.4251\n",
            "Epoch [81/100], Step [1700/2534], Loss: 0.3566\n",
            "Epoch [81/100], Step [1800/2534], Loss: 0.2444\n",
            "Epoch [81/100], Step [1900/2534], Loss: 0.3659\n",
            "Epoch [81/100], Step [2000/2534], Loss: 0.2505\n",
            "Epoch [81/100], Step [2100/2534], Loss: 0.4353\n",
            "Epoch [81/100], Step [2200/2534], Loss: 0.5122\n",
            "Epoch [81/100], Step [2300/2534], Loss: 0.7082\n",
            "Epoch [81/100], Step [2400/2534], Loss: 0.2702\n",
            "Epoch [81/100], Step [2500/2534], Loss: 0.3605\n",
            "Epoch [81/100] Average Training Loss: 0.2950\n",
            "Epoch [82/100], Step [100/2534], Loss: 0.4435\n",
            "Epoch [82/100], Step [200/2534], Loss: 0.0786\n",
            "Epoch [82/100], Step [300/2534], Loss: 0.0952\n",
            "Epoch [82/100], Step [400/2534], Loss: 0.1652\n",
            "Epoch [82/100], Step [500/2534], Loss: 0.3262\n",
            "Epoch [82/100], Step [600/2534], Loss: 0.3342\n",
            "Epoch [82/100], Step [700/2534], Loss: 0.1554\n",
            "Epoch [82/100], Step [800/2534], Loss: 0.1916\n",
            "Epoch [82/100], Step [900/2534], Loss: 0.3303\n",
            "Epoch [82/100], Step [1000/2534], Loss: 0.1702\n",
            "Epoch [82/100], Step [1100/2534], Loss: 0.2957\n",
            "Epoch [82/100], Step [1200/2534], Loss: 0.4411\n",
            "Epoch [82/100], Step [1300/2534], Loss: 0.2954\n",
            "Epoch [82/100], Step [1400/2534], Loss: 0.3316\n",
            "Epoch [82/100], Step [1500/2534], Loss: 0.2308\n",
            "Epoch [82/100], Step [1600/2534], Loss: 0.6487\n",
            "Epoch [82/100], Step [1700/2534], Loss: 0.4691\n",
            "Epoch [82/100], Step [1800/2534], Loss: 0.5135\n",
            "Epoch [82/100], Step [1900/2534], Loss: 0.2843\n",
            "Epoch [82/100], Step [2000/2534], Loss: 0.4027\n",
            "Epoch [82/100], Step [2100/2534], Loss: 0.2427\n",
            "Epoch [82/100], Step [2200/2534], Loss: 0.2457\n",
            "Epoch [82/100], Step [2300/2534], Loss: 0.4794\n",
            "Epoch [82/100], Step [2400/2534], Loss: 0.4031\n",
            "Epoch [82/100], Step [2500/2534], Loss: 0.2644\n",
            "Epoch [82/100] Average Training Loss: 0.2971\n",
            "Epoch [83/100], Step [100/2534], Loss: 0.2413\n",
            "Epoch [83/100], Step [200/2534], Loss: 0.0933\n",
            "Epoch [83/100], Step [300/2534], Loss: 0.2213\n",
            "Epoch [83/100], Step [400/2534], Loss: 0.2444\n",
            "Epoch [83/100], Step [500/2534], Loss: 0.2188\n",
            "Epoch [83/100], Step [600/2534], Loss: 0.1849\n",
            "Epoch [83/100], Step [700/2534], Loss: 0.5014\n",
            "Epoch [83/100], Step [800/2534], Loss: 0.1482\n",
            "Epoch [83/100], Step [900/2534], Loss: 0.3341\n",
            "Epoch [83/100], Step [1000/2534], Loss: 0.5069\n",
            "Epoch [83/100], Step [1100/2534], Loss: 0.2516\n",
            "Epoch [83/100], Step [1200/2534], Loss: 0.4283\n",
            "Epoch [83/100], Step [1300/2534], Loss: 0.4455\n",
            "Epoch [83/100], Step [1400/2534], Loss: 0.2914\n",
            "Epoch [83/100], Step [1500/2534], Loss: 0.2558\n",
            "Epoch [83/100], Step [1600/2534], Loss: 0.3413\n",
            "Epoch [83/100], Step [1700/2534], Loss: 0.3021\n",
            "Epoch [83/100], Step [1800/2534], Loss: 0.2527\n",
            "Epoch [83/100], Step [1900/2534], Loss: 0.2713\n",
            "Epoch [83/100], Step [2000/2534], Loss: 0.3555\n",
            "Epoch [83/100], Step [2100/2534], Loss: 0.4809\n",
            "Epoch [83/100], Step [2200/2534], Loss: 0.4437\n",
            "Epoch [83/100], Step [2300/2534], Loss: 0.3860\n",
            "Epoch [83/100], Step [2400/2534], Loss: 0.2918\n",
            "Epoch [83/100], Step [2500/2534], Loss: 0.4490\n",
            "Epoch [83/100] Average Training Loss: 0.2959\n",
            "Epoch [84/100], Step [100/2534], Loss: 0.1009\n",
            "Epoch [84/100], Step [200/2534], Loss: 0.3385\n",
            "Epoch [84/100], Step [300/2534], Loss: 0.2538\n",
            "Epoch [84/100], Step [400/2534], Loss: 0.3608\n",
            "Epoch [84/100], Step [500/2534], Loss: 0.2165\n",
            "Epoch [84/100], Step [600/2534], Loss: 0.2162\n",
            "Epoch [84/100], Step [700/2534], Loss: 0.4013\n",
            "Epoch [84/100], Step [800/2534], Loss: 0.1314\n",
            "Epoch [84/100], Step [900/2534], Loss: 0.2418\n",
            "Epoch [84/100], Step [1000/2534], Loss: 0.1894\n",
            "Epoch [84/100], Step [1100/2534], Loss: 0.2589\n",
            "Epoch [84/100], Step [1200/2534], Loss: 0.5129\n",
            "Epoch [84/100], Step [1300/2534], Loss: 0.2745\n",
            "Epoch [84/100], Step [1400/2534], Loss: 0.0958\n",
            "Epoch [84/100], Step [1500/2534], Loss: 0.4631\n",
            "Epoch [84/100], Step [1600/2534], Loss: 0.3624\n",
            "Epoch [84/100], Step [1700/2534], Loss: 0.3273\n",
            "Epoch [84/100], Step [1800/2534], Loss: 0.2414\n",
            "Epoch [84/100], Step [1900/2534], Loss: 0.6316\n",
            "Epoch [84/100], Step [2000/2534], Loss: 0.2895\n",
            "Epoch [84/100], Step [2100/2534], Loss: 0.2060\n",
            "Epoch [84/100], Step [2200/2534], Loss: 0.3422\n",
            "Epoch [84/100], Step [2300/2534], Loss: 0.3812\n",
            "Epoch [84/100], Step [2400/2534], Loss: 0.4573\n",
            "Epoch [84/100], Step [2500/2534], Loss: 0.3552\n",
            "Epoch [84/100] Average Training Loss: 0.2938\n",
            "Epoch [85/100], Step [100/2534], Loss: 0.1604\n",
            "Epoch [85/100], Step [200/2534], Loss: 0.4449\n",
            "Epoch [85/100], Step [300/2534], Loss: 0.2113\n",
            "Epoch [85/100], Step [400/2534], Loss: 0.1495\n",
            "Epoch [85/100], Step [500/2534], Loss: 0.1329\n",
            "Epoch [85/100], Step [600/2534], Loss: 0.3473\n",
            "Epoch [85/100], Step [700/2534], Loss: 0.3988\n",
            "Epoch [85/100], Step [800/2534], Loss: 0.3171\n",
            "Epoch [85/100], Step [900/2534], Loss: 0.3254\n",
            "Epoch [85/100], Step [1000/2534], Loss: 0.1130\n",
            "Epoch [85/100], Step [1100/2534], Loss: 0.3357\n",
            "Epoch [85/100], Step [1200/2534], Loss: 0.3955\n",
            "Epoch [85/100], Step [1300/2534], Loss: 0.1991\n",
            "Epoch [85/100], Step [1400/2534], Loss: 0.1374\n",
            "Epoch [85/100], Step [1500/2534], Loss: 0.2686\n",
            "Epoch [85/100], Step [1600/2534], Loss: 0.2968\n",
            "Epoch [85/100], Step [1700/2534], Loss: 0.2246\n",
            "Epoch [85/100], Step [1800/2534], Loss: 0.3328\n",
            "Epoch [85/100], Step [1900/2534], Loss: 0.2492\n",
            "Epoch [85/100], Step [2000/2534], Loss: 0.4666\n",
            "Epoch [85/100], Step [2100/2534], Loss: 0.4026\n",
            "Epoch [85/100], Step [2200/2534], Loss: 0.7056\n",
            "Epoch [85/100], Step [2300/2534], Loss: 0.3251\n",
            "Epoch [85/100], Step [2400/2534], Loss: 0.4502\n",
            "Epoch [85/100], Step [2500/2534], Loss: 0.3954\n",
            "Epoch [85/100] Average Training Loss: 0.2962\n",
            "Epoch [86/100], Step [100/2534], Loss: 0.3045\n",
            "Epoch [86/100], Step [200/2534], Loss: 0.2227\n",
            "Epoch [86/100], Step [300/2534], Loss: 0.1532\n",
            "Epoch [86/100], Step [400/2534], Loss: 0.2643\n",
            "Epoch [86/100], Step [500/2534], Loss: 0.1772\n",
            "Epoch [86/100], Step [600/2534], Loss: 0.1570\n",
            "Epoch [86/100], Step [700/2534], Loss: 0.1370\n",
            "Epoch [86/100], Step [800/2534], Loss: 0.1441\n",
            "Epoch [86/100], Step [900/2534], Loss: 0.2495\n",
            "Epoch [86/100], Step [1000/2534], Loss: 0.3066\n",
            "Epoch [86/100], Step [1100/2534], Loss: 0.3601\n",
            "Epoch [86/100], Step [1200/2534], Loss: 0.3329\n",
            "Epoch [86/100], Step [1300/2534], Loss: 0.4963\n",
            "Epoch [86/100], Step [1400/2534], Loss: 0.6642\n",
            "Epoch [86/100], Step [1500/2534], Loss: 0.3773\n",
            "Epoch [86/100], Step [1600/2534], Loss: 0.2500\n",
            "Epoch [86/100], Step [1700/2534], Loss: 0.5084\n",
            "Epoch [86/100], Step [1800/2534], Loss: 0.3065\n",
            "Epoch [86/100], Step [1900/2534], Loss: 0.6077\n",
            "Epoch [86/100], Step [2000/2534], Loss: 0.5094\n",
            "Epoch [86/100], Step [2100/2534], Loss: 0.3415\n",
            "Epoch [86/100], Step [2200/2534], Loss: 0.3193\n",
            "Epoch [86/100], Step [2300/2534], Loss: 0.1980\n",
            "Epoch [86/100], Step [2400/2534], Loss: 0.4465\n",
            "Epoch [86/100], Step [2500/2534], Loss: 0.4236\n",
            "Epoch [86/100] Average Training Loss: 0.2946\n",
            "Epoch [87/100], Step [100/2534], Loss: 0.1867\n",
            "Epoch [87/100], Step [200/2534], Loss: 0.3391\n",
            "Epoch [87/100], Step [300/2534], Loss: 0.0801\n",
            "Epoch [87/100], Step [400/2534], Loss: 0.2955\n",
            "Epoch [87/100], Step [500/2534], Loss: 0.3147\n",
            "Epoch [87/100], Step [600/2534], Loss: 0.3013\n",
            "Epoch [87/100], Step [700/2534], Loss: 0.2877\n",
            "Epoch [87/100], Step [800/2534], Loss: 0.2451\n",
            "Epoch [87/100], Step [900/2534], Loss: 0.1783\n",
            "Epoch [87/100], Step [1000/2534], Loss: 0.3258\n",
            "Epoch [87/100], Step [1100/2534], Loss: 0.2254\n",
            "Epoch [87/100], Step [1200/2534], Loss: 0.2850\n",
            "Epoch [87/100], Step [1300/2534], Loss: 0.2911\n",
            "Epoch [87/100], Step [1400/2534], Loss: 0.4517\n",
            "Epoch [87/100], Step [1500/2534], Loss: 0.1418\n",
            "Epoch [87/100], Step [1600/2534], Loss: 0.2216\n",
            "Epoch [87/100], Step [1700/2534], Loss: 0.1917\n",
            "Epoch [87/100], Step [1800/2534], Loss: 0.2659\n",
            "Epoch [87/100], Step [1900/2534], Loss: 0.3410\n",
            "Epoch [87/100], Step [2000/2534], Loss: 0.2718\n",
            "Epoch [87/100], Step [2100/2534], Loss: 0.2791\n",
            "Epoch [87/100], Step [2200/2534], Loss: 0.4470\n",
            "Epoch [87/100], Step [2300/2534], Loss: 0.3706\n",
            "Epoch [87/100], Step [2400/2534], Loss: 0.3460\n",
            "Epoch [87/100], Step [2500/2534], Loss: 0.3962\n",
            "Epoch [87/100] Average Training Loss: 0.2931\n",
            "Epoch [88/100], Step [100/2534], Loss: 0.1494\n",
            "Epoch [88/100], Step [200/2534], Loss: 0.1907\n",
            "Epoch [88/100], Step [300/2534], Loss: 0.0544\n",
            "Epoch [88/100], Step [400/2534], Loss: 0.4226\n",
            "Epoch [88/100], Step [500/2534], Loss: 0.2827\n",
            "Epoch [88/100], Step [600/2534], Loss: 0.2830\n",
            "Epoch [88/100], Step [700/2534], Loss: 0.1211\n",
            "Epoch [88/100], Step [800/2534], Loss: 0.2341\n",
            "Epoch [88/100], Step [900/2534], Loss: 0.2351\n",
            "Epoch [88/100], Step [1000/2534], Loss: 0.2067\n",
            "Epoch [88/100], Step [1100/2534], Loss: 0.2498\n",
            "Epoch [88/100], Step [1200/2534], Loss: 0.2419\n",
            "Epoch [88/100], Step [1300/2534], Loss: 0.2430\n",
            "Epoch [88/100], Step [1400/2534], Loss: 0.1580\n",
            "Epoch [88/100], Step [1500/2534], Loss: 0.4211\n",
            "Epoch [88/100], Step [1600/2534], Loss: 0.3324\n",
            "Epoch [88/100], Step [1700/2534], Loss: 0.1692\n",
            "Epoch [88/100], Step [1800/2534], Loss: 0.3322\n",
            "Epoch [88/100], Step [1900/2534], Loss: 0.3180\n",
            "Epoch [88/100], Step [2000/2534], Loss: 0.2958\n",
            "Epoch [88/100], Step [2100/2534], Loss: 0.3135\n",
            "Epoch [88/100], Step [2200/2534], Loss: 0.3067\n",
            "Epoch [88/100], Step [2300/2534], Loss: 0.3577\n",
            "Epoch [88/100], Step [2400/2534], Loss: 0.2982\n",
            "Epoch [88/100], Step [2500/2534], Loss: 0.3815\n",
            "Epoch [88/100] Average Training Loss: 0.2938\n",
            "Epoch [89/100], Step [100/2534], Loss: 0.2099\n",
            "Epoch [89/100], Step [200/2534], Loss: 0.1979\n",
            "Epoch [89/100], Step [300/2534], Loss: 0.2364\n",
            "Epoch [89/100], Step [400/2534], Loss: 0.2588\n",
            "Epoch [89/100], Step [500/2534], Loss: 0.1752\n",
            "Epoch [89/100], Step [600/2534], Loss: 0.3671\n",
            "Epoch [89/100], Step [700/2534], Loss: 0.2095\n",
            "Epoch [89/100], Step [800/2534], Loss: 0.3284\n",
            "Epoch [89/100], Step [900/2534], Loss: 0.2248\n",
            "Epoch [89/100], Step [1000/2534], Loss: 0.1056\n",
            "Epoch [89/100], Step [1100/2534], Loss: 0.1766\n",
            "Epoch [89/100], Step [1200/2534], Loss: 0.1871\n",
            "Epoch [89/100], Step [1300/2534], Loss: 0.2624\n",
            "Epoch [89/100], Step [1400/2534], Loss: 0.4555\n",
            "Epoch [89/100], Step [1500/2534], Loss: 0.3245\n",
            "Epoch [89/100], Step [1600/2534], Loss: 0.2006\n",
            "Epoch [89/100], Step [1700/2534], Loss: 0.4062\n",
            "Epoch [89/100], Step [1800/2534], Loss: 0.3416\n",
            "Epoch [89/100], Step [1900/2534], Loss: 0.5141\n",
            "Epoch [89/100], Step [2000/2534], Loss: 0.2226\n",
            "Epoch [89/100], Step [2100/2534], Loss: 0.4601\n",
            "Epoch [89/100], Step [2200/2534], Loss: 0.3791\n",
            "Epoch [89/100], Step [2300/2534], Loss: 0.4537\n",
            "Epoch [89/100], Step [2400/2534], Loss: 0.3918\n",
            "Epoch [89/100], Step [2500/2534], Loss: 0.2119\n",
            "Epoch [89/100] Average Training Loss: 0.2947\n",
            "Epoch [90/100], Step [100/2534], Loss: 0.0849\n",
            "Epoch [90/100], Step [200/2534], Loss: 0.2302\n",
            "Epoch [90/100], Step [300/2534], Loss: 0.2588\n",
            "Epoch [90/100], Step [400/2534], Loss: 0.1482\n",
            "Epoch [90/100], Step [500/2534], Loss: 0.1418\n",
            "Epoch [90/100], Step [600/2534], Loss: 0.1487\n",
            "Epoch [90/100], Step [700/2534], Loss: 0.3598\n",
            "Epoch [90/100], Step [800/2534], Loss: 0.3624\n",
            "Epoch [90/100], Step [900/2534], Loss: 0.2055\n",
            "Epoch [90/100], Step [1000/2534], Loss: 0.2832\n",
            "Epoch [90/100], Step [1100/2534], Loss: 0.2011\n",
            "Epoch [90/100], Step [1200/2534], Loss: 0.3886\n",
            "Epoch [90/100], Step [1300/2534], Loss: 0.2168\n",
            "Epoch [90/100], Step [1400/2534], Loss: 0.3079\n",
            "Epoch [90/100], Step [1500/2534], Loss: 0.2453\n",
            "Epoch [90/100], Step [1600/2534], Loss: 0.1205\n",
            "Epoch [90/100], Step [1700/2534], Loss: 0.2457\n",
            "Epoch [90/100], Step [1800/2534], Loss: 0.3978\n",
            "Epoch [90/100], Step [1900/2534], Loss: 0.3411\n",
            "Epoch [90/100], Step [2000/2534], Loss: 0.2168\n",
            "Epoch [90/100], Step [2100/2534], Loss: 0.4044\n",
            "Epoch [90/100], Step [2200/2534], Loss: 0.2372\n",
            "Epoch [90/100], Step [2300/2534], Loss: 0.2135\n",
            "Epoch [90/100], Step [2400/2534], Loss: 0.3646\n",
            "Epoch [90/100], Step [2500/2534], Loss: 0.2461\n",
            "Epoch [90/100] Average Training Loss: 0.2939\n",
            "Epoch [91/100], Step [100/2534], Loss: 0.1099\n",
            "Epoch [91/100], Step [200/2534], Loss: 0.4164\n",
            "Epoch [91/100], Step [300/2534], Loss: 0.2816\n",
            "Epoch [91/100], Step [400/2534], Loss: 0.1169\n",
            "Epoch [91/100], Step [500/2534], Loss: 0.1999\n",
            "Epoch [91/100], Step [600/2534], Loss: 0.2715\n",
            "Epoch [91/100], Step [700/2534], Loss: 0.3903\n",
            "Epoch [91/100], Step [800/2534], Loss: 0.3083\n",
            "Epoch [91/100], Step [900/2534], Loss: 0.2769\n",
            "Epoch [91/100], Step [1000/2534], Loss: 0.1442\n",
            "Epoch [91/100], Step [1100/2534], Loss: 0.2396\n",
            "Epoch [91/100], Step [1200/2534], Loss: 0.2171\n",
            "Epoch [91/100], Step [1300/2534], Loss: 0.0901\n",
            "Epoch [91/100], Step [1400/2534], Loss: 0.3907\n",
            "Epoch [91/100], Step [1500/2534], Loss: 0.1779\n",
            "Epoch [91/100], Step [1600/2534], Loss: 0.3376\n",
            "Epoch [91/100], Step [1700/2534], Loss: 0.2836\n",
            "Epoch [91/100], Step [1800/2534], Loss: 0.2733\n",
            "Epoch [91/100], Step [1900/2534], Loss: 0.4394\n",
            "Epoch [91/100], Step [2000/2534], Loss: 0.2687\n",
            "Epoch [91/100], Step [2100/2534], Loss: 0.5666\n",
            "Epoch [91/100], Step [2200/2534], Loss: 0.3723\n",
            "Epoch [91/100], Step [2300/2534], Loss: 0.5387\n",
            "Epoch [91/100], Step [2400/2534], Loss: 0.2649\n",
            "Epoch [91/100], Step [2500/2534], Loss: 0.2575\n",
            "Epoch [91/100] Average Training Loss: 0.2930\n",
            "Epoch [92/100], Step [100/2534], Loss: 0.1567\n",
            "Epoch [92/100], Step [200/2534], Loss: 0.3123\n",
            "Epoch [92/100], Step [300/2534], Loss: 0.2103\n",
            "Epoch [92/100], Step [400/2534], Loss: 0.2626\n",
            "Epoch [92/100], Step [500/2534], Loss: 0.1895\n",
            "Epoch [92/100], Step [600/2534], Loss: 0.2520\n",
            "Epoch [92/100], Step [700/2534], Loss: 0.2970\n",
            "Epoch [92/100], Step [800/2534], Loss: 0.2718\n",
            "Epoch [92/100], Step [900/2534], Loss: 0.2236\n",
            "Epoch [92/100], Step [1000/2534], Loss: 0.2047\n",
            "Epoch [92/100], Step [1100/2534], Loss: 0.4276\n",
            "Epoch [92/100], Step [1200/2534], Loss: 0.3230\n",
            "Epoch [92/100], Step [1300/2534], Loss: 0.0963\n",
            "Epoch [92/100], Step [1400/2534], Loss: 0.3449\n",
            "Epoch [92/100], Step [1500/2534], Loss: 0.3070\n",
            "Epoch [92/100], Step [1600/2534], Loss: 0.3566\n",
            "Epoch [92/100], Step [1700/2534], Loss: 0.3743\n",
            "Epoch [92/100], Step [1800/2534], Loss: 0.2932\n",
            "Epoch [92/100], Step [1900/2534], Loss: 0.5180\n",
            "Epoch [92/100], Step [2000/2534], Loss: 0.3554\n",
            "Epoch [92/100], Step [2100/2534], Loss: 0.2362\n",
            "Epoch [92/100], Step [2200/2534], Loss: 0.2587\n",
            "Epoch [92/100], Step [2300/2534], Loss: 0.1387\n",
            "Epoch [92/100], Step [2400/2534], Loss: 0.6188\n",
            "Epoch [92/100], Step [2500/2534], Loss: 0.3847\n",
            "Epoch [92/100] Average Training Loss: 0.2930\n",
            "Epoch [93/100], Step [100/2534], Loss: 0.5001\n",
            "Epoch [93/100], Step [200/2534], Loss: 0.2123\n",
            "Epoch [93/100], Step [300/2534], Loss: 0.2206\n",
            "Epoch [93/100], Step [400/2534], Loss: 0.2055\n",
            "Epoch [93/100], Step [500/2534], Loss: 0.1990\n",
            "Epoch [93/100], Step [600/2534], Loss: 0.1568\n",
            "Epoch [93/100], Step [700/2534], Loss: 0.5280\n",
            "Epoch [93/100], Step [800/2534], Loss: 0.2168\n",
            "Epoch [93/100], Step [900/2534], Loss: 0.3854\n",
            "Epoch [93/100], Step [1000/2534], Loss: 0.3230\n",
            "Epoch [93/100], Step [1100/2534], Loss: 0.2535\n",
            "Epoch [93/100], Step [1200/2534], Loss: 0.3227\n",
            "Epoch [93/100], Step [1300/2534], Loss: 0.3251\n",
            "Epoch [93/100], Step [1400/2534], Loss: 0.3854\n",
            "Epoch [93/100], Step [1500/2534], Loss: 0.1645\n",
            "Epoch [93/100], Step [1600/2534], Loss: 0.4401\n",
            "Epoch [93/100], Step [1700/2534], Loss: 0.2527\n",
            "Epoch [93/100], Step [1800/2534], Loss: 0.3851\n",
            "Epoch [93/100], Step [1900/2534], Loss: 0.3850\n",
            "Epoch [93/100], Step [2000/2534], Loss: 0.2466\n",
            "Epoch [93/100], Step [2100/2534], Loss: 0.6124\n",
            "Epoch [93/100], Step [2200/2534], Loss: 0.3536\n",
            "Epoch [93/100], Step [2300/2534], Loss: 0.3726\n",
            "Epoch [93/100], Step [2400/2534], Loss: 0.3641\n",
            "Epoch [93/100], Step [2500/2534], Loss: 0.2976\n",
            "Epoch [93/100] Average Training Loss: 0.2938\n",
            "Epoch [94/100], Step [100/2534], Loss: 0.1438\n",
            "Epoch [94/100], Step [200/2534], Loss: 0.2685\n",
            "Epoch [94/100], Step [300/2534], Loss: 0.2107\n",
            "Epoch [94/100], Step [400/2534], Loss: 0.2266\n",
            "Epoch [94/100], Step [500/2534], Loss: 0.1156\n",
            "Epoch [94/100], Step [600/2534], Loss: 0.2254\n",
            "Epoch [94/100], Step [700/2534], Loss: 0.2399\n",
            "Epoch [94/100], Step [800/2534], Loss: 0.1833\n",
            "Epoch [94/100], Step [900/2534], Loss: 0.1405\n",
            "Epoch [94/100], Step [1000/2534], Loss: 0.2614\n",
            "Epoch [94/100], Step [1100/2534], Loss: 0.3874\n",
            "Epoch [94/100], Step [1200/2534], Loss: 0.3355\n",
            "Epoch [94/100], Step [1300/2534], Loss: 0.4051\n",
            "Epoch [94/100], Step [1400/2534], Loss: 0.2172\n",
            "Epoch [94/100], Step [1500/2534], Loss: 0.2190\n",
            "Epoch [94/100], Step [1600/2534], Loss: 0.2438\n",
            "Epoch [94/100], Step [1700/2534], Loss: 0.3271\n",
            "Epoch [94/100], Step [1800/2534], Loss: 0.3436\n",
            "Epoch [94/100], Step [1900/2534], Loss: 0.3637\n",
            "Epoch [94/100], Step [2000/2534], Loss: 0.5356\n",
            "Epoch [94/100], Step [2100/2534], Loss: 0.3297\n",
            "Epoch [94/100], Step [2200/2534], Loss: 0.4125\n",
            "Epoch [94/100], Step [2300/2534], Loss: 0.2309\n",
            "Epoch [94/100], Step [2400/2534], Loss: 0.3570\n",
            "Epoch [94/100], Step [2500/2534], Loss: 0.3964\n",
            "Epoch [94/100] Average Training Loss: 0.2904\n",
            "Epoch [95/100], Step [100/2534], Loss: 0.1623\n",
            "Epoch [95/100], Step [200/2534], Loss: 0.0867\n",
            "Epoch [95/100], Step [300/2534], Loss: 0.1526\n",
            "Epoch [95/100], Step [400/2534], Loss: 0.3366\n",
            "Epoch [95/100], Step [500/2534], Loss: 0.3445\n",
            "Epoch [95/100], Step [600/2534], Loss: 0.2311\n",
            "Epoch [95/100], Step [700/2534], Loss: 0.3397\n",
            "Epoch [95/100], Step [800/2534], Loss: 0.2316\n",
            "Epoch [95/100], Step [900/2534], Loss: 0.2775\n",
            "Epoch [95/100], Step [1000/2534], Loss: 0.3239\n",
            "Epoch [95/100], Step [1100/2534], Loss: 0.1350\n",
            "Epoch [95/100], Step [1200/2534], Loss: 0.4798\n",
            "Epoch [95/100], Step [1300/2534], Loss: 0.3946\n",
            "Epoch [95/100], Step [1400/2534], Loss: 0.3539\n",
            "Epoch [95/100], Step [1500/2534], Loss: 0.3042\n",
            "Epoch [95/100], Step [1600/2534], Loss: 0.2544\n",
            "Epoch [95/100], Step [1700/2534], Loss: 0.3234\n",
            "Epoch [95/100], Step [1800/2534], Loss: 0.2891\n",
            "Epoch [95/100], Step [1900/2534], Loss: 0.1929\n",
            "Epoch [95/100], Step [2000/2534], Loss: 0.3415\n",
            "Epoch [95/100], Step [2100/2534], Loss: 0.2079\n",
            "Epoch [95/100], Step [2200/2534], Loss: 0.0874\n",
            "Epoch [95/100], Step [2300/2534], Loss: 0.3222\n",
            "Epoch [95/100], Step [2400/2534], Loss: 0.4441\n",
            "Epoch [95/100], Step [2500/2534], Loss: 0.4588\n",
            "Epoch [95/100] Average Training Loss: 0.2937\n",
            "Epoch [96/100], Step [100/2534], Loss: 0.2457\n",
            "Epoch [96/100], Step [200/2534], Loss: 0.2725\n",
            "Epoch [96/100], Step [300/2534], Loss: 0.2802\n",
            "Epoch [96/100], Step [400/2534], Loss: 0.2225\n",
            "Epoch [96/100], Step [500/2534], Loss: 0.3385\n",
            "Epoch [96/100], Step [600/2534], Loss: 0.3097\n",
            "Epoch [96/100], Step [700/2534], Loss: 0.4407\n",
            "Epoch [96/100], Step [800/2534], Loss: 0.2494\n",
            "Epoch [96/100], Step [900/2534], Loss: 0.2193\n",
            "Epoch [96/100], Step [1000/2534], Loss: 0.2340\n",
            "Epoch [96/100], Step [1100/2534], Loss: 0.2766\n",
            "Epoch [96/100], Step [1200/2534], Loss: 0.4553\n",
            "Epoch [96/100], Step [1300/2534], Loss: 0.2583\n",
            "Epoch [96/100], Step [1400/2534], Loss: 0.2258\n",
            "Epoch [96/100], Step [1500/2534], Loss: 0.5133\n",
            "Epoch [96/100], Step [1600/2534], Loss: 0.3140\n",
            "Epoch [96/100], Step [1700/2534], Loss: 0.2058\n",
            "Epoch [96/100], Step [1800/2534], Loss: 0.3794\n",
            "Epoch [96/100], Step [1900/2534], Loss: 0.2399\n",
            "Epoch [96/100], Step [2000/2534], Loss: 0.2929\n",
            "Epoch [96/100], Step [2100/2534], Loss: 0.2262\n",
            "Epoch [96/100], Step [2200/2534], Loss: 0.1022\n",
            "Epoch [96/100], Step [2300/2534], Loss: 0.3527\n",
            "Epoch [96/100], Step [2400/2534], Loss: 0.2811\n",
            "Epoch [96/100], Step [2500/2534], Loss: 0.2956\n",
            "Epoch [96/100] Average Training Loss: 0.2914\n",
            "Epoch [97/100], Step [100/2534], Loss: 0.3508\n",
            "Epoch [97/100], Step [200/2534], Loss: 0.1748\n",
            "Epoch [97/100], Step [300/2534], Loss: 0.2582\n",
            "Epoch [97/100], Step [400/2534], Loss: 0.2065\n",
            "Epoch [97/100], Step [500/2534], Loss: 0.2367\n",
            "Epoch [97/100], Step [600/2534], Loss: 0.2109\n",
            "Epoch [97/100], Step [700/2534], Loss: 0.1095\n",
            "Epoch [97/100], Step [800/2534], Loss: 0.1950\n",
            "Epoch [97/100], Step [900/2534], Loss: 0.1858\n",
            "Epoch [97/100], Step [1000/2534], Loss: 0.2449\n",
            "Epoch [97/100], Step [1100/2534], Loss: 0.4450\n",
            "Epoch [97/100], Step [1200/2534], Loss: 0.1551\n",
            "Epoch [97/100], Step [1300/2534], Loss: 0.5003\n",
            "Epoch [97/100], Step [1400/2534], Loss: 0.2478\n",
            "Epoch [97/100], Step [1500/2534], Loss: 0.3659\n",
            "Epoch [97/100], Step [1600/2534], Loss: 0.2859\n",
            "Epoch [97/100], Step [1700/2534], Loss: 0.3873\n",
            "Epoch [97/100], Step [1800/2534], Loss: 0.2407\n",
            "Epoch [97/100], Step [1900/2534], Loss: 0.2747\n",
            "Epoch [97/100], Step [2000/2534], Loss: 0.3451\n",
            "Epoch [97/100], Step [2100/2534], Loss: 0.3585\n",
            "Epoch [97/100], Step [2200/2534], Loss: 0.5649\n",
            "Epoch [97/100], Step [2300/2534], Loss: 0.3741\n",
            "Epoch [97/100], Step [2400/2534], Loss: 0.6385\n",
            "Epoch [97/100], Step [2500/2534], Loss: 0.2431\n",
            "Epoch [97/100] Average Training Loss: 0.2924\n",
            "Epoch [98/100], Step [100/2534], Loss: 0.1123\n",
            "Epoch [98/100], Step [200/2534], Loss: 0.3045\n",
            "Epoch [98/100], Step [300/2534], Loss: 0.1465\n",
            "Epoch [98/100], Step [400/2534], Loss: 0.1437\n",
            "Epoch [98/100], Step [500/2534], Loss: 0.2669\n",
            "Epoch [98/100], Step [600/2534], Loss: 0.3041\n",
            "Epoch [98/100], Step [700/2534], Loss: 0.3576\n",
            "Epoch [98/100], Step [800/2534], Loss: 0.2016\n",
            "Epoch [98/100], Step [900/2534], Loss: 0.1463\n",
            "Epoch [98/100], Step [1000/2534], Loss: 0.2168\n",
            "Epoch [98/100], Step [1100/2534], Loss: 0.4406\n",
            "Epoch [98/100], Step [1200/2534], Loss: 0.2652\n",
            "Epoch [98/100], Step [1300/2534], Loss: 0.3675\n",
            "Epoch [98/100], Step [1400/2534], Loss: 0.3797\n",
            "Epoch [98/100], Step [1500/2534], Loss: 0.2328\n",
            "Epoch [98/100], Step [1600/2534], Loss: 0.4399\n",
            "Epoch [98/100], Step [1700/2534], Loss: 0.2805\n",
            "Epoch [98/100], Step [1800/2534], Loss: 0.3073\n",
            "Epoch [98/100], Step [1900/2534], Loss: 0.3160\n",
            "Epoch [98/100], Step [2000/2534], Loss: 0.3656\n",
            "Epoch [98/100], Step [2100/2534], Loss: 0.4646\n",
            "Epoch [98/100], Step [2200/2534], Loss: 0.1505\n",
            "Epoch [98/100], Step [2300/2534], Loss: 0.4764\n",
            "Epoch [98/100], Step [2400/2534], Loss: 0.2588\n",
            "Epoch [98/100], Step [2500/2534], Loss: 0.3540\n",
            "Epoch [98/100] Average Training Loss: 0.2913\n",
            "Epoch [99/100], Step [100/2534], Loss: 0.4140\n",
            "Epoch [99/100], Step [200/2534], Loss: 0.0694\n",
            "Epoch [99/100], Step [300/2534], Loss: 0.1694\n",
            "Epoch [99/100], Step [400/2534], Loss: 0.2339\n",
            "Epoch [99/100], Step [500/2534], Loss: 0.3548\n",
            "Epoch [99/100], Step [600/2534], Loss: 0.2294\n",
            "Epoch [99/100], Step [700/2534], Loss: 0.4386\n",
            "Epoch [99/100], Step [800/2534], Loss: 0.3039\n",
            "Epoch [99/100], Step [900/2534], Loss: 0.1955\n",
            "Epoch [99/100], Step [1000/2534], Loss: 0.2034\n",
            "Epoch [99/100], Step [1100/2534], Loss: 0.3291\n",
            "Epoch [99/100], Step [1200/2534], Loss: 0.1667\n",
            "Epoch [99/100], Step [1300/2534], Loss: 0.3018\n",
            "Epoch [99/100], Step [1400/2534], Loss: 0.3874\n",
            "Epoch [99/100], Step [1500/2534], Loss: 0.2789\n",
            "Epoch [99/100], Step [1600/2534], Loss: 0.3768\n",
            "Epoch [99/100], Step [1700/2534], Loss: 0.4631\n",
            "Epoch [99/100], Step [1800/2534], Loss: 0.2299\n",
            "Epoch [99/100], Step [1900/2534], Loss: 0.3996\n",
            "Epoch [99/100], Step [2000/2534], Loss: 0.5365\n",
            "Epoch [99/100], Step [2100/2534], Loss: 0.5078\n",
            "Epoch [99/100], Step [2200/2534], Loss: 0.1636\n",
            "Epoch [99/100], Step [2300/2534], Loss: 0.4419\n",
            "Epoch [99/100], Step [2400/2534], Loss: 0.5296\n",
            "Epoch [99/100], Step [2500/2534], Loss: 0.2506\n",
            "Epoch [99/100] Average Training Loss: 0.2906\n",
            "Epoch [100/100], Step [100/2534], Loss: 0.3632\n",
            "Epoch [100/100], Step [200/2534], Loss: 0.2425\n",
            "Epoch [100/100], Step [300/2534], Loss: 0.4507\n",
            "Epoch [100/100], Step [400/2534], Loss: 0.3165\n",
            "Epoch [100/100], Step [500/2534], Loss: 0.1027\n",
            "Epoch [100/100], Step [600/2534], Loss: 0.4042\n",
            "Epoch [100/100], Step [700/2534], Loss: 0.1890\n",
            "Epoch [100/100], Step [800/2534], Loss: 0.3613\n",
            "Epoch [100/100], Step [900/2534], Loss: 0.5287\n",
            "Epoch [100/100], Step [1000/2534], Loss: 0.3023\n",
            "Epoch [100/100], Step [1100/2534], Loss: 0.3368\n",
            "Epoch [100/100], Step [1200/2534], Loss: 0.3513\n",
            "Epoch [100/100], Step [1300/2534], Loss: 0.2195\n",
            "Epoch [100/100], Step [1400/2534], Loss: 0.2872\n",
            "Epoch [100/100], Step [1500/2534], Loss: 0.3119\n",
            "Epoch [100/100], Step [1600/2534], Loss: 0.2193\n",
            "Epoch [100/100], Step [1700/2534], Loss: 0.3335\n",
            "Epoch [100/100], Step [1800/2534], Loss: 0.1629\n",
            "Epoch [100/100], Step [1900/2534], Loss: 0.3354\n",
            "Epoch [100/100], Step [2000/2534], Loss: 0.2158\n",
            "Epoch [100/100], Step [2100/2534], Loss: 0.4422\n",
            "Epoch [100/100], Step [2200/2534], Loss: 0.2972\n",
            "Epoch [100/100], Step [2300/2534], Loss: 0.3980\n",
            "Epoch [100/100], Step [2400/2534], Loss: 0.2803\n",
            "Epoch [100/100], Step [2500/2534], Loss: 0.3029\n",
            "Epoch [100/100] Average Training Loss: 0.2914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55a5bb06",
        "outputId": "d6b12633-adfb-4d46-f006-efca19abafb0"
      },
      "source": [
        "# --- Step to get a prediction from the model ---\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation for evaluation\n",
        "with torch.no_grad():\n",
        "    # Get one batch from the test loader\n",
        "    # We can iterate through the test_loader or use `next(iter(test_loader))` to get a single batch\n",
        "    context_batch, target_batch = next(iter(test_loader))\n",
        "\n",
        "    # Move the batch to the same device as the model\n",
        "    context_batch, target_batch = context_batch.to(device), target_batch.to(device)\n",
        "\n",
        "    # Get the model's output for the context batch\n",
        "    outputs = model(context_batch)\n",
        "\n",
        "    # The outputs are logits (raw scores). To get probabilities, you'd typically use softmax.\n",
        "    # However, for finding the predicted class (word ID), we can just find the index\n",
        "    # with the maximum logit value, as softmax preserves the order of the logits.\n",
        "    # torch.argmax returns the index of the maximum value along a dimension.\n",
        "    # Here, dim=1 means we find the max index for each item in the batch (each example's output).\n",
        "    predicted_ids = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Move the tensors back to CPU and convert to numpy arrays or lists for easier iteration and use with the dictionary\n",
        "    context_batch_cpu = context_batch.cpu().numpy()\n",
        "    target_batch_cpu = target_batch.cpu().numpy()\n",
        "    predicted_ids_cpu = predicted_ids.cpu().numpy()\n",
        "\n",
        "    # Display predictions for a few samples in the batch\n",
        "    num_samples_to_display = 5\n",
        "    print(\"\\n--- Sample Predictions ---\")\n",
        "    for i in range(min(num_samples_to_display, context_batch_cpu.shape[0])):\n",
        "        context_ids = context_batch_cpu[i]\n",
        "        real_target_id = target_batch_cpu[i]\n",
        "        predicted_target_id = predicted_ids_cpu[i]\n",
        "\n",
        "        # Translate IDs back to words\n",
        "        context_words = [id_to_word.get(idx, '<UNK>') for idx in context_ids]\n",
        "        real_target_word = id_to_word.get(real_target_id, '<UNK>')\n",
        "        predicted_target_word = id_to_word.get(predicted_target_id, '<UNK>')\n",
        "\n",
        "        print(f\"Context: {context_words}\")\n",
        "        print(f\"Real Target: {real_target_word}\")\n",
        "        print(f\"Predicted Target: {predicted_target_word}\")\n",
        "        print(\"-\" * 20)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample Predictions ---\n",
            "Context: ['i', 'do', 'your', 'words.']\n",
            "Real Target: <UNK>\n",
            "Predicted Target: beseech\n",
            "--------------------\n",
            "Context: ['do', '<UNK>', 'words.', 'be']\n",
            "Real Target: your\n",
            "Predicted Target: <UNK>\n",
            "--------------------\n",
            "Context: ['<UNK>', 'your', 'be', 'that']\n",
            "Real Target: words.\n",
            "Predicted Target: <UNK>\n",
            "--------------------\n",
            "Context: ['your', 'words.', 'that', 'you']\n",
            "Real Target: be\n",
            "Predicted Target: for\n",
            "--------------------\n",
            "Context: ['words.', 'be', 'you', 'are,']\n",
            "Real Target: that\n",
            "Predicted Target: <UNK>\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dd94cf2",
        "outputId": "c1aba57f-a1c0-4d15-959a-dc78dacbc39b"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "embedding_layer = model.embedding\n",
        "embedding_matrix = embedding_layer.weight.data.cpu().numpy()\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "embedding_3d = pca.fit_transform(embedding_matrix)\n",
        "\n",
        "# Assuming you have word_to_id and id_to_word dictionaries from previous steps\n",
        "# Iterate through the vocabulary and print the word and its 3D embedding\n",
        "print(\"--- Word Embeddings (3D PCA reduced) ---\")\n",
        "for word, idx in word_to_id.items(): # Use .items() to iterate through key-value pairs\n",
        "    if idx < len(embedding_3d): # Ensure the index is within the bounds of the reduced embeddings\n",
        "        word_embedding = embedding_3d[idx]\n",
        "        print(f\"{word}: {word_embedding}\")\n",
        "\n",
        "# Optional: Add code here later to visualize the 3D embeddings using matplotlib or a more interactive library like Plotly\n",
        "# For example, using matplotlib for a basic 3D scatter plot:\n",
        "# fig = plt.figure(figsize=(10, 10))\n",
        "# ax = fig.add_subplot(111, projection='3d')\n",
        "# ax.scatter(embedding_3d[:, 0], embedding_3d[:, 1], embedding_3d[:, 2])\n",
        "#\n",
        "# # Add labels for a few words (optional, can make the plot cluttered)\n",
        "# for word, idx in word_to_id.items():\n",
        "#     if idx < len(embedding_3d):\n",
        "#         ax.text(embedding_3d[idx, 0], embedding_3d[idx, 1], embedding_3d[idx, 2], word)\n",
        "#\n",
        "# plt.title(\"Word Embeddings (3D PCA)\")\n",
        "# plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Word Embeddings (3D PCA reduced) ---\n",
            "<PAD>: [-0.27103522 -0.03780319  0.23448652]\n",
            "<UNK>: [-0.20933999 -0.02985658  0.10057826]\n",
            "the: [-0.28122187 -0.20792985  0.49075952]\n",
            "and: [-0.46544057 -0.33584952  0.3411419 ]\n",
            "to: [-0.5463037  -0.20920393  0.3822745 ]\n",
            "i: [-0.4206137  -0.18306234  0.2040831 ]\n",
            "of: [-0.44358888 -0.10516313  0.45383886]\n",
            "my: [-0.2661458  -0.24126875  0.44891065]\n",
            "a: [-0.05130346 -0.33895633  0.5146702 ]\n",
            "you: [-0.41408998 -0.3372705   0.13028586]\n",
            "that: [-0.39657837 -0.20565575  0.3862762 ]\n",
            "in: [-0.4156024   0.07394608  0.5056672 ]\n",
            "is: [-0.55725086 -0.09385255  0.25307846]\n",
            "for: [-0.5408285  -0.29112718  0.47276872]\n",
            "not: [-0.5379752   0.06596622  0.33327785]\n",
            "with: [-0.48994714  0.01385933  0.35845754]\n",
            "your: [-0.08366466 -0.42621967  0.50161755]\n",
            "be: [-0.73531836  0.28941494 -0.10072008]\n",
            "his: [-0.11455855 -0.07782812  0.45676172]\n",
            "it: [-0.399845   -0.18766952  0.2633434 ]\n",
            "he: [-0.1878209  -0.14499275  0.41226974]\n",
            "this: [-0.24750787 -0.30679932  0.41946015]\n",
            "have: [-0.5845023   0.5100021   0.24033356]\n",
            "as: [-0.41640404 -0.24138513  0.2922561 ]\n",
            "but: [-0.4976243  -0.22015473  0.29716128]\n",
            "me: [-0.45184675 -0.37716594  0.10612077]\n",
            "thou: [-0.22342812 -0.2125461   0.484607  ]\n",
            "thy: [-0.08724935 -0.2890527   0.57135624]\n",
            "so: [-0.35075098 -0.27432358  0.5444781 ]\n",
            "what: [-0.5076581  -0.4624273   0.37597945]\n",
            "will: [-0.61004335  0.0318285  -0.03150567]\n",
            "by: [-0.3761348  -0.04618657  0.2851654 ]\n",
            "him: [-0.35650906 -0.25628874 -0.18024229]\n",
            "we: [-0.36448744 -0.05931531  0.40709728]\n",
            "shall: [-0.56016004 -0.05279888  0.4707473 ]\n",
            "all: [-0.24225466  0.05017046  0.1894204 ]\n",
            "if: [-0.56220263 -0.14728865  0.26052096]\n",
            "our: [-0.06934713 -0.2120947   0.47527528]\n",
            "king: [ 0.06443811 -0.16670354 -0.22345217]\n",
            "are: [-0.42016935  0.22629991  0.07630564]\n",
            "no: [-0.31144685 -0.21196267  0.753592  ]\n",
            "her: [-0.19073077 -0.17720991  0.2897531 ]\n",
            "do: [-0.66443115  0.72998047 -0.22249545]\n",
            "from: [-0.6904263  -0.07261524  0.31785882]\n",
            "good: [ 0.10847024 -0.33619863  0.5337484 ]\n",
            "on: [-0.68645656 -0.16426298  0.3469768 ]\n",
            "or: [-0.2530146  -0.14756013  0.21545963]\n",
            "at: [-0.5413389   0.08624335  0.5891724 ]\n",
            "which: [-0.28643668 -0.2148458   0.27312103]\n",
            "would: [-0.55709213 -0.03682075  0.5851348 ]\n",
            "they: [-0.30872154  0.13301305  0.27103513]\n",
            "thee: [-0.3471483  -0.2482676   0.11652917]\n",
            "how: [-0.5535491 -0.6435257  0.459571 ]\n",
            "was: [-0.36095017  0.09421322  0.28071463]\n",
            "more: [ 0.05282541 -0.15595835  0.5865901 ]\n",
            "than: [-0.42336398 -0.10378598  0.42894915]\n",
            "their: [ 0.14777462 -0.30247012  0.4508034 ]\n",
            "she: [-0.03941583 -0.15499634  0.32083932]\n",
            "now: [-0.28306618 -0.4632349   0.804431  ]\n",
            "hath: [-0.41161838 -0.07715528  0.4920246 ]\n",
            "am: [-0.93360174  0.3495972   0.47041348]\n",
            "let: [-0.53186566  0.2555059  -0.09454134]\n",
            "then: [-0.58223623 -0.42521974  0.31979585]\n",
            "duke: [ 0.09503099 -0.53314584 -0.0315075 ]\n",
            "i'll: [-0.31431982 -0.6076871   0.2522076 ]\n",
            "when: [-0.30085146 -0.4818418   0.3468952 ]\n",
            "here: [-0.10947178 -0.2688142   0.37419382]\n",
            "were: [-0.47151     0.24152914  0.36807457]\n",
            "lord: [ 0.11463781 -0.22163206 -0.15206097]\n",
            "make: [-0.5977955   0.74851424 -0.06202299]\n",
            "one: [ 0.21756442 -0.25837108  0.2830386 ]\n",
            "may: [-0.32364166 -0.07236997  0.31464273]\n",
            "upon: [-0.8213499  -0.02500065  0.3887271 ]\n",
            "them: [-0.18939842 -0.38327363 -0.239074  ]\n",
            "you,: [-0.4390475  -0.77230895  0.34348944]\n",
            "like: [-0.08902943  0.356142    0.525344  ]\n",
            "come: [-0.76932025  0.2578477   0.01593634]\n",
            "an: [ 0.08433982 -0.2552112   0.93009883]\n",
            "must: [-0.7130261   0.0401842   0.45158294]\n",
            "should: [-0.26947176 -0.09351499  0.4842035 ]\n",
            "richard: [-0.4949183  -0.59258866 -0.0498355 ]\n",
            "had: [-0.9077559   0.35806435  0.506429  ]\n",
            "yet: [-0.3553917  -0.27248743  0.6404108 ]\n",
            "sir,: [-0.31819376 -0.7885404   0.06773375]\n",
            "'tis: [-0.6091384   0.04453117  0.27347943]\n",
            "did: [-0.57610476  0.32667002  0.5903603 ]\n",
            "first: [ 0.01879476 -0.27474406  0.1729115 ]\n",
            "say: [-0.8826336   0.4391352   0.23138691]\n",
            "where: [-0.6036449  -0.24865448  0.3497962 ]\n",
            "there: [-0.6807545  -0.43293232  0.5404403 ]\n",
            "some: [-0.04883024 -0.29043594  0.43872428]\n",
            "go: [-0.67823017  0.19041422  0.00406445]\n",
            "us: [-0.59423304 -0.2278316  -0.06286851]\n",
            "queen: [ 0.08084229 -0.3667448  -0.50080293]\n",
            "know: [-1.0874211   0.8812377  -0.07055231]\n",
            "well: [-0.26100892 -0.5142399   0.91959834]\n",
            "love: [-0.11783974  0.6348705  -0.21136233]\n",
            "give: [-1.0700195   0.63135004 -0.1049496 ]\n",
            "these: [0.13291323 0.02441365 0.5548395 ]\n",
            "take: [-0.97319525  0.8932991   0.03025866]\n",
            "me,: [-0.4496293  -0.80451834 -0.44070235]\n",
            "such: [ 0.12354816 -0.36190194  0.35112086]\n",
            "o: [-0.30160698 -0.4459006   0.3892927 ]\n",
            "see: [-0.95485425  0.77989817 -0.3873955 ]\n",
            "who: [-0.2234527  -0.6222604   0.61294216]\n",
            "o,: [-0.57378346 -0.5954481  -0.03168729]\n",
            "can: [-0.55751246  0.561924    0.14671132]\n",
            "edward: [-0.34308562 -0.70201576  0.23202789]\n",
            "tell: [-0.858423    0.53122526 -0.29148114]\n",
            "henry: [-0.30276543 -0.69321597  0.06621233]\n",
            "man: [ 0.59841985 -0.52800566  0.01605495]\n",
            "too: [-0.22554573 -0.3662052   0.8044999 ]\n",
            "most: [0.70784825 0.03555468 0.40632534]\n",
            "york: [-0.32642937 -0.7613736  -0.17787011]\n",
            "gloucester: [-0.29933745 -0.85119385 -0.27536356]\n",
            "romeo: [-0.41554725 -0.948435   -0.13355897]\n",
            "nor: [-0.63694745 -0.63188523  0.33478117]\n",
            "out: [-0.35118687 -0.07709181  0.35650566]\n",
            "vincentio: [-0.59749323 -0.92262274 -0.01452126]\n",
            "come,: [-1.0057836  -0.5789175   0.17384326]\n",
            "mine: [ 0.03698365 -0.3488023   0.21151252]\n",
            "lord,: [-0.0724314  -0.38517052 -0.5227572 ]\n",
            "up: [-0.24586064 -0.22144173  0.16097419]\n",
            "speak: [-0.6800752   0.5774313   0.03874999]\n",
            "why,: [-0.89099175 -0.7889072   0.44576254]\n",
            "lady: [ 0.07166018 -0.56207865 -0.07141028]\n",
            "now,: [-0.9524191  -0.43937868  0.02151879]\n",
            "made: [-0.17188348  0.22004083  0.6950878 ]\n",
            "time: [ 0.69098276  0.25515574 -0.1413816 ]\n",
            "never: [-0.0181024   0.17689861  0.9546585 ]\n",
            "hear: [-0.95705754  1.098256   -0.60586554]\n",
            "and,: [-0.6048497  -0.46688616  0.77177244]\n",
            "art: [-0.4502674   0.25739062  0.7509529 ]\n",
            "doth: [-0.14676009  0.05775044  0.1443997 ]\n",
            "sir: [-0.6938845  -1.0236737  -0.04593747]\n",
            "much: [0.09923024 0.15352988 1.1384294 ]\n",
            "any: [0.09663953 0.02433176 0.5230677 ]\n",
            "being: [-0.0790906   0.13661799  0.4213961 ]\n",
            "think: [-0.7485127   0.5057069  -0.16457783]\n",
            "cannot: [-0.58049834  0.00811939 -0.01342092]\n",
            "thee,: [-0.47773236 -0.4154774   0.17567794]\n",
            "very: [0.73494583 0.06820992 1.01553   ]\n",
            "noble: [ 0.7107099  -0.36911166  0.53650385]\n",
            "before: [-0.664914    0.16437484 -0.06828241]\n",
            "petruchio: [-0.26053718 -0.08956243  0.21622439]\n",
            "him,: [-0.36131984 -0.47315437 -0.51168853]\n",
            "been: [-0.51768935  0.7254493   1.0651138 ]\n",
            "god: [-0.27489296 -0.6193635  -0.31743926]\n",
            "ay,: [-0.5155499  -0.48725846  0.51119685]\n",
            "menenius: [-0.74844325 -0.85672987 -0.14279012]\n",
            "death: [ 0.2907276  -0.4778229  -0.48021537]\n",
            "second: [ 0.2918985  -0.33259478  0.00244521]\n",
            "coriolanus: [-0.47454497 -1.0697083  -0.0948002 ]\n",
            "warwick: [-0.2753149 -0.6840998 -0.2887529]\n",
            "thus: [ 0.1295023 -0.3251275  0.8388473]\n",
            "father: [ 0.3400959   0.37035057 -0.36722007]\n",
            "against: [-0.9882883   0.2340429   0.09249059]\n",
            "fair: [ 1.0827655  -0.06356756  0.5700739 ]\n",
            "both: [-0.74820167  0.22048804  0.2495118 ]\n",
            "no,: [-0.7269317  -0.38760218  0.15387884]\n",
            "sweet: [ 0.5917299  -0.31477797  0.46109426]\n",
            "though: [-0.61855555 -0.2955584   0.4936275 ]\n",
            "him.: [-0.6738301  -1.1568922  -0.35574177]\n",
            "great: [0.7919844  0.15949483 0.90459764]\n",
            "poor: [0.5603484  0.16286944 0.51662296]\n",
            "heart: [ 0.8822799   0.00704719 -0.58941215]\n",
            "well,: [-0.86476445 -0.6839683   0.53219205]\n",
            "hast: [-0.45711222 -0.0653971   0.70019144]\n",
            "you.: [-0.535558   -1.0725094  -0.22097617]\n",
            "own: [0.09883662 0.15359876 0.01442777]\n",
            "it.: [-0.98143125 -0.91085976  0.02997011]\n",
            "son: [ 0.46410555 -0.3841933  -0.2187001 ]\n",
            "juliet: [-0.24732554 -1.0730525  -0.32772037]\n",
            "look: [-0.7686947   0.81254786 -0.48019332]\n",
            "till: [-1.0067365  -0.40868637  0.10814947]\n",
            "call: [-0.6682194   0.84694153 -0.19326235]\n",
            "name: [ 0.13811208  0.49747446 -0.8455913 ]\n",
            "why: [-0.62198216 -0.5639297   0.6065343 ]\n",
            "nay,: [-0.5564471  -0.75558007  0.02323383]\n",
            "what,: [-0.89948857 -0.6205009   0.699797  ]\n",
            "iii: [-0.70299965 -0.7885098   0.27288613]\n",
            "whose: [ 0.11811481 -0.67479527  0.4048364 ]\n",
            "true: [0.64812714 0.0692835  0.96564555]\n",
            "many: [ 0.30685404 -0.11311553  0.99287766]\n",
            "men: [ 0.06917297 -0.41229853  0.10292675]\n",
            "even: [-0.6394783  -0.21337214  0.31249484]\n",
            "leave: [-0.51552236  1.0218788   0.21700902]\n",
            "old: [0.49832    0.12513688 0.5623619 ]\n",
            "life: [ 0.5684349  -0.11400814 -0.37069148]\n",
            "isabella: [-0.29484093 -1.3075423  -0.07895859]\n",
            "brother: [ 0.4951375   0.16603601 -0.45859388]\n",
            "leontes: [-0.54069585 -1.1012084  -0.2060259 ]\n",
            "it,: [-0.5858245  -0.63359976 -0.29163045]\n",
            "not,: [-0.9944131  -0.7615928   0.47975555]\n",
            "pray: [-1.4767796   0.08101495  0.01110484]\n",
            "unto: [-0.68531543 -0.2864213   0.06353878]\n",
            "so,: [-1.0748502  -0.21602772  0.20546085]\n",
            "those: [0.13065523 0.11505133 0.41907093]\n",
            "me.: [-0.4977107  -1.0872823   0.14213394]\n",
            "blood: [ 0.4368562  -0.25018626 -0.44145212]\n",
            "away: [-0.8984811  -0.27622783  0.4804569 ]\n",
            "ever: [-0.1765967 -0.2269562  1.1030939]\n",
            "bear: [-0.6631911   1.0568546   0.04668284]\n",
            "comes: [-0.914992   -0.16973819  0.37752548]\n",
            "stand: [-0.6144526   0.35561284  0.06634042]\n",
            "angelo: [-0.45559928 -1.5338653  -0.06593165]\n",
            "nothing: [-0.32789597 -0.15854812  0.01312892]\n",
            "then,: [-0.11054575 -0.546966    0.40115872]\n",
            "day: [ 1.0475823  -0.21386696 -0.6522441 ]\n",
            "sicinius: [-0.57444215 -0.78309166 -0.22246167]\n",
            "could: [-0.33579198 -0.27754313  0.07919762]\n",
            "myself: [-0.44210863  0.10124454  0.27118626]\n",
            "other: [0.99135023 0.0844577  0.7543668 ]\n",
            "lucio: [ 0.2002389  -1.1975      0.13759392]\n",
            "way: [ 0.60239697  0.07225467 -0.2261085 ]\n",
            "put: [-0.43452102  1.1940194   0.09370545]\n",
            "fear: [-0.5978766  1.0667497 -0.463397 ]\n",
            "two: [0.45174763 0.03153016 0.5353202 ]\n",
            "prince: [ 0.61714965 -0.43022272 -0.62197185]\n",
            "bolingbroke: [-0.27209347 -0.22748724 -0.09765934]\n",
            "clarence: [-0.5640369  -0.9347667  -0.26047695]\n",
            "hand: [ 0.76222765  0.00951041 -0.7713375 ]\n",
            "nurse: [-0.5581359  -0.7454812  -0.43248022]\n",
            "o': [-0.12768248 -0.11515282  0.35614604]\n",
            "ere: [-0.70507807 -0.3641253   0.23199697]\n",
            "iv: [-1.0881547  -0.65558314  0.02032284]\n",
            "better: [ 0.2994044 -0.2764005  0.6730701]\n",
            "therefore: [-0.24605888 -0.3402235   0.46992666]\n",
            "since: [-0.51361144 -0.21919505  0.7429771 ]\n",
            "set: [-0.39638376  0.7563699   0.43232712]\n",
            "done: [-0.48386073  0.06373651  1.1595179 ]\n",
            "little: [0.96042144 0.15105478 0.5031618 ]\n",
            "say,: [-0.97717446  0.14513668  0.36297578]\n",
            "whom: [-0.50877166 -0.67640156 -0.35667717]\n",
            "grace: [ 0.5678165   0.533772   -0.81082284]\n",
            "elizabeth: [-1.0940357  -0.8976     -0.21495451]\n",
            "buckingham: [-0.5505818  -0.98702663 -0.08924887]\n",
            "capulet: [-0.2181569  -1.1402465  -0.00736144]\n",
            "eyes: [ 0.3829128  -0.10916356 -0.4996908 ]\n",
            "friar: [ 0.132119   -0.70092446 -0.6583365 ]\n",
            "heaven: [ 0.55823076 -0.639067   -0.41995466]\n",
            "down: [-0.42696938 -0.31889614  0.05399921]\n",
            "might: [-0.9883953   0.5885082   0.64276475]\n",
            "but,: [-1.1211935  -0.56704974  0.10681477]\n",
            "still: [-0.22059304  0.35029674  0.7888836 ]\n",
            "honour: [ 0.05264328  0.26197085 -0.26672772]\n",
            "margaret: [ 0.07325973 -0.9795144   0.18413201]\n",
            "tranio: [-0.2780202  -0.04398235  0.22337094]\n",
            "i,: [-0.32544515  0.05804747 -0.19909205]\n",
            "vi: [-0.19178677 -0.42343152  0.32457376]\n",
            "citizen: [-0.5169878  -0.5665051   0.05991921]\n",
            "stay: [-0.9095585   0.55709773 -0.32260537]\n",
            "keep: [-0.62580967  1.0078107  -0.5235265 ]\n",
            "bring: [-1.0149372   0.7044684  -0.10424957]\n",
            "ii: [-0.8138382  -0.59529704  0.05218642]\n",
            "young: [ 0.97400004 -0.26562393  0.3929459 ]\n",
            "every: [0.25297967 0.16644712 1.0104858 ]\n",
            "bid: [-0.5398189   0.60218346 -0.31795555]\n",
            "gentleman: [ 0.31959915 -0.4206941  -0.08325889]\n",
            "once: [-0.08829675 -0.28988165 -0.12992087]\n",
            "else: [-0.80025375 -0.2649148   0.44468385]\n",
            "katharina: [-0.34638962 -0.06737074  0.19343188]\n",
            "dear: [ 0.1929599  -0.17936493  0.3824654 ]\n",
            "off: [-0.55329305 -0.33693904  0.08754195]\n",
            "live: [-0.42917368  0.92779416 -0.16218676]\n",
            "gentle: [ 0.64179766 -0.33233187  0.27206787]\n",
            "into: [-0.6393753   0.04740564 -0.03941072]\n",
            "marcius: [-0.32014424 -0.7440889  -0.3927758 ]\n",
            "find: [-0.7695108   1.117074    0.08709138]\n",
            "thing: [ 0.29339382 -0.40805972 -0.01405532]\n",
            "world: [ 1.1269163   0.10270537 -0.6694609 ]\n",
            "brutus: [-0.30138472 -0.80897975 -0.11918078]\n",
            "again: [-0.5716542  -0.52100796 -0.00291169]\n",
            "long: [ 0.3414674  -0.02485602  0.9883995 ]\n",
            "best: [0.8089208  0.30550212 0.23849642]\n",
            "i': [-0.89793503  0.05996771  0.3960964 ]\n",
            "go,: [-0.23117873 -0.14019066  0.01593934]\n",
            "news: [-0.2293236  -0.6377208  -0.77069867]\n",
            "that's: [-0.5597281  -0.15907125  0.3458318 ]\n",
            "thine: [-0.4382991  -0.11824395  0.09324288]\n",
            "dost: [-0.51054376 -0.3143675   0.51068896]\n",
            "camillo: [-0.5128524  -1.545667   -0.05458801]\n",
            "came: [-0.87710303  0.5279289   0.4561057 ]\n",
            "word: [0.5134981  0.17821257 0.16461156]\n",
            "dead: [ 0.3372675 -0.0876959  0.4818901]\n",
            "master: [ 0.12073875 -0.49499983 -0.12740946]\n",
            "that,: [-0.35827166 -0.22828506  0.45130607]\n",
            "let's: [-0.5876855   0.16287634  0.22351664]\n",
            "head: [ 0.7110571   0.11305947 -0.5719182 ]\n",
            "king,: [ 0.56606567 -0.3670276  -0.09147259]\n",
            "makes: [-0.47155517  0.37576544  0.3934965 ]\n",
            "full: [0.156133  0.6287643 0.4497913]\n",
            "die: [-0.42597285  0.00490606 -0.34880945]\n",
            "what's: [-0.35267624 -0.3752815   0.14244133]\n",
            "hence: [-0.23254351 -0.04927539 -0.09753194]\n",
            "has: [-0.3901607   0.3012632   0.35688877]\n",
            "baptista: [-0.0578988  -0.00695719  0.07776988]\n",
            "please: [-0.794977   0.7786401 -0.4484411]\n",
            "about: [-0.6050878   0.62217987  0.3755954 ]\n",
            "death,: [ 0.20518918 -0.15896507 -0.63725275]\n",
            "show: [-0.5127109   1.1281552  -0.24359222]\n",
            "house: [ 1.0968      0.59168994 -0.7543559 ]\n",
            "night: [ 0.8178407  -0.2587836  -0.21126044]\n",
            "he's: [-0.2142503   0.00240462  0.8676791 ]\n",
            "heard: [-0.35845232  0.54456645  0.06363015]\n",
            "none: [-0.43363926 -0.14886236  0.5091254 ]\n",
            "escalus: [-1.1022063  -1.7334929  -0.46151748]\n",
            "lucentio: [-0.41381338 -0.21952796  0.16151753]\n",
            "words: [ 0.78786165 -0.05010483 -0.67845   ]\n",
            "soul: [ 0.594084  -0.3327445 -0.5485982]\n",
            "thou,: [-0.4007727  -0.60436183  0.29467222]\n",
            "father,: [ 0.34324485 -0.35296538 -0.7599996 ]\n",
            "thought: [-0.21959761  0.8287381  -0.11453918]\n",
            "after: [-0.43068388  0.02509095  0.25611714]\n",
            "lord.: [ 0.1767111  -0.63693845 -0.6967915 ]\n",
            "here,: [-0.45593122  0.00351363  0.31783658]\n",
            "love,: [ 0.46665448  0.20713708 -0.54460394]\n",
            "wilt: [-0.65254366  0.0893146   0.5039523 ]\n",
            "duchess: [-0.6750967  -0.69605297 -0.98645097]\n",
            "himself: [-0.3343489  -0.11223183  0.08210596]\n",
            "wife: [ 0.34831893 -0.1751338  -0.9360512 ]\n",
            "daughter: [ 0.56199586 -0.12672973 -0.22972845]\n",
            "provost: [-0.39550683 -0.94728965 -0.41564587]\n",
            "hortensio: [-0.08442505 -0.12875487  0.27228448]\n",
            "mother: [ 0.5280324  -0.16420683 -0.39514562]\n",
            "forth: [-0.65702724 -0.33713543  0.21467659]\n",
            "power: [0.51622057 0.631769   0.09450261]\n",
            "gracious: [ 0.45897797 -0.42866102  1.0087869 ]\n",
            "servant: [-0.33924162 -0.92502457  0.30056873]\n",
            "royal: [1.0154841  0.24824932 0.6102188 ]\n",
            "cominius: [-0.5745232  -0.7725075   0.32421938]\n",
            "part: [ 0.21705492  0.7884985  -0.6321738 ]\n",
            "grumio: [-0.23490664 -0.17773202  0.20708823]\n",
            "while: [ 0.19701137 -0.44277915  0.1395909 ]\n",
            "rest: [ 0.34379745  0.22197242 -0.92294407]\n",
            "tongue: [ 0.6071722  -0.12003802 -0.5421705 ]\n",
            "friends: [ 0.1452272 -0.4219671 -0.7798637]\n",
            "far: [ 0.16666704 -0.15385972  1.1807288 ]\n",
            "third: [-0.0891944  -0.6483854   0.15654832]\n",
            "hastings: [-0.6174577  -0.72156465 -0.5380619 ]\n",
            "thousand: [ 0.6212874 -0.0294205  0.8380548]\n",
            "peace: [ 0.4564468  -0.7123209  -0.47971264]\n",
            "hold: [-0.6250774   0.84949994 -0.7154069 ]\n",
            "we'll: [-0.49082276 -0.43416968  0.13261226]\n",
            "only: [-0.43921167  0.31095904 -0.02423489]\n",
            "shalt: [-0.2282534  0.1294275  0.6626516]\n",
            "pardon: [-0.861343   0.8186909 -0.6057518]\n",
            "hope: [0.13017806 0.29283988 0.07431242]\n",
            "pompey: [-0.92904866 -0.9490707   0.17323455]\n",
            "lay: [-0.8687988  0.9312011 -0.6956278]\n",
            "beseech: [-0.76819     0.42445135  0.15386376]\n",
            "gremio: [-0.16734871  0.04129771  0.17470965]\n",
            "rather: [-0.43947843 -0.00138152  0.621229  ]\n",
            "within: [-0.78803086  0.02052932 -0.09505223]\n",
            "cause: [0.51215005 0.32442668 0.57312554]\n",
            "son,: [ 0.13623123 -0.04668817 -0.27063775]\n",
            "murderer: [-0.17207664 -0.91106963 -0.39825162]\n",
            "autolycus: [-0.34517208 -0.9104194  -0.57488513]\n",
            "help: [-0.47418138  0.9314117  -0.514427  ]\n",
            "face: [ 0.5973413   0.3446288  -0.76362854]\n",
            "holy: [ 0.87948865 -0.10047752  0.9312159 ]\n",
            "life,: [ 0.06153694  0.06416743 -0.040953  ]\n",
            "benvolio: [-0.58097804 -0.90595794 -0.03748645]\n",
            "mercutio: [-0.7754068  -0.73320234 -0.26995277]\n",
            "back: [ 0.20937137  0.49895987 -0.4244358 ]\n",
            "use: [-0.22226386  0.57154036 -0.4782059 ]\n",
            "said: [-0.22919755  0.66021067  1.6007704 ]\n",
            "thank: [-1.3002807   0.5713445  -0.10107838]\n",
            "father's: [ 0.79956406 -0.08764366  0.08025761]\n",
            "prove: [-0.50574625  1.145697   -0.6589034 ]\n",
            "sir.: [-1.1251148  -1.3861696   0.59963065]\n",
            "happy: [0.2391924  0.09756798 1.0827235 ]\n",
            "paulina: [-0.719327  -1.3496134 -0.1900493]\n",
            "gone: [-0.40378454  0.02152362  1.1327772 ]\n",
            "meet: [-0.49464488  1.1476887   0.4393217 ]\n",
            "right: [ 0.5584231   0.35566702 -0.5971939 ]\n",
            "is,: [-0.80807704 -0.58208907  0.07678874]\n",
            "answer: [-0.24010196  1.0067803  -0.18595624]\n",
            "mind: [ 0.56917095  0.12348662 -0.40079638]\n",
            "tears: [ 0.6970336  -0.11010508 -0.71380913]\n",
            "polixenes: [-0.5097061  -1.0256398  -0.71650517]\n",
            "clown: [-0.6259267  -1.4286815  -0.46622285]\n",
            "prospero: [-0.26510173 -0.15918054  0.19536729]\n",
            "aufidius: [-0.7825602 -1.1778218 -0.3145882]\n",
            "madam,: [-0.8952543 -1.325582   0.2658365]\n",
            "without: [-0.03347369  0.21360932  0.89846635]\n",
            "thee.: [-0.5554425  -0.87862086 -0.30811104]\n",
            "hands: [ 0.40225008  0.09318717 -1.0037175 ]\n",
            "there's: [-0.5178017  -0.40960273  0.47869605]\n",
            "save: [-0.7998657   0.34956732 -1.5705677 ]\n",
            "mean: [ 0.02155296  0.8751489  -0.07875289]\n",
            "get: [-0.610302    0.53146243 -0.25025344]\n",
            "lords,: [-0.03621506 -0.9170686  -0.25676212]\n",
            "northumberland: [-0.22394547 -1.1830194  -0.14821847]\n",
            "this,: [ 0.10377191 -1.1839947   0.40313432]\n",
            "send: [-1.4266273   1.1707265  -0.37007698]\n",
            "another: [-0.03134403 -0.44977245  0.850589  ]\n",
            "lies: [-0.30737892 -0.00873753  0.3948526 ]\n",
            "mistress: [ 1.0672125  -0.55525804  0.18611315]\n",
            "her,: [-0.18960682 -0.0496549  -0.7289685 ]\n",
            "claudio: [-0.52594805 -0.86743754 -1.1202385 ]\n",
            "man,: [ 0.8830465 -0.8272712 -0.2315105]\n",
            "last: [ 0.4416531   0.16224864 -0.98971987]\n",
            "lie: [-0.33017364  0.5941754  -0.3819056 ]\n",
            "so.: [-0.5017626 -0.9144538  0.7271971]\n",
            "clifford: [-0.21223058 -0.6547226  -0.42398718]\n",
            "ne'er: [-0.44797334  0.64596564  0.39369166]\n",
            "things: [ 0.3521086  -0.09315953  0.05233592]\n",
            "three: [0.22391452 0.01759977 0.7332499 ]\n",
            "volumnia: [-0.3506568  -1.697941    0.12590948]\n",
            "believe: [-1.0795922   1.1541713  -0.09169865]\n",
            "crown: [ 0.7080627   0.39533547 -0.24904399]\n",
            "less: [0.19175617 0.02173303 0.6036059 ]\n",
            "saw: [-0.66343033  0.6155195   0.27778116]\n",
            "brother,: [ 0.05168809 -0.6820672  -0.30270892]\n",
            "foul: [0.7391411  0.38477176 0.4959137 ]\n",
            "laurence: [-0.38645667 -0.56044275 -0.16960043]\n",
            "them,: [-0.57182646 -0.6660332  -0.16898198]\n",
            "joy: [ 0.36336115 -0.07438039  0.05245526]\n",
            "shame: [ 0.546144   -0.00822407 -0.58208895]\n",
            "servingman: [-0.11237682 -0.84671026  0.17847952]\n",
            "sorrow: [ 0.64140755 -0.52273965 -0.5632519 ]\n",
            "us,: [-0.41224703 -0.84596735 -0.06913691]\n",
            "boy: [-0.20674725 -0.67789954 -0.4464554 ]\n",
            "ah,: [-0.8305494  -0.97386295 -0.1845608 ]\n",
            "worthy: [ 0.2720811  -0.12879123  0.66349465]\n",
            "kind: [ 0.8196564   0.01420963 -0.20233887]\n",
            "not.: [-0.9364899  -1.0176845   0.23594001]\n",
            "bloody: [ 1.5985205  -0.08105176  0.47774702]\n",
            "state: [ 1.0895499   0.15053429 -0.6390326 ]\n",
            "left: [-0.3172576  0.8086941  0.654708 ]\n",
            "because: [-0.85677814 -0.14152826  0.38629043]\n",
            "all,: [-0.25660104 -0.5956992   0.05587668]\n",
            "follow: [-0.67578447  1.0016818  -0.1026462 ]\n",
            "does: [-0.5914651  -0.09280981  0.78979224]\n",
            "turn: [-0.4235441   1.2283654  -0.64147115]\n",
            "home: [-0.40074188 -0.0635969  -0.6347043 ]\n",
            "husband: [ 0.26227978 -0.17885411 -0.2323007 ]\n",
            "anne: [-0.47967592 -0.63001955  0.33546174]\n",
            "under: [-0.30222502  0.58270097  0.43917486]\n",
            "therefore,: [-0.15237051 -0.25584805  0.4731716 ]\n",
            "earth: [ 0.58399516 -0.4808089  -0.58269894]\n",
            "false: [0.94213134 0.33800045 0.9134357 ]\n",
            "through: [-0.35742584 -0.09173405  0.24267843]\n",
            "high: [1.0302402 0.2450637 0.4874348]\n",
            "told: [-0.59047216  0.8526692   0.41217428]\n",
            "present: [0.19847038 0.6266094  0.43309724]\n",
            "place: [ 0.2238891   0.23332834 -0.37475616]\n",
            "people: [ 0.6686045   0.04552158 -0.7692229 ]\n",
            "fall: [-0.55368     1.2940621  -0.39194587]\n",
            "fortune: [ 0.30634135  0.04714068 -0.77876836]\n",
            "here's: [-0.8004908  -0.48780367  0.48849285]\n",
            "lives: [-0.11978791  0.38746965 -0.12366733]\n",
            "talk: [-0.51175356  0.72072953 -0.4058303 ]\n",
            "comfort: [ 0.30549917  0.34016848 -0.31352818]\n",
            "marry,: [-0.9465769  -0.48377228  0.40888715]\n",
            "grief: [ 0.7918056  -0.08998033 -0.32370678]\n",
            "he,: [-0.6991862  -0.821747   -0.02810728]\n",
            "messenger: [-0.0697551  -0.92905396 -0.27028072]\n",
            "means: [ 0.35895726 -0.01089228 -0.6125515 ]\n",
            "shepherd: [-0.1186246  -1.0699149  -0.59026945]\n",
            "years: [ 0.4360266  -0.10552715 -0.3699611 ]\n",
            "cousin: [ 0.10210737 -0.9470587  -1.1378924 ]\n",
            "swear: [-1.5764687   0.731458    0.01049051]\n",
            "gods: [ 0.49095228  0.04305489 -0.03263218]\n",
            "end: [-0.09178763  0.6953311  -0.7635953 ]\n",
            "need: [-0.03844116  0.5383544  -0.21714324]\n",
            "hither: [-0.23348539 -0.33275914  0.3501896 ]\n",
            "on,: [-0.492937   -0.46953598 -0.68921924]\n",
            "lords: [ 0.17412557 -1.1632901  -0.55929005]\n",
            "death.: [ 0.05715193 -0.60584235 -0.19223091]\n",
            "'twas: [-0.3178204   0.19046965  0.17333168]\n",
            "welcome: [-0.2471708   0.5321152  -0.18875848]\n",
            "sun: [ 1.2726007   0.40331674 -0.394586  ]\n",
            "bianca: [-0.2127872  -0.11916763  0.16346173]\n",
            "either: [-0.6442335  -0.0059377   0.24967334]\n",
            "grave: [ 0.6641443  -0.10943395  0.20490465]\n",
            "body: [ 0.9628928   0.22508784 -0.52993685]\n",
            "well.: [-0.4774842 -1.1902344  0.8745986]\n",
            "hate: [-0.28312704  0.66774815 -0.5526552 ]\n",
            "break: [-0.47521338  0.9923884  -0.30576766]\n",
            "see,: [-1.0103098   0.5092062   0.34223634]\n",
            "pity: [-0.11839732  0.7922178   0.0755858 ]\n",
            "god,: [ 0.1719017  -0.55668485 -0.48962405]\n",
            "edward's: [ 0.17260821 -0.14949961  0.8320325 ]\n",
            "signior: [ 0.3668621   0.07346255 -0.25222427]\n",
            "matter: [ 0.5138983  -0.2053107  -0.82931006]\n",
            "rome: [ 0.0633784  -0.55349755 -1.0576869 ]\n",
            "sit: [-0.55984855  0.5380773  -0.53210604]\n",
            "wish: [-0.2482829  1.5821428 -0.3920609]\n",
            "more,: [-0.00844213 -0.25355542 -0.33079302]\n",
            "hour: [ 1.114445   -0.10459618  0.5171231 ]\n",
            "each: [-0.06216399 -0.65767807  0.40189803]\n",
            "john: [ 0.13624196 -1.2393508  -0.3611639 ]\n",
            "florizel: [-0.73788476 -1.2013936   0.20858474]\n",
            "brought: [0.2690393 1.1328884 0.3480461]\n",
            "looks: [-0.06566482  0.09782557 -0.5512669 ]\n",
            "again.: [-0.5107677  -0.99421394 -0.29482678]\n",
            "eye: [ 0.7087794  0.1287714 -0.910908 ]\n",
            "heavy: [1.1896412  0.5913879  0.64357215]\n",
            "canst: [-0.00828218  0.02929356  0.5961004 ]\n",
            "warwick,: [ 0.00210467 -0.3533308  -0.09272526]\n",
            "gaunt: [-0.27744007 -0.16009963 -0.64150643]\n",
            "proud: [0.6864297 0.1109016 1.053804 ]\n",
            "up,: [-0.30745807 -0.6555156  -0.14062828]\n",
            "sent: [-0.5296876   1.0238899   0.23039344]\n",
            "new: [ 1.271096   -0.18170804  0.6944748 ]\n",
            "wife,: [-0.41264594 -0.03950908 -0.27277535]\n",
            "didst: [-0.37459147  0.31967777  0.43506813]\n",
            "aumerle: [-0.23358634 -0.9284692  -0.01651149]\n",
            "biondello: [-0.19598147 -0.07034992  0.10348371]\n",
            "friend: [ 0.0187799  -0.28518748 -0.00601086]\n",
            "senator: [-0.03405458 -0.71629614  0.13846788]\n",
            "play: [-0.8306146   0.89965737 -0.27748463]\n",
            "light: [ 0.70219374  0.3973366  -0.4439468 ]\n",
            "having: [-0.00092591  0.21637833  0.09182765]\n",
            "will,: [-0.89566123  0.14083253 -0.03641304]\n",
            "hand,: [ 0.21397513 -0.25169346 -0.5627103 ]\n",
            "montague: [-0.19155978 -0.4217798  -0.40135944]\n",
            "kill: [-0.40817136  1.2777749   0.52026874]\n",
            "for,: [-1.079212   -0.69260573  0.42025247]\n",
            "cry: [0.3180507  0.5229233  0.16921566]\n",
            "seem: [-0.3184737   1.4030188   0.37378025]\n",
            "truth: [ 0.4004699   0.10813093 -0.7153285 ]\n",
            "lest: [-1.0187528  -0.4863539   0.04998999]\n",
            "god's: [-0.19122015 -0.7506178   0.06720436]\n",
            "sebastian: [-0.22645742 -0.09846369  0.05647167]\n",
            "enough: [-0.28726712  0.05639719  0.72657305]\n",
            "us.: [-0.64475214 -0.97526294  0.07178167]\n",
            "common: [ 1.1103117  -0.3739883   0.18590143]\n",
            "fellow: [ 0.27422005 -0.32178694 -0.47399008]\n",
            "you'll: [ 0.09354888 -0.88456017  0.32168117]\n",
            "war: [ 0.7655126   0.28034973 -0.25796053]\n",
            "fight: [-0.36312982  0.7999046  -0.6212649 ]\n",
            "know,: [-0.85478586  0.41842106 -0.89678574]\n",
            "'twere: [0.2142103  0.10531964 0.5033098 ]\n",
            "them.: [-0.396121  -1.4680839 -0.0765611]\n",
            "be,: [-1.158145    0.34296662  0.1540096 ]\n",
            "her.: [-1.0903454  -1.0147066  -0.27556184]\n",
            "city: [ 0.78926444  0.14854975 -0.4020996 ]\n",
            "friends,: [ 0.02556424 -0.72079146 -0.11840729]\n",
            "care: [-0.06276829  0.09414522 -0.03034494]\n",
            "blood,: [ 0.30110046 -0.10328878 -0.6128398 ]\n",
            "mark: [-0.92710376  0.9337256  -0.82651615]\n",
            "seen: [-0.5152396   0.90025634  0.5137554 ]\n",
            "reason: [ 0.11045309  0.31754088 -0.27735588]\n",
            "nature: [ 0.72974885 -0.0651372   0.13953955]\n",
            "law: [ 0.9691553  -0.4629671  -0.47593677]\n",
            "she's: [-0.18836042 -0.08651651  0.7868047 ]\n",
            "to-morrow: [-0.46462494 -0.5505963   0.60917354]\n",
            "lost: [-0.5305021   0.4757795   0.05740045]\n",
            "grey: [-0.45797434 -0.73949504 -0.18840326]\n",
            "catesby: [-0.62379026 -1.1678706   0.5740276 ]\n",
            "paris: [ 0.28832868 -1.0099545   0.54354036]\n",
            "hermione: [-0.97569746 -1.2022539  -0.15282291]\n",
            "business: [ 1.0764762  -0.29253593 -0.0397455 ]\n",
            "do,: [-1.4445368  -0.18709473 -0.404991  ]\n",
            "yet,: [-0.6515064  -0.39888805  0.62131923]\n",
            "knows: [-0.30503786 -0.48468167  0.09788264]\n",
            "mother,: [ 0.6180184  -0.338968   -0.34627184]\n",
            "free: [0.321967  1.1291362 0.6671424]\n",
            "mercy: [ 0.2666278  -0.07136978  0.1569881 ]\n",
            "there,: [-0.27946717  0.4014812   0.521649  ]\n",
            "all.: [-0.5572183  -0.95278347 -0.2259778 ]\n",
            "something: [-0.06031249  0.3236821   0.7236406 ]\n",
            "cut: [-0.6990601   0.8285574  -0.39440647]\n",
            "twenty: [0.1399294  0.04020421 1.2874445 ]\n",
            "kiss: [-0.66185194  0.92031646 -0.60774857]\n",
            "child: [ 1.1620075 -0.623335  -0.5686081]\n",
            "king.: [ 0.7177556  -0.35819367  0.48485497]\n",
            "tybalt: [ 0.01884639 -1.3207008  -0.18171105]\n",
            "speak,: [-1.0924481  0.3523395 -0.2892738]\n",
            "yield: [-1.4792429  0.8896041 -0.2792489]\n",
            "rivers: [-0.20660168 -1.2839305  -0.5831921 ]\n",
            "says: [-0.8731238  -0.52575135  0.26391938]\n",
            "strange: [ 0.7017684  -0.06112792  0.96013767]\n",
            "days: [ 0.75504625 -0.830204   -0.45300695]\n",
            "indeed,: [-0.4568447  -0.09631957  0.5567664 ]\n",
            "children: [ 0.5810792  -0.07856681 -0.05704846]\n",
            "warrant: [-0.35449183  0.7516556   0.11929941]\n",
            "horse: [ 0.80134714 -0.5084019  -0.23773894]\n",
            "heaven,: [ 0.2551616 -0.6472936 -1.0958449]\n",
            "done,: [-0.4906544  -0.14370152  0.75988156]\n",
            "dare: [-0.6146246   0.48223114  0.7644439 ]\n",
            "which,: [-0.3730752  -0.52520335 -0.5562107 ]\n",
            "faith,: [ 0.2703317  -0.02156875 -0.5068708 ]\n",
            "peace,: [-0.25276232  0.19060692  0.05253329]\n",
            "unless: [-0.9896677   0.13932061  0.04421385]\n",
            "queen,: [ 0.5611496  -0.24830252  0.01078029]\n",
            "tender: [0.30351406 0.30383885 0.6386694 ]\n",
            "antonio: [-0.30230108 -0.17608172  0.308142  ]\n",
            "speak.: [-1.2107038  -0.40811682 -0.67120624]\n",
            "arms: [ 0.29108414 -0.03312419 -1.0285692 ]\n",
            "indeed: [-1.1309956  -0.15263207 -0.17845358]\n",
            "justice: [ 0.39721668 -0.37974027 -0.8280428 ]\n",
            "pray,: [-0.60676736 -0.01673609  0.24531938]\n",
            "yourself: [-0.20268242  0.04603718  0.5395118 ]\n",
            "brave: [ 1.0023326  -0.24100472  0.26961857]\n",
            "deep: [1.3591093  0.31478915 0.4224306 ]\n",
            "desire: [-0.26729247  1.4182622  -0.6059696 ]\n",
            "remember: [-0.99699897  1.300357    0.00692951]\n",
            "soon: [0.24798083 0.05307294 1.0801263 ]\n",
            "cold: [ 0.7848219  -0.08343602  0.46542296]\n",
            "farewell: [-0.10082646 -0.39697313  0.03136104]\n",
            "married: [-0.5169242  0.6606284  1.4940413]\n",
            "gonzalo: [-0.25319692 -0.05518321  0.23414996]\n",
            "first,: [ 0.25821844 -0.20061632  0.19353457]\n",
            "content: [-0.17358147  0.27206194  0.65995896]\n",
            "report: [-0.17415467  1.2137457  -0.44053394]\n",
            "come.: [-0.68772537  0.49171498 -0.9394718 ]\n",
            "strike: [-0.9139598   0.82353956 -0.48251027]\n",
            "fire: [1.1890261  0.05303197 0.34671903]\n",
            "ye: [-0.33986685 -0.6902327  -0.05374952]\n",
            "bed: [ 0.08800913  0.03139715 -0.91580296]\n",
            "found: [-0.24271706  0.65563214  0.9933039 ]\n",
            "lose: [-0.72640854  0.6022498  -0.51156646]\n",
            "beat: [-0.59307545  1.5333952  -0.15055922]\n",
            "stands: [-0.74595135  0.38254386  0.19632897]\n",
            "sound: [0.506688   0.26829886 0.0553241 ]\n",
            "o'er: [-0.8336156  -0.49406588  0.18685998]\n",
            "times: [ 0.90544784 -0.8191454   0.03478828]\n",
            "mine,: [-0.32641983  0.05998162  0.5847736 ]\n",
            "breath: [ 0.85667497  0.28753552 -0.64466244]\n",
            "again,: [-0.05204225 -0.31109023  0.14790733]\n",
            "women: [ 0.2592405  -0.13408111 -0.46794143]\n",
            "lewis: [-0.25885674  0.11371888 -0.02138193]\n",
            "become: [-1.0502155   1.0626318   0.18886347]\n",
            "ten: [0.32907534 0.2910383  0.73775643]\n",
            "heart,: [ 0.6219616  -0.37493074 -1.1472259 ]\n",
            "ready: [-0.20892926  0.08751347  0.5059132 ]\n",
            "trust: [-0.6507772   0.52655244 -0.3746707 ]\n",
            "heavens: [ 1.387532   -0.16727242  0.08123575]\n",
            "return: [-0.71958905  1.5675694   0.04572048]\n",
            "sleep: [-0.06052798  0.38480556 -0.04940799]\n",
            "wear: [-0.18008564  0.9306282  -0.4496079 ]\n",
            "too,: [-1.1552002  -0.13950063  0.35013336]\n",
            "honour,: [ 0.32998657 -0.57331383 -0.04413011]\n",
            "fearful: [1.31403    0.18020043 0.8997232 ]\n",
            "near: [0.15008634 0.508314   0.5583336 ]\n",
            "sea: [ 1.2204072  -0.13940278 -0.7316633 ]\n",
            "thyself: [-0.568887   -0.4494929  -0.17119282]\n",
            "knew: [-0.7226863   0.826678    0.34792468]\n",
            "oath: [ 0.42862153  0.72086287 -0.4218859 ]\n",
            "land: [ 0.3629768   0.26086035 -0.49862134]\n",
            "is't: [ 0.08148336 -0.01770447  0.5520012 ]\n",
            "change: [-0.24743347  0.48000202 -0.91348463]\n",
            "ears: [ 0.5994591   0.06518313 -0.12211861]\n",
            "yes,: [-0.5941395 -0.8846346  0.4050508]\n",
            "doubt: [-0.15758339  0.5416042  -0.08206295]\n",
            "love.: [ 0.08751319 -0.5777606  -0.33315045]\n",
            "seek: [-0.28958246  0.89910936 -0.4265932 ]\n",
            "charge: [ 0.10819996  0.49264312 -0.6672266 ]\n",
            "hours: [ 0.61450005 -0.6368939  -0.3832735 ]\n",
            "late: [ 0.03033728 -0.08301082  0.32669342]\n",
            "draw: [-0.9373589   1.46335    -0.42347008]\n",
            "time,: [ 0.37472713 -0.52577853  0.03872821]\n",
            "yea,: [-0.62642944 -0.30062193  0.8024695 ]\n",
            "brother's: [ 0.9804498  -0.46620405 -0.6392657 ]\n",
            "gave: [-0.62293947  0.47624615  0.10359167]\n",
            "loving: [0.81528586 0.533327   0.4978895 ]\n",
            "straight: [-0.6710848  -0.1105992   0.15249404]\n",
            "name,: [ 0.44313812  0.2614376  -0.6324145 ]\n",
            "maid: [ 0.5873558   0.14115459 -0.18679287]\n",
            "hence,: [-0.9089409   0.06922814 -0.2523137 ]\n",
            "bound: [-0.15926561  0.7876566  -0.24556363]\n",
            "given: [-0.29438394  0.8783243   0.8583665 ]\n",
            "serve: [-0.9226807  0.8221314 -0.6306442]\n",
            "world,: [ 0.21119924 -0.1256306  -0.82983834]\n",
            "woman: [ 1.1276262   0.02560377 -0.03278089]\n",
            "read: [-0.4460214   0.8447064   0.01395632]\n",
            "past: [0.18171555 0.34197664 0.78531945]\n",
            "day,: [ 0.72838926 -0.56737983 -0.3829482 ]\n",
            "farewell,: [-0.06982586 -1.1569083  -0.15058431]\n",
            "villain: [ 0.990993   -0.05633413 -0.60408175]\n",
            "majesty: [ 0.37684292 -0.26629424 -0.693262  ]\n",
            "romeo,: [-0.20774192 -0.7763695   0.08188265]\n",
            "kate: [-0.23837313  0.14653938  0.27106065]\n",
            "miranda: [-0.19890007 -0.18254516  0.23978461]\n",
            "honest: [-0.25534153  0.18973565  1.0889169 ]\n",
            "more.: [ 0.13608895 -0.5468313  -0.34339774]\n",
            "general: [0.69247085 0.14532994 0.10736037]\n",
            "half: [-0.01078267  0.6930761   0.03474372]\n",
            "known: [-0.11596045  0.7238076   0.5208288 ]\n",
            "pluck: [-0.9196766   1.0281739  -0.08699001]\n",
            "gone,: [-0.34128037 -0.22586566  1.044922  ]\n",
            "open: [0.5478316  0.48360026 0.17892186]\n",
            "took: [-0.60435754  0.8700854   0.73024   ]\n",
            "ill: [ 0.539103   -0.09978978  1.1927979 ]\n",
            "welcome,: [-0.48216113 -0.47353882  0.39680263]\n",
            "live,: [-0.68751675  0.55341405 -0.45975968]\n",
            "traitor: [ 0.95088726 -0.3311217  -0.22850075]\n",
            "king's: [ 1.388564   -0.22288035  0.2595488 ]\n",
            "beauty: [ 0.7480976   0.65234137 -0.10714152]\n",
            "good,: [-0.28256685 -0.64475095 -0.35189146]\n",
            "cousin,: [ 0.4051026  -0.3787986  -0.41926265]\n",
            "woe: [ 0.02274302 -0.16216017  0.63039523]\n",
            "richmond: [-0.2210111  -0.22669835 -0.50969386]\n",
            "sin: [ 0.3338079   0.11876114 -0.22116947]\n",
            "between: [-0.45012382 -0.17236483 -0.04026866]\n",
            "born: [-0.23418221  1.079813    1.6706933 ]\n",
            "fetch: [-0.8338333   0.7817398  -0.36417738]\n",
            "neither: [-0.83771497  0.06293441  0.5998643 ]\n",
            "citizens: [-0.526267  -0.8865516 -0.693418 ]\n",
            "person: [ 0.39587152 -0.08835316 -0.2887887 ]\n",
            "loss: [ 0.57359105  0.2853079  -0.97788835]\n",
            "heir: [ 0.73824966  0.06146824 -0.5051514 ]\n",
            "prince,: [ 0.5510371   0.05966106 -0.40588745]\n",
            "night,: [ 0.29998645 -0.6062075  -0.6239011 ]\n",
            "elbow: [-0.74251604 -1.6485643   0.22857207]\n",
            "loved: [-0.5673619   0.84446007  0.01400004]\n",
            "true,: [-0.47072473 -0.55885905  0.74287677]\n",
            "sworn: [-0.03951932  0.0561161   0.2676132 ]\n",
            "over: [-0.50401896  0.15677324 -0.05962456]\n",
            "prithee,: [-0.5007053  -0.09613364  0.1627645 ]\n",
            "hearts: [ 0.5931531   0.6691591  -0.77893233]\n",
            "men,: [ 0.40626833 -1.1289914  -1.0387082 ]\n",
            "whilst: [-0.04705096 -0.45828354  0.18400243]\n",
            "away.: [-0.5059183  -0.30071518  0.70110047]\n",
            "sword: [ 0.36395842 -0.09977362 -0.5433322 ]\n",
            "same: [1.04082    0.27474344 0.36848035]\n",
            "fly: [-0.8952942  0.9404922 -0.7614233]\n",
            "itself: [ 0.05000804 -0.3013126  -0.38825804]\n",
            "eyes,: [ 0.2694837  -0.5411899  -0.51838505]\n",
            "foot: [ 0.87958777  0.71407473 -0.5456274 ]\n",
            "sight: [ 0.67121    -0.01188981 -0.5563206 ]\n",
            "widow: [ 0.69617623  0.02235505 -0.46136978]\n",
            "saint: [0.6217591  0.23613822 0.29061943]\n",
            "marry: [-0.41881272  0.41001764 -1.0669703 ]\n",
            "edward,: [-0.47044277 -0.5742231  -0.18748531]\n",
            "thomas: [-0.10389774 -0.7165544   0.5478267 ]\n",
            "crown,: [ 0.6827576   0.26741785 -0.6344717 ]\n",
            "fit: [-0.8360998   0.75016147  0.5452181 ]\n",
            "who,: [-0.55527866 -0.3987345   1.0375603 ]\n",
            "together: [-0.48049226 -0.51090187  0.12871158]\n",
            "look,: [-0.5302528 -0.8261621 -0.2627799]\n",
            "rome,: [ 0.08043037 -0.45929798 -0.2575507 ]\n",
            "hadst: [-0.87363994  0.6807571   0.3464865 ]\n",
            "often: [-0.11153432  0.40975598  0.84933233]\n",
            "fault: [ 0.67791927 -0.17328231 -0.2955009 ]\n",
            "friend,: [ 1.1112199 -0.6022891 -0.9195007]\n",
            "sad: [ 1.2235942  -0.29896128  1.088968  ]\n",
            "duty: [ 0.5884309  0.341762  -0.2781429]\n",
            "earth,: [ 0.36012635  0.202967   -0.58199203]\n",
            "dead,: [-0.8387799  -0.68434846  0.3594061 ]\n",
            "liege,: [ 0.21225882 -0.48039234 -0.9060121 ]\n",
            "post: [-0.552657    0.14210242 -0.04302965]\n",
            "away,: [-0.3220549  -0.38996124  0.52872396]\n",
            "head,: [ 0.18037258  0.21365283 -0.8622026 ]\n",
            "win: [-1.452595    1.4250518  -0.49957603]\n",
            "attend: [-1.1669517   0.48185423 -0.9702269 ]\n",
            "madam: [ 0.06482174 -0.7368092   0.5025492 ]\n",
            "look'd: [-0.5349817   0.31686673  0.8239124 ]\n",
            "run: [-0.3514327   0.94558436 -0.1737347 ]\n",
            "pale: [0.9823217  0.0139997  0.40567294]\n",
            "here.: [-0.51395863 -0.9891393   0.2682161 ]\n",
            "princely: [ 0.9545322  -0.42125353  0.17272481]\n",
            "ask: [-1.3572571   0.16317369 -0.1241395 ]\n",
            "worth: [ 0.41347286  0.8333542  -0.0018775 ]\n",
            "grant: [-0.8690418   0.96982366 -0.3335889 ]\n",
            "die.: [-0.7405564   0.13096415  0.06516106]\n",
            "weep: [-1.425066   0.3750853 -0.3339518]\n",
            "myself,: [-0.49584472 -0.23288716  0.57837486]\n",
            "face,: [ 0.66815215 -0.07540273 -0.68782556]\n",
            "down,: [-0.35353875 -0.69300795 -0.44253674]\n",
            "slain: [0.02534906 0.63584846 0.7213305 ]\n",
            "next: [0.762296   0.32940114 0.44727117]\n",
            "uncle: [ 0.6647431   0.0152995  -0.29690504]\n",
            "mad: [ 0.40426347 -0.44788244  0.62391317]\n",
            "perdita: [-0.48398206 -1.1795672  -0.96612453]\n",
            "revenge: [-0.08989988  0.32558346 -1.1096057 ]\n",
            "confess: [-1.2018815   0.35575435 -0.2797592 ]\n",
            "arm: [ 0.3819173   0.85427195 -0.69559133]\n",
            "think,: [-0.6039866   0.6389341  -0.25445628]\n",
            "have,: [-1.3108261   0.32091582  0.2701496 ]\n",
            "virtue: [ 1.1119306   0.12907279 -0.57714605]\n",
            "rich: [ 0.8648103  -0.00256092  1.1272215 ]\n",
            "throw: [-0.394044    0.55897653 -0.30769816]\n",
            "worse: [0.45472708 0.5759274  0.9633905 ]\n",
            "way,: [ 0.65985876 -0.06391707 -0.4049734 ]\n",
            "methinks: [-1.3068572  -0.7184283   0.13705638]\n",
            "spoke: [-0.22235827  0.19216798  1.002687  ]\n",
            "ho: [-0.8098839  -1.3432679  -0.06259056]\n",
            "haste: [-0.6262387   0.05142359 -0.80427915]\n",
            "gentlemen,: [-0.5235061  -0.6083725  -0.10706471]\n",
            "coming: [ 0.96922326 -0.5595798   0.7721977 ]\n",
            "purpose: [ 0.2628509   0.7755229  -0.88331413]\n",
            "ground: [ 0.47456408 -0.26032484 -0.9101412 ]\n",
            "service: [ 0.389762    0.53517646 -0.62005687]\n",
            "spirit: [ 0.39661944  0.31999767 -0.4076786 ]\n",
            "entreat: [-1.596274    0.8945195  -0.36151832]\n",
            "said,: [-1.0477128  0.6583693  0.9722637]\n",
            "house,: [ 0.09126443 -0.06586023 -0.73462796]\n",
            "banish'd: [0.9237252  0.20055404 0.43531126]\n",
            "thoughts: [0.01713532 0.44350362 0.15418828]\n",
            "alas,: [-0.8035867  -0.47909176 -0.46070883]\n",
            "one,: [ 0.19337347 -1.0081365  -0.00756145]\n",
            "counsel: [-0.5188401   0.42952597 -0.6123905 ]\n",
            "clarence,: [-0.4200173  -0.28414315 -0.36721   ]\n",
            "mighty: [ 0.82957935 -0.06333247  0.6264614 ]\n",
            "soul,: [ 0.18599583  0.06755925 -0.95792556]\n",
            "title: [ 0.8515762   0.36602005 -0.18857187]\n",
            "sly: [1.7395566 0.1679473 0.0851416]\n",
            "peter: [-0.49920058 -1.2552428  -0.17001939]\n",
            "strong: [ 0.7115169  -0.05549596  1.0915692 ]\n",
            "curse: [ 0.12012667  0.6069442  -0.06988896]\n",
            "pretty: [1.1429456  0.21284948 0.593359  ]\n",
            "almost: [0.05601215 0.4459098  1.2348541 ]\n",
            "five: [0.34868425 0.21085653 0.4593433 ]\n",
            "home,: [-0.09068958 -0.40813476 -0.44019902]\n",
            "goes: [-0.9934879  -0.41026893  0.18945664]\n",
            "whether: [-0.8424539  -0.43126982  0.19967134]\n",
            "wherein: [-1.0985159   0.561502   -0.16756253]\n",
            "fie,: [-0.99774003 -0.44045305 -0.23013398]\n",
            "buy: [-0.90964967  1.2517891   0.08768116]\n",
            "dangerous: [1.2116975 0.8275046 0.9543668]\n",
            "bad: [0.79073757 0.628412   0.8222058 ]\n",
            "wert: [-0.5930044   0.3991341   0.08859364]\n",
            "e'er: [-0.21733966 -0.1193131   0.14972636]\n",
            "heart.: [ 0.95620376 -0.6977389  -0.16638729]\n",
            "pleasure: [ 0.75872487 -0.17042498 -0.99111056]\n",
            "suit: [ 0.28149015  0.04593115 -0.5202637 ]\n",
            "call'd: [0.15471338 1.031298   0.6286992 ]\n",
            "voices: [ 0.7993472  -0.40379187 -0.2582299 ]\n",
            "behold: [-0.6222757   0.49193066 -0.14950907]\n",
            "gone.: [-0.04890665 -0.11404043  0.6068721 ]\n",
            "life.: [ 0.1714817 -0.5518713 -0.431133 ]\n",
            "speed: [-0.07659196  0.27703628  0.37406862]\n",
            "wrong: [-0.16541782 -0.10989089  0.4561824 ]\n",
            "sister: [ 0.5483013  -0.19413961 -0.9571437 ]\n",
            "earl: [ 0.46938702  0.51363105 -1.1347708 ]\n",
            "master,: [ 0.43229842  0.00493106 -0.20146553]\n",
            "3: [-0.528682   -1.2279284  -0.04668949]\n",
            "mariana: [-0.34417772 -0.08637316  0.1311082 ]\n",
            "enemy: [ 0.6086036   0.13856314 -0.2510093 ]\n",
            "soft: [0.54461783 0.8479475  0.630031  ]\n",
            "yours: [0.13347106 0.0531468  0.71043974]\n",
            "farewell.: [-0.2422116  -0.62328416 -0.03131758]\n",
            "daughter,: [ 0.63479155 -0.24517356 -0.20555612]\n",
            "whence: [-0.66693604 -1.2573645   0.29750666]\n",
            "man.: [ 0.2907118  -0.6579041   0.53702533]\n",
            "got: [-0.6919775   0.3001576   0.14922763]\n",
            "prayers: [ 0.5245111   0.44248158 -1.057155  ]\n",
            "mine.: [-0.19791196 -0.87358654 -0.01620122]\n",
            "or,: [-0.28262818 -0.45684454 -0.5136391 ]\n",
            "gates: [ 0.710955   -0.21726961  0.03683561]\n",
            "merry: [0.49689618 0.6500547  0.45611817]\n",
            "air: [ 0.77765983  0.507765   -0.7798095 ]\n",
            "wounds: [ 0.03967197  0.24494326 -0.5810457 ]\n",
            "ha: [-0.7038499  -1.3239594   0.60092115]\n",
            "ancient: [ 0.8384966  -0.20104916  0.23896918]\n",
            "ear: [ 1.2807484  0.1864757 -0.0114022]\n",
            "age: [ 1.3838252   0.20649108 -0.55009574]\n",
            "die,: [-0.342804    1.0085675  -0.13051246]\n",
            "towards: [-0.06937055  0.1932155   1.0920041 ]\n",
            "strength: [ 0.7615641   0.27447075 -0.66901743]\n",
            "sirrah,: [-0.39761993 -1.3975513   0.13392855]\n",
            "court: [ 1.6552727   0.05029415 -0.2605157 ]\n",
            "just: [0.509244   0.24806088 0.73179847]\n",
            "richard,: [-0.35920358 -0.3243447   0.30053422]\n",
            "france: [-0.1743945 -0.3455658 -1.0530301]\n",
            "uncle,: [-0.17690213 -0.97108394 -1.0244395 ]\n",
            "stanley: [-0.53124833 -1.3157738   0.5870279 ]\n",
            "marriage: [ 0.6627993   0.32219738 -0.7852842 ]\n",
            "kate,: [-0.20660384 -0.04331316  0.11263785]\n",
            "tale: [ 1.3290334  -0.09403301 -0.38504243]\n",
            "point: [ 0.57125473  0.5022317  -1.2303106 ]\n",
            "man's: [ 1.3589166  -0.15085734  0.9210154 ]\n",
            "bold: [ 0.88005173 -0.10611873  1.0694317 ]\n",
            "themselves: [-0.74870056 -0.12678495 -0.58081794]\n",
            "virgilia: [-1.2810234  -1.1255628  -0.18425098]\n",
            "want: [-0.41243654  0.88525134 -0.30493072]\n",
            "lartius: [-0.6416379  -0.42439288  0.33991534]\n",
            "hark: [-0.46858385 -0.12870388 -0.11844306]\n",
            "issue: [ 0.41792116  0.6440274  -0.24374828]\n",
            "further: [-0.25221038  0.42377603  0.5467407 ]\n",
            "souls: [ 0.6099866  -0.05737593 -0.38636142]\n",
            "wast: [-0.25630647  0.5758681   0.3431228 ]\n",
            "above: [-0.25985965  0.27107844  0.8353728 ]\n",
            "wouldst: [-0.21120028 -0.17617434  0.84029615]\n",
            "learn: [-0.7895762   1.3389994  -0.55008566]\n",
            "loves: [-0.05749349  0.9245379   0.4431572 ]\n",
            "fast: [ 0.6711023  -0.32793805  0.8553644 ]\n",
            "sovereign: [ 0.5046185  -0.45351934  0.03170428]\n",
            "forget: [-0.54841375  1.3327179  -0.38188198]\n",
            "deserved: [-0.3265603   0.60622793  1.270317  ]\n",
            "deed: [ 0.712943   -0.04314177 -0.05525865]\n",
            "stay,: [-0.85193175 -0.0800973  -0.35401276]\n",
            "now.: [-0.9126335  -0.9827773   0.45659176]\n",
            "presence: [ 0.7040223  0.5178259 -1.445326 ]\n",
            "quarrel: [ 0.56395495  0.86243635 -1.1822981 ]\n",
            "nurse,: [ 0.31619236 -0.5388695  -0.1880811 ]\n",
            "boy,: [ 0.03728217 -0.5184949  -0.17477229]\n",
            "black: [0.7212338  0.13136804 0.7370593 ]\n",
            "dream: [0.85557944 1.3355594  0.0418522 ]\n",
            "green: [ 0.40480915 -0.75895387  0.0412297 ]\n",
            "york,: [-0.02758881  0.07145838 -0.4315273 ]\n",
            "roman: [ 0.6730182  -0.11692881 -0.39017695]\n",
            "state,: [ 1.0741912 -0.1701636 -0.2931093]\n",
            "patience: [-0.49929246 -0.1080962  -0.27627873]\n",
            "battle: [ 0.46207005 -0.3419602  -0.06873532]\n",
            "valiant: [ 0.51233596 -0.7124174   1.1551647 ]\n",
            "needs: [-0.646117    0.31099972  0.7907898 ]\n",
            "youth: [ 0.46467912 -0.00153989 -0.7675218 ]\n",
            "night.: [-0.01530065 -0.89283156 -0.40635583]\n",
            "march: [ 0.01440988  0.49642074 -0.11873579]\n",
            "deny: [-1.0749738  1.5025734  0.8907624]\n",
            "pay: [-0.7840791  0.9085967 -1.0733413]\n",
            "begin: [-0.30088943  1.1531795  -0.15558074]\n",
            "people,: [ 1.0212578  -0.40323606  0.30177456]\n",
            "go.: [-1.0713516  -0.6366604  -0.45187542]\n",
            "officer: [-0.02609919 -0.96468073  0.34041753]\n",
            "measure: [ 0.51397485  0.09089419 -1.0412672 ]\n",
            "pass: [-0.56837696  1.3824309  -0.5527341 ]\n",
            "goodly: [ 1.5423425  -0.31530812  1.564506  ]\n",
            "lawful: [ 0.6175396  -0.31778893  0.70731467]\n",
            "say.: [-0.66216147 -0.20287094 -0.3567843 ]\n",
            "witness: [-0.3042802   1.32933     0.05406198]\n",
            "knock: [-0.6466801   0.14667639 -0.4055597 ]\n",
            "kill'd: [-0.44658116  0.73987657  0.39186096]\n",
            "george: [-0.40993786  0.11910924 -0.9669662 ]\n",
            "henry's: [ 0.6525982  -0.6446541   0.51803195]\n",
            "golden: [1.6269908  0.3087738  0.58737147]\n",
            "england's: [0.45332143 0.08583351 1.2297292 ]\n",
            "keeper: [-0.20098254 -1.0362269  -0.47858796]\n",
            "oxford: [-0.03169445 -1.3268659  -0.7280734 ]\n",
            "seems: [-0.7057852   0.678379    0.46143097]\n",
            "norfolk: [-0.40383172 -0.5133679   0.3585336 ]\n",
            "father.: [ 0.4394149  -0.40800422 -0.3785464 ]\n",
            "word,: [ 0.8514072  -0.5418516  -0.18766855]\n",
            "too.: [-0.810698   -0.67742014 -0.1359828 ]\n",
            "eye,: [ 0.36581737 -0.24217896 -0.58467007]\n",
            "wise: [0.77252114 0.25431982 0.8901574 ]\n",
            "hang: [-0.89262056  0.4248977  -0.21466143]\n",
            "greater: [1.0678955  0.11815526 1.1611491 ]\n",
            "fool: [-0.10973634  0.27909288  0.28633833]\n",
            "he'll: [ 0.11732452 -0.97415596  0.9497384 ]\n",
            "met: [-0.9720716   1.0967416   0.45039472]\n",
            "feel: [-1.093194    0.43112284 -0.135803  ]\n",
            "wind: [ 0.33100942  0.11819532 -0.31035087]\n",
            "i.: [-0.39581296 -1.0891546   0.09956016]\n",
            "down.: [-0.6359335  -0.68711543 -0.06688954]\n",
            "consent: [ 0.24858405  1.008958   -0.7704804 ]\n",
            "toward: [-0.724806    0.82476354  0.7149769 ]\n",
            "touch: [ 0.08202413  1.163238   -0.38691613]\n",
            "office: [ 1.0884757   0.72060466 -1.0892422 ]\n",
            "sure: [-0.19525267  0.6040013   0.9578871 ]\n",
            "wherefore: [-0.9275054  -0.8363458   0.84905887]\n",
            "slew: [-0.41157025 -0.04053864  0.07551914]\n",
            "out,: [-1.2591928  -0.58801144 -0.4212324 ]\n",
            "move: [-0.4738274  1.130418  -0.4718295]\n",
            "to-day: [-0.3486678  -0.545651    0.52814335]\n",
            "until: [-0.28291577 -0.47158703 -0.0594838 ]\n",
            "murder: [-0.8905846   0.7773936  -0.80973524]\n",
            "friar,: [ 0.6010099  -0.22515443 -0.6307942 ]\n",
            "rest,: [ 0.78286463  0.1980941  -1.059364  ]\n",
            "small: [1.1990334  0.22248231 0.88521975]\n",
            "defend: [-0.63558865  0.84619033 -0.48014164]\n",
            "marcius,: [-0.4820398  -0.13110857 -0.00157299]\n",
            "wonder: [-0.4783452   0.19679013 -0.57065314]\n",
            "held: [0.029923   0.02113545 0.4698972 ]\n",
            "whither: [-0.9494344  -0.99387735 -0.12635262]\n",
            "field: [ 1.1428385   0.26525533 -0.5751894 ]\n",
            "hearing: [ 0.59478515  0.22949727 -0.18874389]\n",
            "fear,: [-0.20271282 -0.1936173   0.46462685]\n",
            "done.: [-0.5306143  -0.58889014  0.6059315 ]\n",
            "work: [ 0.09268498 -0.500345   -0.65948457]\n",
            "base: [1.4155316  0.2595368  0.26744944]\n",
            "gold: [ 0.1565313  -0.57567245 -0.05807874]\n",
            "letters: [ 0.38910025  0.85403734 -0.16104451]\n",
            "lady,: [ 0.03726283 -0.61031246 -0.68384403]\n",
            "out.: [-0.9391694  -1.1421165  -0.02306982]\n",
            "lips: [ 0.8317     0.4714551 -0.8124654]\n",
            "mortal: [1.6690582  0.4658658  0.65451574]\n",
            "wit: [ 1.1865623  -0.11673354 -0.34678283]\n",
            "child,: [ 0.4927471  -0.40901974  0.10793778]\n",
            "watch: [-0.3061311   0.64284503 -1.059975  ]\n",
            "himself,: [-0.31338653 -0.4990474   0.48279864]\n",
            "guilty: [0.8599479  0.46162027 0.6586019 ]\n",
            "order: [ 0.22783373 -0.03380135 -0.42370656]\n",
            "day.: [ 0.8770232 -1.048821  -0.0394161]\n",
            "mayor: [ 0.11976623 -0.6349809  -0.11485028]\n",
            "music: [ 0.63504744  0.20091787 -0.03994661]\n",
            "pedant: [-0.34811062 -0.16956899  0.12454954]\n",
            "ariel: [-0.23591514 -0.04418677  0.28456914]\n",
            "country: [ 0.35036573 -0.02396939 -0.23391788]\n",
            "side: [0.10362833 0.24245238 0.06342527]\n",
            "course: [-0.09058075 -0.46676642 -1.3885609 ]\n",
            "appear: [-0.76983064  1.0007921   0.28459078]\n",
            "wars: [ 0.7901767  -0.01586382 -0.07210527]\n",
            "tribunes: [ 0.5592408   0.07389172 -0.7392845 ]\n",
            "where's: [-0.59403133 -0.4177868   0.5957562 ]\n",
            "glad: [-0.00728881 -0.24262653  0.9739269 ]\n",
            "is.: [-1.0061293  -1.0736378   0.45278394]\n",
            "chance: [-0.08278105  0.5765194  -0.06132835]\n",
            "visit: [-0.85104084  0.36142454 -0.15046923]\n",
            "myself.: [-1.10932   -1.7730188 -0.6011707]\n",
            "tear: [-0.14630733  1.078519   -0.5758156 ]\n",
            "along: [-0.0745403  -0.43985176  0.24358304]\n",
            "hell: [ 0.38696632 -0.76647586 -0.522161  ]\n",
            "this.: [-0.48765838  0.0136582  -0.7049371 ]\n",
            "breathe: [-1.2518619  0.5654496 -0.8414726]\n",
            "will.: [-0.470606   -0.44468486 -0.10149577]\n",
            "shed: [-0.6064902   0.0028714   0.32087228]\n",
            "country's: [0.8301702  0.35889083 0.00828444]\n",
            "hard: [0.368421   0.14357959 1.2046758 ]\n",
            "grow: [-0.20995054  1.0665748   0.07811165]\n",
            "morning: [ 0.6907434  -0.4651821  -0.23625384]\n",
            "may,: [-0.4670939 -0.5768789  1.02109  ]\n",
            "'twixt: [-0.6501923   0.01938771  0.2534623 ]\n",
            "heads: [ 0.285064    0.18878451 -0.6011258 ]\n",
            "virtuous: [1.1902462  0.41840696 1.2391696 ]\n",
            "water: [ 0.60362816  0.6350588  -0.30131486]\n",
            "commend: [-0.75517607  0.64858663  0.21674049]\n",
            "rage: [ 0.6552839   0.85954833 -0.21596976]\n",
            "promise: [ 0.37756222  0.6026611  -0.7769314 ]\n",
            "oft: [-0.5971261   0.22458066  0.95358264]\n",
            "bed,: [ 0.14147188  0.1585489  -0.8324734 ]\n",
            "command: [-0.550307   1.1785136 -0.4309471]\n",
            "soldiers: [0.6642979  0.16915613 0.4173791 ]\n",
            "living: [0.73441553 0.3932887  0.21904889]\n",
            "case: [ 1.2924479  -0.7524785  -0.15124473]\n",
            "highness: [ 0.8548195   0.5826758  -0.21919736]\n",
            "lancaster: [ 0.57455736 -0.6772722  -0.26310706]\n",
            "villain,: [ 0.740074   -0.95456487 -0.35634285]\n",
            "henry,: [ 0.19179824 -0.6690576  -0.32918382]\n",
            "hastings,: [-0.47209772 -0.3107944   0.01799309]\n",
            "to-night: [-1.2614955 -0.4760991  0.1927281]\n",
            "sovereign,: [ 0.0752728  0.9379137 -0.5028522]\n",
            "ratcliff: [-1.1337621  -1.2047734  -0.05283808]\n",
            "subject: [ 1.3371625   0.33660433 -0.25002354]\n",
            "norfolk,: [-0.18843229 -0.06755479 -0.05813871]\n",
            "mowbray: [-0.21737115 -0.38804302  0.02746158]\n",
            "prison: [ 0.14636815  0.41101918 -1.775805  ]\n",
            "watchman: [-0.74551463 -1.2316661  -0.69351476]\n",
            "somerset: [-0.34377337 -1.1517223  -0.46320495]\n",
            "clifford,: [ 0.05738495 -1.0173327   0.09326319]\n",
            "xi: [-0.47943562 -0.58969957 -0.08445303]\n",
            "antigonus: [-0.05305039 -1.8170795   0.5765861 ]\n",
            "alack,: [-1.1846331  -0.77538455 -0.50033957]\n",
            "speaks: [-0.615864   -0.00738871  0.14356722]\n",
            "quoth: [-0.61065423 -1.2758002  -0.75112355]\n",
            "lead: [-0.73483455  0.9753898  -0.6053908 ]\n",
            "sword,: [ 0.4560352  -0.17789748 -1.468963  ]\n",
            "already: [ 0.13330509 -0.08433637  0.4769427 ]\n",
            "alone: [-0.62576485 -0.27530667 -0.10363456]\n",
            "both,: [-0.2958296  0.2939373 -0.1948005]\n",
            "jest: [-0.02679284  0.17814244 -0.7832625 ]\n",
            "yours,: [-0.01912695  0.12516558  0.06645925]\n",
            "hundred: [1.5522612  0.59660584 1.3713716 ]\n",
            "off,: [-0.03943833 -0.7817638   0.11400376]\n",
            "shows: [0.10686441 0.38093704 0.29006425]\n",
            "deeds: [ 0.5654338  -0.65079564 -0.6909846 ]\n",
            "hide: [-0.5723443  1.5112011 -0.6972236]\n",
            "'gainst: [ 0.09234089 -0.1733616  -0.2817608 ]\n",
            "in,: [-0.5107654  -0.5505921  -0.10594156]\n",
            "hot: [0.5541071  0.09620278 0.880031  ]\n",
            "calls: [-0.85389364  0.00551304 -0.18035537]\n",
            "twice: [-0.39293242 -0.379043    0.22973864]\n",
            "piece: [ 0.27556294  0.6515702  -0.7821334 ]\n",
            "kept: [ 0.14871268  0.7041884  -0.16760533]\n",
            "place,: [ 0.5303492  -0.90135974 -0.5823449 ]\n",
            "on.: [-0.47482765 -1.1941338  -0.2043611 ]\n",
            "laid: [-0.22506693 -0.03289078  0.87821376]\n",
            "there.: [-0.69167393 -1.4352019  -0.60227245]\n",
            "blessed: [ 0.8509132  -0.63973117  1.0354416 ]\n",
            "prepare: [-0.45336413  0.6676366  -0.2568733 ]\n",
            "news,: [ 0.3932631 -1.6895028 -0.0870054]\n",
            "forward: [ 0.4628669  -0.11570074  1.3331379 ]\n",
            "bosom: [ 1.0184761   0.74565786 -0.81208724]\n",
            "close: [-0.05739386  0.20262401  0.3100584 ]\n",
            "hither,: [ 0.04344976 -0.59930146  0.6961687 ]\n",
            "sampson: [-0.29580665 -1.3687541   0.54548746]\n",
            "curtis: [-0.21464248 -0.30422792  0.21996874]\n",
            "thither: [-0.1743126  -0.8374912   0.24253505]\n",
            "hear,: [-1.0251745   1.1123024  -0.06615033]\n",
            "note: [0.59197366 0.8157231  0.0983764 ]\n",
            "worst: [1.2824087  0.11411221 0.13796426]\n",
            "making: [-0.03263701 -0.05309395 -0.09250435]\n",
            "broke: [-0.11954685  0.90026927  0.63411176]\n",
            "four: [0.08686214 0.45582157 0.7404625 ]\n",
            "thence: [0.13471095 1.0032321  0.66657835]\n",
            "becomes: [-0.64814615  0.10198191  0.4605278 ]\n",
            "fare: [-0.77517384 -0.4380176   0.07564397]\n",
            "soldier: [ 0.17306268 -0.37716562  0.5734742 ]\n",
            "certain: [ 0.3800159  -0.10664917  1.1951997 ]\n",
            "thanks: [-0.05497359  0.89088976 -0.0477941 ]\n",
            "quickly: [0.14259814 1.0855536  1.3917148 ]\n",
            "ta'en: [-0.51164126  0.13257797  0.9271176 ]\n",
            "wash: [-0.9640995  1.2305692 -0.5057829]\n",
            "angry: [0.82166064 0.28140056 0.62185997]\n",
            "deadly: [0.72249585 0.5678591  0.9482497 ]\n",
            "true.: [-0.23787443 -0.89989066  0.71192455]\n",
            "gives: [-0.5940678   0.46376956  0.22740725]\n",
            "faith: [ 0.5590803   0.24677607 -0.85793686]\n",
            "white: [ 1.3716215   0.3090448  -0.05765362]\n",
            "stood: [-0.69779605  1.0949864   0.05282249]\n",
            "sudden: [0.6903574  0.50727284 0.7731504 ]\n",
            "precious: [0.94107234 0.09647888 0.641642  ]\n",
            "sure,: [-0.5293551  -0.69867545  1.2666669 ]\n",
            "poison: [ 0.4239924   0.18051404 -0.20039201]\n",
            "choose: [-0.5706834   0.69497067  0.0266619 ]\n",
            "enter: [-0.2198404   0.49161825 -1.1040921 ]\n",
            "withal: [-0.74965274 -0.3987208   0.7573219 ]\n",
            "shake: [-0.592406    0.82696545 -0.33726248]\n",
            "forbid: [-1.104156    0.74143696  0.31115872]\n",
            "rough: [ 0.91412574 -0.19994631  1.1598767 ]\n",
            "teach: [-1.0600752  0.7245243  0.0288553]\n",
            "going: [0.07259305 1.0077667  0.51277786]\n",
            "hand.: [-0.12380427 -0.6622144  -1.3354385 ]\n",
            "book: [ 1.0891681  0.3024823 -0.9671278]\n",
            "fresh: [1.0172887  0.26135215 0.64702296]\n",
            "she,: [-0.38625458  0.1700814   0.01160655]\n",
            "tears,: [-0.01013102  0.700307   -0.85651416]\n",
            "hope,: [-0.3660803   0.58909106 -0.43179178]\n",
            "men's: [ 0.90601474 -0.34540406 -0.40691772]\n",
            "kingdom: [ 1.2270672  -0.19753729 -1.8371586 ]\n",
            "alas: [-1.3913589  -1.1643643   0.05827065]\n",
            "dead.: [-0.07589528 -0.45811883  0.26235548]\n",
            "london: [-0.41873077  0.3814679  -0.74760497]\n",
            "early: [0.3391066  0.10319196 1.0839672 ]\n",
            "tybalt,: [-0.55883646 -0.7398662  -0.7292708 ]\n",
            "camillo,: [-0.30195722  0.06700309 -0.2451082 ]\n",
            "nature,: [ 1.0726149   0.4487716  -0.46389073]\n",
            "suffer: [-0.763709    0.90240216 -0.47777423]\n",
            "act: [ 0.47628984  0.87170017 -0.7554834 ]\n",
            "deliver: [-1.0639406   0.87060285 -0.17065996]\n",
            "offence: [ 0.0428489  -0.17734925  0.14712992]\n",
            "behind: [-0.9942979  -0.2466854   0.14861575]\n",
            "faults: [ 0.2467497  -0.23983897 -0.54849416]\n",
            "husband,: [ 0.30397418 -0.4190579   0.5482452 ]\n",
            "won: [-0.31031314  0.18265638  0.528775  ]\n",
            "bless: [-1.2598022   0.59453875 -0.488759  ]\n",
            "fine: [0.49797913 0.24783494 0.09637552]\n",
            "shall,: [-1.0706102   0.47856712  0.79090583]\n",
            "no.: [-0.5888731 -1.2164648  0.3622586]\n",
            "noise: [ 0.39830935  0.32468212 -0.19594939]\n",
            "enemies: [0.36536998 0.41640827 0.14584929]\n",
            "carry: [-0.7175664  1.0238166  0.5537161]\n",
            "gentleman,: [ 0.5899428  -0.2609726   0.25304964]\n",
            "whiles: [-0.9255764  -0.59430414  1.3511614 ]\n",
            "together,: [ 0.23811151 -0.657166   -0.11268959]\n",
            "are,: [-0.53303814  0.30123398  0.03574162]\n",
            "beg: [-0.5905816  0.7764436 -1.2434757]\n",
            "request: [-0.4457349   0.5676858  -0.55562335]\n",
            "war,: [ 1.0168157  -0.5210784  -0.01459322]\n",
            "blows: [ 0.16203329 -0.53556544  0.36364022]\n",
            "dry: [-0.41713414  0.8746754  -0.70844716]\n",
            "fled: [-0.6731157  0.3146584  1.0884072]\n",
            "struck: [0.21174285 0.75561404 0.43779778]\n",
            "voice: [ 0.60621065  0.8300597  -0.95665413]\n",
            "honour.: [-0.43363678 -0.27699876 -0.31950715]\n",
            "ourselves: [-0.1021035  -0.5426001   0.04269087]\n",
            "six: [ 0.5830786  -0.00553794  1.0342829 ]\n",
            "humble: [0.84190804 0.26440057 0.3390082 ]\n",
            "speech: [ 0.9117727   0.22647823 -0.6492459 ]\n",
            "bears: [-0.13622564  0.87437147  0.02225243]\n",
            "stir: [ 0.12944339  0.66774935 -0.23489998]\n",
            "time.: [-0.31150228 -0.2764072  -0.26978412]\n",
            "world.: [ 0.44084668 -0.39552507 -0.30668315]\n",
            "son.: [-0.1440178  -0.50399476 -0.5923295 ]\n",
            "forgot: [-0.49227363  0.44281805  0.78871393]\n",
            "shame,: [ 0.19071443  0.07380331 -0.4250728 ]\n",
            "bids: [-0.8186574   0.29451373  0.24448462]\n",
            "be.: [-0.7953205  -0.38429078 -0.78563493]\n",
            "hands,: [ 0.4622055 -0.1384171 -0.6152293]\n",
            "part,: [ 4.7718850e-01 -3.3038811e-04 -4.1964573e-01]\n",
            "was,: [-0.30003846  0.05920767  0.60717773]\n",
            "weak: [1.6049287  0.21915701 0.42446426]\n",
            "queen's: [ 1.3928283   0.27943316 -0.40202013]\n",
            "sister,: [ 0.4373635   0.52645224 -0.35791355]\n",
            "secret: [0.7907282  0.25481802 0.8464866 ]\n",
            "gentlemen: [-0.25523913 -0.15813014  0.6883614 ]\n",
            "cheer: [ 0.2965433   0.40068033 -0.82902527]\n",
            "bitter: [0.7762842  0.7419576  0.75439954]\n",
            "kings: [ 0.61030036 -0.5900476  -0.13365711]\n",
            "right,: [-0.26365602  0.04356284 -0.37835136]\n",
            "stay.: [-0.9732649   0.16583829 -0.8012038 ]\n",
            "grace,: [ 0.5662029   0.24802373 -0.54940313]\n",
            "york.: [-0.08322652 -0.7820147   0.2125698 ]\n",
            "england: [ 0.15608573 -0.6003925   0.03311142]\n",
            "weeping: [ 0.7191878  -0.21804263 -0.5564803 ]\n",
            "english: [ 1.4264864 -0.6399117  0.2829691]\n",
            "montague,: [-0.90819275 -0.88229513 -0.8322913 ]\n",
            "juliet,: [-0.05059071 -0.53871864 -0.23803692]\n",
            "angelo,: [-0.75747097  0.06622489  0.17018983]\n",
            "barnardine: [-0.29703256 -0.1366318   0.2960597 ]\n",
            "tranio,: [-0.3248209  -0.13338579  0.18725637]\n",
            "always: [ 0.18385102 -0.20928809  0.9894923 ]\n",
            "'t: [-0.34859088 -0.67442054  0.17024823]\n",
            "idle: [0.9182007 0.5382147 0.5052461]\n",
            "fought: [-0.11457641  0.15833178  1.2341067 ]\n",
            "company: [ 0.15735127 -0.24943697 -0.34842408]\n",
            "grown: [0.8109933  0.27118492 1.3977629 ]\n",
            "power,: [ 0.56553847  0.10922223 -0.16553116]\n",
            "cruel: [0.99946594 0.88544804 1.1647649 ]\n",
            "truth,: [-0.01415848 -0.27774328 -0.6003911 ]\n",
            "lend: [-0.7300358  1.1585542  0.5369142]\n",
            "blow: [-0.19794373  1.31157    -1.1605574 ]\n",
            "foe: [1.1141388  0.26776928 0.16740718]\n",
            "page: [-1.0676627  -1.4068656  -0.24560317]\n",
            "foolish: [ 0.9485765  -0.03548494  1.0915662 ]\n",
            "tent: [ 0.22629888  0.2584969  -0.5760942 ]\n",
            "sometime: [-0.21035753  0.02945274  0.7602423 ]\n",
            "weary: [1.3639892  0.83095825 0.20495877]\n",
            "least: [ 0.9211181  -0.3726473   0.07961764]\n",
            "honourable: [ 1.044872  -0.2001525  1.0787494]\n",
            "dark: [1.2534631  0.208383   0.64526933]\n",
            "cast: [-0.96624786  0.29033446  0.573497  ]\n",
            "among: [-0.29342413 -0.01063703  0.41557935]\n",
            "vengeance: [ 0.33724788 -0.6635445  -1.1014216 ]\n",
            "harm: [ 0.5306718   0.08682404 -0.40392327]\n",
            "flesh: [ 0.21986252  0.13169453 -1.4976047 ]\n",
            "wife.: [ 0.44941187 -0.07084033 -0.08533175]\n",
            "trouble: [-0.8772691  0.5868453 -0.2598278]\n",
            "crave: [-0.45206442  1.3211985  -0.6764991 ]\n",
            "do.: [-1.1232867   0.09768675 -0.5051965 ]\n",
            "words,: [ 0.10276624 -0.48828888 -0.4536185 ]\n",
            "none,: [-0.5073452  -0.68910426 -0.4212787 ]\n",
            "hold,: [-0.14448519 -0.87164193 -0.46122804]\n",
            "try: [-1.753589    0.42390606 -1.2465571 ]\n",
            "presently: [-0.01321193  0.16371538  0.10351102]\n",
            "stop: [-0.6312572   0.93528736 -0.38386872]\n",
            "longer: [0.5241306  0.040321   0.39626458]\n",
            "fellow,: [-0.32782605 -0.903399    0.38899875]\n",
            "bite: [-0.67457503  1.0591497  -0.3309095 ]\n",
            "mean,: [-0.62688696  0.780581    0.38316447]\n",
            "betwixt: [-0.48363963  0.35888287 -0.3572557 ]\n",
            "leisure: [0.4545143  0.3231473  0.10990036]\n",
            "rutland: [-0.44288856 -0.9869086  -0.6525977 ]\n",
            "derby: [-0.441987   -0.5035585  -0.16221361]\n",
            "queen.: [-0.02499621  0.02851545  0.2682724 ]\n",
            "buckingham,: [-0.28678253 -0.32772562 -0.12374415]\n",
            "live.: [-1.4171406   0.87829393 -0.21043226]\n",
            "sacred: [1.710889  0.1657091 0.4407539]\n",
            "thanks,: [-0.0663291   0.18726082 -0.63297707]\n",
            "musician: [-0.58443016 -0.5687505  -0.06492762]\n",
            "remain: [-0.9164727   0.34588134 -0.7579875 ]\n",
            "seat: [ 0.60825    -0.2542111  -0.44900623]\n",
            "hate,: [ 0.18503538  0.03030839 -0.95340616]\n",
            "who's: [-0.5070937  -0.5767172  -0.20557098]\n",
            "lack: [ 0.07643272  0.46276727 -0.5491053 ]\n",
            "volsces: [ 0.8186707   0.03262648 -0.786106  ]\n",
            "i'ld: [-0.3903986  -0.58311254  0.6554751 ]\n",
            "besides,: [-0.45219    -0.74069923  0.3364991 ]\n",
            "letter: [ 0.7036955  -0.5729353  -0.32517987]\n",
            "therein: [-0.85176176 -0.24249703  0.19657797]\n",
            "valeria: [-0.56110245 -1.1062287  -0.9303726 ]\n",
            "short: [0.6985172  0.55161864 1.0017829 ]\n",
            "mock: [-0.9565336   0.22487292  0.41613665]\n",
            "prisoner: [0.5759239  0.26008654 1.1330345 ]\n",
            "doing: [0.21487424 0.01727072 0.04611875]\n",
            "drink: [-1.1482328   0.45213267 -0.93056357]\n",
            "subjects: [ 0.10100698 -0.1261571  -0.58522457]\n",
            "seven: [0.65879446 0.1944489  0.56168365]\n",
            "lived: [-0.46229228  0.53371054  1.0226355 ]\n",
            "sea,: [ 1.1600876  -0.07396802 -0.53478086]\n",
            "sense: [ 0.5118396  -0.29657084 -0.43554762]\n",
            "sake,: [ 0.00177085  0.1007316  -0.53489447]\n",
            "conscience: [ 0.60516804 -0.28888118 -1.0349318 ]\n",
            "wisdom: [0.56948066 0.62402385 0.31125513]\n",
            "were,: [-0.4609137   0.53469473  0.36051866]\n",
            "help,: [-0.36676863 -0.18923013 -0.07805379]\n",
            "fie: [-0.73696643 -0.8202      0.29759654]\n",
            "leave,: [ 0.15409797 -0.32984453 -0.34189698]\n",
            "although: [-0.90640426 -0.5837344   0.22735693]\n",
            "mayst: [-0.21878096  0.56543887  0.18619289]\n",
            "thinks: [-0.11301412  0.32089162  0.04734868]\n",
            "love's: [ 0.83366233 -0.00828867  0.10819906]\n",
            "fool,: [ 0.57927716 -0.30852115  0.4827793 ]\n",
            "vow: [-0.42192525 -0.27101701 -0.434739  ]\n",
            "kneel: [-0.1028387   0.9998073  -0.40922174]\n",
            "mother's: [1.5432127  0.85771006 0.32449377]\n",
            "brakenbury: [-0.12890862 -0.9170126  -0.04251684]\n",
            "cursed: [0.41914755 0.22965826 0.07467803]\n",
            "devil: [ 0.8074493 -0.2532433 -0.4863781]\n",
            "wrong,: [-0.27253008 -0.10418411  0.28704578]\n",
            "grace.: [ 0.11292541 -0.45748636 -1.1025591 ]\n",
            "walk: [-1.0542948  0.7079483 -1.3620143]\n",
            "tut,: [-0.5827599  -0.07665357  0.36993504]\n",
            "sons: [ 1.1141235  1.0198338 -0.3941634]\n",
            "pardon,: [-0.6135693   0.13275252 -0.39511123]\n",
            "bishop: [ 0.0947868   0.27687904 -0.5795463 ]\n",
            "promised: [-0.2392639  1.1860996  1.0234538]\n",
            "offer: [ 0.21591792  0.54482025 -0.3933505 ]\n",
            "year: [ 1.0117735  -0.5755692  -0.47155645]\n",
            "gregory: [-0.15708044 -1.0779763   0.05965132]\n",
            "worship: [ 0.07653146  0.6054122  -0.9180902 ]\n",
            "exeter: [-0.0242298 -1.0809232  0.0409133]\n",
            "bohemia: [ 0.20164402  0.04150818 -0.11914659]\n",
            "overdone: [-1.0989949  -0.99579287 -0.37082532]\n",
            "petruchio,: [-0.19276492  0.08052491  0.19985533]\n",
            "proceed: [-0.17405832  1.0074693  -0.22816144]\n",
            "caius: [-0.17548166 -0.51884127 -0.24981886]\n",
            "arms,: [ 0.543022   0.160973  -1.1345286]\n",
            "former: [ 0.67392284 -0.9395606  -0.35375264]\n",
            "word.: [-0.09105156  0.0342603  -0.24773312]\n",
            "sick: [-0.14631267  0.18549639  0.5879525 ]\n",
            "vile: [0.6314585  0.30977264 0.83733433]\n",
            "aufidius,: [-0.05837762 -0.21448913 -0.4068802 ]\n",
            "face.: [ 0.52776253 -0.9210751  -1.1052835 ]\n",
            "honours: [ 0.34341416  0.5388433  -0.42359522]\n",
            "dispatch: [ 0.07079612  0.2625538  -1.5060214 ]\n",
            "danger: [ 0.6360921  -0.17479903 -0.17779839]\n",
            "business,: [ 0.7297702 -0.8729792 -0.6047906]\n",
            "fell: [0.46648753 0.7014304  0.30821115]\n",
            "knee: [ 0.2607351   0.2626441  -0.79049605]\n",
            "brief: [ 0.25631562 -0.1259876   1.1606302 ]\n",
            "art,: [0.15588883 0.75742793 0.0212414 ]\n",
            "drop: [-0.10404609  0.81312096 -0.41225708]\n",
            "plague: [ 0.13215412  0.30948332 -0.47752145]\n",
            "to,: [-1.1380442   0.01790978  0.17271282]\n",
            "slave,: [ 0.40262255 -0.7002499  -0.02666356]\n",
            "dull: [ 0.7008899 -0.5007212  1.2360778]\n",
            "perceive: [-0.49963418  0.6636584  -0.5550867 ]\n",
            "cried: [0.32312125 0.652615   0.6135805 ]\n",
            "encounter: [-0.17184184  0.77361757 -0.6916822 ]\n",
            "deserve: [-0.8478566   1.4998444  -0.22128141]\n",
            "action: [ 0.75902253 -0.18401544 -0.5943155 ]\n",
            "leaves: [ 0.6528037   0.26690853 -0.5435402 ]\n",
            "manner: [ 0.7068508   0.07708526 -0.78869945]\n",
            "last,: [ 0.33913565  0.1495142  -1.0168147 ]\n",
            "by,: [-0.68554986 -0.03886465  0.37490714]\n",
            "taken: [ 0.260883   -0.18933423  0.43753994]\n",
            "thing,: [ 0.7851749  -0.07043846 -0.37188923]\n",
            "seal: [-0.2592538  -0.09199052 -0.27427402]\n",
            "anon: [-0.7950584 -0.4125864  0.6885035]\n",
            "warm: [ 1.0514346  -0.34688053  0.5191946 ]\n",
            "show'd: [-0.24151787  0.75295734  0.3112576 ]\n",
            "doubt,: [-0.28618684  0.18961965  0.12490491]\n",
            "mouth: [ 0.85774314  0.17149661 -1.1839621 ]\n",
            "taste: [-0.09335211  1.1110414  -0.6100425 ]\n",
            "traitors: [ 0.51794755 -0.05026807  0.1861085 ]\n",
            "law,: [ 1.1060443   0.2088984  -0.15552644]\n",
            "bones: [ 1.3287668   0.4679414  -0.20413327]\n",
            "seize: [-0.1939276  1.3632792 -0.1088011]\n",
            "scorn: [-0.34445366  1.6701921  -0.90963286]\n",
            "do't: [-0.9987159   0.13370061 -0.36919427]\n",
            "tongue,: [ 0.48543444  0.7991612  -0.16991875]\n",
            "peace.: [ 0.6229073  -0.53835523 -1.2765591 ]\n",
            "lady's: [1.2374328  0.07812003 0.08264194]\n",
            "one.: [-0.09165615 -0.15037179  0.24091768]\n",
            "know'st: [-1.2199873   0.9102792   0.12699838]\n",
            "quit: [-0.6743771  0.761617  -0.5805664]\n",
            "much.: [-0.315975   -0.83798873  0.44534004]\n",
            "woman's: [ 1.5574769  -0.03309993  0.23358409]\n",
            "throne: [ 0.99840015  0.46728402 -1.4039005 ]\n",
            "flowers: [ 0.9247203  -0.18011044 -0.07213706]\n",
            "few: [0.9872927 0.2880412 1.4217944]\n",
            "quiet: [1.107379  0.5383129 1.4465802]\n",
            "daughter.: [ 0.37097692 -1.0169775  -0.6143056 ]\n",
            "fatal: [ 1.6848109  0.5058282 -0.0858166]\n",
            "wicked: [ 0.8021558  -0.04882988  0.9840431 ]\n",
            "thereof: [-4.5069135e-04 -1.7050928e-01 -5.8821177e-01]\n",
            "broken: [1.0303769  0.05166014 1.0745393 ]\n",
            "fain: [-0.44254282 -0.41443372  0.21970484]\n",
            "woes: [ 0.69186825  0.6383363  -0.93436605]\n",
            "desperate: [0.9049332  0.64361185 1.3755257 ]\n",
            "head.: [ 0.8387119  -0.44428858 -1.2327304 ]\n",
            "went: [-0.6009184   0.05329894 -0.11362299]\n",
            "duke,: [ 0.8782658 -0.4770082 -0.6739392]\n",
            "swift: [2.1246154  0.40369147 2.100594  ]\n",
            "woo: [ 0.02578253  0.5594536  -1.0592034 ]\n",
            "proclaim: [-1.079107    0.57360005 -0.41091722]\n",
            "bushy: [-0.86760634 -0.34816736 -0.13074943]\n",
            "county: [ 0.9588193  -1.0182457  -0.26114863]\n",
            "mamillius: [-0.7006294  -0.9534142  -0.13646275]\n",
            "abhorson: [-0.3500072  -0.08427344  0.24702464]\n",
            "dog: [ 0.8119981   0.10296311 -0.96907187]\n",
            "intend: [-0.43402702  1.1175863   0.41174027]\n",
            "knees: [ 0.6914704   0.23343264 -1.5134308 ]\n",
            "receive: [-0.5601995   0.5396778  -0.25286645]\n",
            "flatter: [-0.2286136   0.8419177  -0.05279639]\n",
            "fire,: [ 0.69080913  0.10704638 -0.4548613 ]\n",
            "passing: [0.6306717  0.23539202 0.80025417]\n",
            "aught: [-0.18593858 -0.35933992  0.06979343]\n",
            "hie: [-0.824361   -0.12032747 -0.28479794]\n",
            "corioli: [-0.29282853  0.18114895 -0.8828443 ]\n",
            "troth,: [-0.03661125 -0.03573211 -0.38958162]\n",
            "alone,: [-0.6634911  -0.02993747  0.10318249]\n",
            "then.: [-0.30710372 -0.70750886  0.27519196]\n",
            "thoughts,: [-0.23393296 -0.565958   -0.386087  ]\n",
            "up.: [-0.66685057 -1.525571    0.7187254 ]\n",
            "friends.: [ 0.2842383 -0.5139195 -0.7650545]\n",
            "hour,: [ 0.35056174 -0.19390386 -0.08845745]\n",
            "own.: [ 0.31756333 -0.4348163  -0.41523075]\n",
            "shouldst: [-0.2575864  -0.6455132   0.68808436]\n",
            "before.: [-1.0621214  -1.4153349  -0.11118319]\n",
            "fair,: [ 0.13538449 -0.61180925  0.46497124]\n",
            "write: [-0.56470203  0.7979632  -0.8653375 ]\n",
            "in't: [-1.25051     0.05946605  1.1350261 ]\n",
            "rotten: [1.6821101  0.27265462 0.8518241 ]\n",
            "alone.: [-0.39388272 -1.215542    0.09373453]\n",
            "victory: [ 0.2847755  -0.581549   -0.03331918]\n",
            "right.: [ 0.16887882 -0.38225952 -0.57358927]\n",
            "wanton: [1.3765104  0.5162738  0.49860796]\n",
            "easy: [1.2470429  0.67389077 1.7237592 ]\n",
            "consul: [-0.15979077 -0.0878819   0.38933918]\n",
            "knowledge: [ 0.731599    0.33457878 -0.05479781]\n",
            "much,: [ 0.73497146 -0.07314478  0.6867915 ]\n",
            "breast: [ 0.63199806 -0.09754312 -0.44399256]\n",
            "owe: [-0.4721909   0.98503816 -0.11817349]\n",
            "company.: [-0.10164449 -0.52253526  0.5340446 ]\n",
            "spake: [-0.23070851  0.5972907   0.22273396]\n",
            "touch'd: [0.3117373  0.48949754 1.179089  ]\n",
            "repent: [-0.9775846   1.1033964  -0.43851092]\n",
            "home.: [-0.4021168  -0.1881265  -0.50743765]\n",
            "plain: [ 0.8633449  -0.35408974  0.7834416 ]\n",
            "sleep,: [ 1.0824146  0.1807431 -0.660873 ]\n",
            "frown: [ 0.5512323   0.9733597  -0.84706247]\n",
            "traitor,: [0.4537933  0.08900452 0.06128935]\n",
            "treason: [ 0.16383895 -0.4847848  -0.34354112]\n",
            "cure: [-0.10369915  0.39148828 -1.0958246 ]\n",
            "heart's: [0.6877869  0.1441469  0.17577918]\n",
            "cross: [-0.4456954  0.6491037 -1.214252 ]\n",
            "burn: [-0.06281596  1.1727941  -0.0149495 ]\n",
            "force: [-0.6647005   0.5733779  -0.21987467]\n",
            "blood.: [ 0.2841198 -0.7805538 -0.2548639]\n",
            "fiery: [1.5047985  0.02114049 0.57408655]\n",
            "bend: [-1.075739    1.2104594  -0.40742406]\n",
            "knave: [ 0.65083927 -1.4767473  -1.1430554 ]\n",
            "lo,: [-0.63987225 -0.3660223   0.26448983]\n",
            "justice,: [-0.43951672 -0.27592972 -1.2577509 ]\n",
            "banish: [-0.1811244   0.6823437  -0.22312787]\n",
            "fond: [0.7333688  0.31763437 1.0562543 ]\n",
            "house.: [ 0.5685891  -0.43872985 -0.804215  ]\n",
            "thine.: [-0.8941073 -0.7160646  0.07017  ]\n",
            "moon: [ 1.8005147  0.3219026 -0.6241197]\n",
            "moved: [0.61278933 0.7865359  0.24209273]\n",
            "scarce: [-0.57849514  0.8611357   0.591127  ]\n",
            "changed: [ 0.36229938 -0.26526615  0.68892914]\n",
            "think'st: [-0.41805893  0.1959923   0.25493968]\n",
            "clouds: [ 1.1977686  -0.03531097 -0.72084016]\n",
            "dreadful: [0.8156207  0.22819906 0.6349672 ]\n",
            "chamber: [ 0.7850818   0.25344265 -0.6274961 ]\n",
            "humbly: [0.06324445 0.13059059 1.11228   ]\n",
            "abroad: [-0.6369496 -0.8310168  0.5284579]\n",
            "wretched: [ 0.7525142  -0.50714517  0.7733964 ]\n",
            "woman,: [ 0.38302675 -0.5369712   0.03051467]\n",
            "comfort,: [0.37117246 0.24829794 0.4044696 ]\n",
            "gloucester,: [ 0.02774318 -0.6010139  -0.11781625]\n",
            "envious: [1.9985529  0.6266846  0.73668826]\n",
            "simple: [0.89738584 0.31846595 0.65012574]\n",
            "due: [ 0.61004764  0.76678693 -0.39835352]\n",
            "quite: [0.48530594 0.41388685 1.9422923 ]\n",
            "pain: [ 0.09452999 -0.7093559  -0.09795305]\n",
            "northumberland,: [-0.5112244  -0.905454   -0.16419205]\n",
            "grief,: [ 0.6709088  -0.03257468  0.16200241]\n",
            "bright: [0.5884264  0.15080394 0.35784543]\n",
            "ghost: [-0.07305513 -0.00948715 -1.4789735 ]\n",
            "keeps: [-0.47119388  0.37098598  0.6203968 ]\n",
            "sentence: [ 1.2803814 -0.4314711 -0.6371061]\n",
            "girl: [-0.10272688 -0.61474645 -0.90614736]\n",
            "fares: [-1.1559675  0.266191   0.4088012]\n",
            "hereford,: [ 0.23490395  0.11314536 -0.14595371]\n",
            "rise: [-1.1520625   0.43190235 -0.30326515]\n",
            "master's: [ 0.60643977 -0.19757876  0.28243232]\n",
            "rid: [-0.5049941   0.81064934 -0.42867675]\n",
            "resolve: [-0.61235785  0.9031099  -0.6276877 ]\n",
            "crown.: [ 0.94563085 -0.48330078 -0.6875354 ]\n",
            "oxford,: [-0.48697898  0.19179046  0.65182877]\n",
            "fury: [ 0.6000785  0.5271823 -0.675541 ]\n",
            "money: [ 0.06205458  0.00840384 -0.9035696 ]\n",
            "tailor: [ 0.6004467   0.63756007 -0.13483794]\n",
            "maid,: [ 0.6718595  -0.03688289 -0.07809946]\n",
            "mistress,: [-0.27537695  0.7283544  -0.47885993]\n",
            "mopsa: [-0.97674096 -1.3491822  -0.18048592]\n",
            "padua: [-0.2592801  -0.1668233   0.13360876]\n",
            "hortensio,: [-0.20625219 -0.06939978  0.21918446]\n",
            "resolved: [-0.05188239  0.4401395   0.57393634]\n",
            "good.: [-0.3952162  -1.1075418   0.34349155]\n",
            "account: [-0.00984443  0.7733813  -0.5357805 ]\n",
            "'em: [-0.52061194 -0.7630212  -0.68658113]\n",
            "labour: [-0.16693343  0.71460944 -0.7239635 ]\n",
            "whole: [ 1.6346517  -0.01870514  0.14591001]\n",
            "once,: [-0.17731938 -1.0114385  -0.1620194 ]\n",
            "benefit: [ 1.0336026   0.13248432 -0.8362483 ]\n",
            "matter,: [ 0.51229155  0.01191503 -0.4184269 ]\n",
            "several: [1.100839   0.38440675 1.0000553 ]\n",
            "aside: [-0.7234211 -0.7051166 -0.5532551]\n",
            "stone: [ 0.57534933 -0.6257806   0.9735037 ]\n",
            "arms.: [ 0.19731134 -0.4003538  -1.060979  ]\n",
            "am,: [-0.36707807  0.94456446  0.6802265 ]\n",
            "spare: [-1.3504485   1.4283103  -0.09316732]\n",
            "borne: [-0.4768432  0.7880157  0.8135362]\n",
            "army: [ 1.2510633  -0.18108189 -0.48169613]\n",
            "absence: [0.6498311  0.5345885  0.01878382]\n",
            "ladies: [ 0.8919581  -1.1689167  -0.45153496]\n",
            "husband.: [ 0.13631107 -0.36859676 -0.2914024 ]\n",
            "yonder: [ 1.1790863  -0.62978935  0.9226036 ]\n",
            "we,: [ 0.18102366 -0.47807607  0.18040384]\n",
            "beyond: [-0.4724871   0.42089278  0.34630674]\n",
            "slain,: [-0.17234492 -0.36029825  0.24796976]\n",
            "prize: [-0.24921004  1.0094149   0.23883952]\n",
            "hark,: [-0.82023406 -1.0346123  -0.11063442]\n",
            "spirit,: [ 1.3362226  -0.5067969  -0.03520744]\n",
            "party: [ 1.0986803   0.84935373 -0.07713942]\n",
            "aid: [-0.14829338 -0.03925878 -0.5334796 ]\n",
            "doom: [ 0.4647299   1.0416512  -0.59444374]\n",
            "sign: [ 0.9054693   0.6577271  -0.47103104]\n",
            "yours.: [-0.23001407 -0.43865618 -0.08105344]\n",
            "town: [ 0.80655295 -0.38451523 -0.78105664]\n",
            "malice: [0.71918535 0.23037173 0.30171528]\n",
            "enough.: [-0.446058   -0.75562835 -0.2544262 ]\n",
            "are.: [-1.0163476  -0.03278684  0.12748134]\n",
            "place.: [ 0.446803   -0.64488876 -1.1116241 ]\n",
            "tongues: [ 0.26114997 -0.5129983  -0.79260296]\n",
            "stand,: [ 0.1305497   0.12079744 -0.46009624]\n",
            "maids: [0.22718693 0.21120746 0.05072324]\n",
            "himself.: [-0.77492535 -0.56166077  0.3132003 ]\n",
            "blest: [-0.39182457  0.16540575  1.5535    ]\n",
            "words.: [ 0.11171096 -0.36521906 -0.7084355 ]\n",
            "years,: [ 0.79537797 -0.49951085 -0.43508735]\n",
            "did,: [-1.3782941   0.03852579  1.001733  ]\n",
            "private: [0.9638402  0.57532537 0.35899016]\n",
            "claim: [-0.9185759  0.8569973 -1.4474311]\n",
            "drawn: [0.70105994 0.80676603 0.8403908 ]\n",
            "double: [0.86985105 0.8291558  1.3886164 ]\n",
            "dishonour: [1.1135584 1.0780059 0.6186813]\n",
            "despite: [ 0.14256988  0.11860524 -0.46097055]\n",
            "chide: [-1.1080259   0.6824616  -0.41408992]\n",
            "despair: [ 0.12451613 -0.24145637 -0.39407903]\n",
            "wail: [-1.2172109  1.1212984 -0.4741524]\n",
            "laugh: [-0.87860143  0.4639759   0.40133166]\n",
            "awhile: [-0.2781981  -0.67693025 -0.13488542]\n",
            "still,: [ 0.03210269 -0.46349722  0.0828559 ]\n",
            "tongue.: [ 0.7349706  -0.50112003 -0.29413784]\n",
            "turns: [-0.3838236   0.8157456  -0.24869311]\n",
            "mistake: [-0.7100031   1.0175506   0.38763636]\n",
            "both.: [-0.17455754 -0.30160147  0.6510698 ]\n",
            "sits: [-0.39695236  0.49318674  0.72727716]\n",
            "prison,: [ 0.8690744  -0.18683665 -0.53971726]\n",
            "excuse: [-0.5787579   0.95296204 -0.5107433 ]\n",
            "neck: [ 1.0885618 -0.1515181 -0.3647266]\n",
            "back.: [-0.62580764 -0.805844   -0.12597781]\n",
            "captain: [ 0.5614977 -0.7886352 -0.6951898]\n",
            "sighs: [ 0.6438693   0.20515771 -0.603531  ]\n",
            "accept: [-1.5367502   1.3751544  -0.00703005]\n",
            "stars: [ 0.467301   -0.24911465 -0.5583546 ]\n",
            "advise: [-1.3718851  0.7361363  0.5213876]\n",
            "mercy,: [ 0.4442     -0.2436681   0.14264195]\n",
            "herself: [ 0.21246517 -0.20185135  0.0515292 ]\n",
            "begins: [-0.48748392  0.19097522  0.02221865]\n",
            "kindred: [ 1.1423131   0.7942652  -0.59509176]\n",
            "shore: [ 0.45290175  0.23705444 -0.5973966 ]\n",
            "duke.: [ 0.89989156 -0.35543594  0.51200783]\n",
            "lordship: [ 1.0144608   0.15632638 -0.30342722]\n",
            "evil: [0.91332    0.6228243  0.00845563]\n",
            "untimely: [0.57502913 0.52175266 0.9208072 ]\n",
            "damned: [1.0288652  0.04380269 1.2183644 ]\n",
            "effect: [-0.48353752  0.1014801  -0.7420767 ]\n",
            "creature: [ 0.80998766  0.2293513  -0.2266013 ]\n",
            "piteous: [1.4084545  0.47125354 0.96399415]\n",
            "honesty: [ 0.02430936 -0.2724782   0.29375365]\n",
            "dorset: [-0.61381185 -0.03828923 -0.49791628]\n",
            "dread: [ 0.21300311 -0.01354328 -0.22126752]\n",
            "age,: [-0.1304402   0.09162186 -0.3311165 ]\n",
            "womb: [ 0.3135969  -0.25671142 -0.9975886 ]\n",
            "vain: [0.4711225  0.41993546 0.46451584]\n",
            "christian: [1.3130853  0.16720669 0.86469936]\n",
            "arm,: [ 0.47334343  0.13320062 -0.76936424]\n",
            "rude: [ 1.1495867 -0.5072487  0.7444892]\n",
            "priest: [ 0.3115437  0.256662  -0.7407366]\n",
            "france,: [ 0.41307282 -0.27855375 -0.4773208 ]\n",
            "glory: [1.0880995 0.8288027 0.0194876]\n",
            "land,: [ 0.8203911   0.4304035  -0.43968832]\n",
            "kindness: [ 0.67782605 -0.1899598   0.05719043]\n",
            "breath,: [ 0.3641237  -0.20798813  0.40072238]\n",
            "tyrrel: [-0.4668656 -1.2570658 -0.4773501]\n",
            "hit: [-0.21140993  1.4111776   0.47092986]\n",
            "add: [-0.21186005  0.13172884 -0.29586342]\n",
            "haste,: [ 0.25222903 -0.5739741  -0.01145236]\n",
            "ross: [-0.8178378  -0.46782622 -0.19216886]\n",
            "percy: [-1.3045304  -0.34715724  0.252651  ]\n",
            "lands: [ 0.6588779   0.23117451 -0.6602563 ]\n",
            "church: [-0.25462157 -0.07352213 -0.24833904]\n",
            "bride: [ 0.71782094 -0.64859587 -0.5843835 ]\n",
            "balthasar: [-0.5109322  -1.3651667   0.01415862]\n",
            "froth: [-1.1203966 -0.8231031  0.5690493]\n",
            "people.: [ 0.58255786 -0.6472909  -0.88026214]\n",
            "vice: [ 0.6421547   0.32941425 -0.40029937]\n",
            "masters,: [-0.23111139 -0.96461385 -0.22916058]\n",
            "yourselves: [-0.59316105 -0.4099819   0.38761443]\n",
            "undone: [0.3703714 0.2744587 0.2864569]\n",
            "eat: [-0.3688276   1.0883207   0.08970055]\n",
            "bearing: [0.60930765 0.25078616 0.3358515 ]\n",
            "affection: [ 0.29743922  0.51449704 -0.16092992]\n",
            "smile: [-0.52404153  0.6513853  -0.28083783]\n",
            "to't: [-0.99345595 -0.42622104 -0.15841998]\n",
            "deserves: [-0.82021856  0.21127482  0.17115113]\n",
            "valour: [1.1965967  0.510274   0.01448664]\n",
            "commanded: [ 0.1733534  -0.00121459  0.16607544]\n",
            "guard: [ 0.03346788  0.5606563  -0.775914  ]\n",
            "safe: [ 0.44468105 -0.05124751  0.7530488 ]\n",
            "sort: [-0.34099403  0.12589353 -0.44410732]\n",
            "seeing: [-0.64519024 -0.47723335 -0.08874834]\n",
            "died: [-0.587639   0.8599392  0.7680694]\n",
            "thus,: [ 0.08965977 -0.93557876  0.43446139]\n",
            "fill: [-0.07582268  1.4269445   0.11658433]\n",
            "view: [ 0.7022926   0.43082577 -1.0842932 ]\n",
            "swords: [ 0.5367133   0.46351466 -1.0086147 ]\n",
            "followers: [ 1.2214103  -0.10890932 -0.8634741 ]\n",
            "fortune,: [ 0.33582732 -0.76400024 -0.24849282]\n",
            "purpose.: [-0.26500836 -0.5534706  -0.7264026 ]\n",
            "endure: [-0.48479086  0.89518976 -0.5070514 ]\n",
            "silence: [ 0.08960678  0.00557258 -1.8372293 ]\n",
            "used: [-0.7162764  1.0836687  0.9021509]\n",
            "pride: [ 1.2141587  -0.31571868 -0.33394632]\n",
            "occasion: [ 1.1674827  -0.2964557  -0.09735858]\n",
            "deal: [0.8399345  1.064849   0.28877768]\n",
            "blame: [-1.3928509   0.83162844 -0.6400247 ]\n",
            "motion: [ 0.40610886 -0.38608563  0.06100211]\n",
            "spend: [-0.45868355  1.4802226  -0.62057465]\n",
            "pair: [ 0.98135436  0.51492316 -0.2415972 ]\n",
            "wont: [0.39047965 0.3630728  1.1667262 ]\n",
            "received: [-0.3546683  0.8709936  0.9217238]\n",
            "nine: [ 0.4964766  -0.20625061 -0.271066  ]\n",
            "know.: [-0.790643   -0.17251149  0.17696634]\n",
            "had,: [-0.76935977  0.30931187 -0.5014989 ]\n",
            "renowned: [-0.19114868 -0.10763045 -0.27627766]\n",
            "triumph: [ 0.11046197 -0.53718704 -0.9593293 ]\n",
            "root: [ 0.86189294  0.28822675 -0.0892987 ]\n",
            "leads: [0.46960956 0.27240235 0.47465283]\n",
            "blind: [0.86405325 0.38415432 0.6238028 ]\n",
            "pass'd: [-0.43640587  1.1431817   0.58651227]\n",
            "sport: [ 0.40001538 -0.79546    -0.29960293]\n",
            "painted: [0.5248859  0.18834263 0.7219853 ]\n",
            "when,: [ 0.58067465 -0.48886535 -0.08603131]\n",
            "of,: [-0.3803364  -0.7607957   0.73690337]\n",
            "judge: [-0.84522706 -0.10611692 -0.88747114]\n",
            "air,: [ 0.7848022 -0.4744271 -0.9725124]\n",
            "dust: [ 1.48372    0.66557   -0.7378165]\n",
            "repair: [-0.6682992   1.0265435  -0.42641765]\n",
            "judgment: [ 0.60413885 -0.21041358 -0.4836938 ]\n",
            "fortunes: [ 0.8724749  -0.81023407 -0.3587096 ]\n",
            "that.: [-0.47055212 -0.5862604  -0.34875196]\n",
            "cares: [ 0.21212031  0.52272874 -0.17583346]\n",
            "cause.: [ 0.6219738  -0.40222317  0.81908876]\n",
            "hands.: [ 0.92297995 -0.17003228 -0.9763339 ]\n",
            "wild: [ 1.6154767  -0.58846587  0.83873487]\n",
            "accuse: [-0.6905951   0.9669655  -0.17355159]\n",
            "have.: [-1.7347385   0.13200314  0.02095958]\n",
            "spirits: [ 1.001697    0.4005859  -0.40549228]\n",
            "cunning: [1.0146316  0.09930041 1.0946414 ]\n",
            "red: [1.4474958  0.56213814 0.42072374]\n",
            "thrust: [-0.42414448  1.0631177  -0.41306323]\n",
            "ground,: [ 0.68092585 -0.8469432  -0.310732  ]\n",
            "favour: [0.6327241  0.2345326  0.23120818]\n",
            "feast: [0.6969749  0.04597967 0.10256014]\n",
            "thou'rt: [-0.00462646 -0.10962476  1.1614739 ]\n",
            "wrongs: [ 0.73756987 -0.12192126  0.02759739]\n",
            "twelve: [ 0.8130476  -0.21157435  0.9050323 ]\n",
            "durst: [-0.74412626  0.4298221   0.06753641]\n",
            "summer: [ 0.7477679  -0.36803967  0.41197705]\n",
            "embrace: [-0.6377283   0.98013836 -0.986822  ]\n",
            "understand: [0.11866403 1.1251954  0.4082369 ]\n",
            "haply: [-0.52204776  0.6088513  -0.03839242]\n",
            "back,: [-0.44303423 -0.6640096  -0.7026014 ]\n",
            "sake: [ 0.9007411  -0.29478878 -1.1738778 ]\n",
            "withal.: [-0.5448408  -0.56877714 -0.2529937 ]\n",
            "earth.: [ 0.7311534 -1.3317319 -1.0253966]\n",
            "tower.: [ 0.62023604 -0.3822396  -0.50472534]\n",
            "tower: [ 0.37666407  0.40650964 -0.8030509 ]\n",
            "tower,: [ 1.2807724  -0.50502217 -0.8453925 ]\n",
            "warwick's: [ 0.21181281 -0.30103576  0.747103  ]\n",
            "light,: [ 0.2667242  -0.10155238 -0.19618325]\n",
            "empty: [1.1351873  0.28721434 0.19515091]\n",
            "brothers: [ 0.83606064  0.1709255  -0.4645023 ]\n",
            "else,: [-0.46468353  0.11135109 -0.18982205]\n",
            "sun,: [ 1.105759   0.0280708 -0.5339951]\n",
            "woful: [ 0.6486592  -0.20737706  1.0409962 ]\n",
            "gross: [ 1.3186562  -0.15863551  0.89400595]\n",
            "england,: [ 0.09449484 -0.04399895 -0.442016  ]\n",
            "drown: [-1.0063925   0.94158924 -0.02579456]\n",
            "innocent: [1.0549152  0.32274485 0.29790345]\n",
            "plantagenet,: [ 0.04584639 -0.27984866 -0.4648149 ]\n",
            "side,: [ 0.43523434 -0.4916927  -0.28262156]\n",
            "continue: [-0.4500863  0.9783779 -1.1241033]\n",
            "end.: [-0.13723888 -1.0786251  -1.2836857 ]\n",
            "steal: [-0.27189776  0.7376049  -0.54096305]\n",
            "crown'd: [-0.02268992  0.69845235  0.6960331 ]\n",
            "to-morrow,: [-1.0389416  -0.96009797 -0.05999334]\n",
            "richard's: [-0.13401322 -0.38016003  0.8812789 ]\n",
            "fairly: [ 0.2937872 -1.3501017  0.8903664]\n",
            "kingly: [1.0666256  0.7670067  0.23329355]\n",
            "hereford: [-0.32896683 -0.100248    0.28519434]\n",
            "tyrant: [ 0.59309804 -0.43334395  0.1956238 ]\n",
            "kinsman: [ 1.2286516  0.430384  -0.386403 ]\n",
            "bolingbroke,: [ 0.611639   -0.20729114  0.16145033]\n",
            "sometimes: [ 0.03430164 -0.35692307 -0.6323296 ]\n",
            "hollow: [1.1359309  0.26008338 0.5382359 ]\n",
            "sour: [ 0.9109418  -0.12729093  0.85682774]\n",
            "bawd: [ 0.30707982 -0.1634277   0.28858042]\n",
            "romeo's: [ 0.09119547 -0.28669003  0.31965837]\n",
            "its: [ 0.42327487 -0.3142429  -0.59891003]\n",
            "ship: [1.0585748 0.8703042 0.1958649]\n",
            "cleomenes: [-0.12291936 -0.414854   -0.35463488]\n",
            "boatswain: [-0.14103198 -0.24650028  0.11146797]\n",
            "caliban: [-0.363491   -0.1653959   0.18331113]\n",
            "gain: [ 0.27689412  0.64863026 -0.47619095]\n",
            "cannot,: [-1.1407936  0.7468893  0.586274 ]\n",
            "slander: [-0.7979123  1.0232164  1.0389735]\n",
            "natural: [1.4155047  0.67869496 0.9426378 ]\n",
            "mind,: [ 0.63534737  0.3150803  -0.57543993]\n",
            "granted: [-0.25455052  0.36564374  0.83066684]\n",
            "choice: [ 0.2113438  1.0170275 -0.3193553]\n",
            "puts: [-0.5922431   0.3966661   0.43325737]\n",
            "shadow: [ 0.839013   0.1332042 -0.7375446]\n",
            "fault,: [ 0.22648509  0.11462317 -0.4899823 ]\n",
            "prepared: [-0.16117659  0.17472252 -0.18548927]\n",
            "pleased: [0.08820727 0.23130882 0.94179344]\n",
            "sons,: [ 0.56804705  0.34713078 -0.76950467]\n",
            "brow: [ 0.94660985 -0.26379147 -0.6954461 ]\n",
            "tread: [-0.82866055  0.47251406 -0.36014095]\n",
            "in.: [-1.4502764  -0.5239378   0.54050386]\n",
            "fellows: [ 0.49096486 -0.25315723 -0.00167247]\n",
            "hurt: [ 0.3698389  -0.08303839 -0.48747796]\n",
            "big: [0.6738907  0.24539363 1.2689059 ]\n",
            "forced: [-0.25573972  0.61897063  0.4448158 ]\n",
            "slave: [ 0.03555452  0.04769152 -0.22148605]\n",
            "his.: [-0.5075448  -0.7964387   0.09632044]\n",
            "command,: [-0.30533817  0.15022367 -0.7807582 ]\n",
            "whereof: [ 0.40101114 -0.8382152  -0.60361195]\n",
            "forth,: [-0.045409  -0.4393378  0.6066439]\n",
            "steel: [ 0.6909093  -0.7667108   0.06157371]\n",
            "nose: [ 0.4892949   1.0826529  -0.43606606]\n",
            "truly: [-0.54746586  0.20838286  0.44604945]\n",
            "proper: [1.1764601 0.5100953 0.8501738]\n",
            "power.: [ 0.346849   -0.17015752 -1.0567199 ]\n",
            "condition: [ 0.7054064  -0.18592176 -0.6405343 ]\n",
            "times,: [ 1.0299158  -0.77029514 -0.48526627]\n",
            "stain: [ 0.32816523 -0.1657218  -1.5153894 ]\n",
            "prayer: [ 0.8718935   0.26066786 -0.23656064]\n",
            "follows: [-0.48878312 -0.06734426  0.6935855 ]\n",
            "health: [ 0.28041145  0.13746107 -0.07720007]\n",
            "brings: [-0.0221127   0.7022668   0.53495395]\n",
            "off.: [-1.3861722  -1.2369677   0.65728015]\n",
            "cheeks: [ 0.8005162   0.49789566 -0.7182458 ]\n",
            "giving: [1.0486408  0.2392414  0.34449527]\n",
            "cries: [0.5352647  0.11648481 1.0936121 ]\n",
            "called: [-0.01983372  0.3195131   0.01886251]\n",
            "cause,: [ 0.33795708 -0.53257996  0.06961155]\n",
            "standing: [ 0.75629663 -0.0053671  -0.16802213]\n",
            "practise: [-0.03966002  0.23796381 -0.0778872 ]\n",
            "mock'd: [0.39498845 0.34615052 0.7165006 ]\n",
            "yourself,: [-0.91770965 -0.2426958   0.11507179]\n",
            "ignorant: [1.5422348  0.78301656 0.58090365]\n",
            "daughter's: [0.6127833  0.73305285 0.02333097]\n",
            "hardly: [-0.4349822  0.8505125  1.1607062]\n",
            "foes: [ 0.2438024   0.38408056 -0.38683414]\n",
            "outward: [2.064136   0.19345503 0.34307042]\n",
            "patient: [1.1908021  1.002571   0.88856375]\n",
            "for't: [-0.9040616  -0.35396197  0.62002075]\n",
            "enough,: [-0.7480163  -1.2218429   0.51876277]\n",
            "divine: [ 1.358455   -0.29433513  0.55409074]\n",
            "withdraw: [-1.0937058   0.30029324  0.5516145 ]\n",
            "trial: [ 0.17929636  0.22423421 -1.4842945 ]\n",
            "on't.: [-0.97317094  0.05400876 -0.09572013]\n",
            "respected: [0.22618432 0.09722226 1.6764876 ]\n",
            "undertake: [-0.2848536   1.4650726   0.52171695]\n",
            "anger: [-0.36667296  0.1202996  -0.48282963]\n",
            "frame: [-0.15791664  0.7222803  -0.62488234]\n",
            "envy: [ 0.20015582  0.9189051  -0.23148103]\n",
            "fear'd: [ 0.04216938 -0.23446132 -0.2008064 ]\n",
            "truly,: [-0.49735948 -1.3491086   0.5860511 ]\n",
            "joyful: [0.46934542 0.233641   1.0954586 ]\n",
            "ay: [-1.0655663  0.0037464  0.5482082]\n",
            "comest: [-1.0468693   0.27368912  0.6254594 ]\n",
            "drops: [ 0.8111569  -0.18772428 -0.47048488]\n",
            "looking: [ 0.62709606 -0.13693677  0.5950551 ]\n",
            "men.: [ 0.2989442  -0.35524505  0.37756222]\n",
            "children,: [ 0.39949584 -0.07473896  0.27904087]\n",
            "can,: [-0.9662402  -0.07159651  0.18777327]\n",
            "led: [0.07890037 0.2529342  0.6309093 ]\n",
            "hair: [0.91177446 0.3439228  0.16256684]\n",
            "fear.: [-0.63114256 -0.25179467 -0.2799776 ]\n",
            "urge: [-1.2337267   0.56814826  0.73541975]\n",
            "forgive: [-1.0978873  0.6372213 -0.1428469]\n",
            "request,: [ 0.22303124  0.18617645 -0.6831793 ]\n",
            "deceived: [-0.41070333 -0.56554925  1.1950072 ]\n",
            "glorious: [1.3073547  0.38996154 0.15550634]\n",
            "access: [ 0.0115432  -0.00503121 -0.9364179 ]\n",
            "affairs: [ 0.09210541 -0.22275497 -0.76185805]\n",
            "writ: [0.35517302 0.31954962 0.47521093]\n",
            "long.: [-0.2845039  -0.25478745  0.6374406 ]\n",
            "wings: [ 0.19559127 -0.09426669 -0.26895133]\n",
            "tidings: [ 0.46247688 -0.68314    -0.90440744]\n",
            "paper: [ 0.8038365  -0.12869762 -0.17121996]\n",
            "sends: [-0.09176054  0.31327328  0.41791248]\n",
            "wench: [ 1.5770303  -0.09272492 -0.14810324]\n",
            "miserable: [ 0.4484844  -0.22408038  0.2294027 ]\n",
            "story: [1.2339805  0.28020722 0.12619305]\n",
            "since,: [ 0.34683797 -0.0473924   0.62515056]\n",
            "plead: [-1.3859086  1.2315427 -0.4599637]\n",
            "pains: [-0.4940859   0.65264726 -0.9154988 ]\n",
            "princes: [ 0.4599789   0.06918164 -0.7534317 ]\n",
            "alive: [-0.8445251  -0.95663196  0.28710398]\n",
            "remembrance: [ 1.4261631   0.44490373 -0.860141  ]\n",
            "resign: [-0.7715392   1.8949244  -0.21090938]\n",
            "none.: [-0.09560287 -0.6301281   0.3529652 ]\n",
            "tedious: [ 0.6186713  -0.43833604  1.1802816 ]\n",
            "ely: [-0.39577124 -0.84334797 -0.8445872 ]\n",
            "woe,: [ 0.07277314 -0.08840674  0.13314311]\n",
            "castle: [0.94864476 0.17100087 0.37619197]\n",
            "deputy: [ 1.2082812   0.51340955 -0.9020935 ]\n",
            "leave.: [-0.3556926  -0.7408453   0.54551715]\n",
            "harry: [ 0.40617585 -0.06504974 -0.00493402]\n",
            "drunk: [ 0.41360214  0.36104012 -0.93335325]\n",
            "safety: [ 0.30783737  0.00536148 -0.46321896]\n",
            "assurance: [ 0.18397154 -0.38985544 -0.38786468]\n",
            "colours: [ 1.2418869   0.16193382 -0.49228004]\n",
            "marshal: [ 0.2155696 -1.0128927 -0.1652642]\n",
            "silver: [0.656492   0.32112786 0.23587692]\n",
            "deposed: [0.02538031 0.34293598 0.9319698 ]\n",
            "raise: [-0.25369123  1.351425   -1.3571433 ]\n",
            "satisfied: [ 0.20533256 -0.01948574  1.108489  ]\n",
            "goodness: [ 0.2995145  -0.53127337 -0.20108865]\n",
            "play'd: [-0.26958403  0.86470854  0.34912315]\n",
            "thursday: [-0.33395484 -0.9858636   0.13522476]\n",
            "you're: [-0.40218583 -0.5522139   0.66542447]\n",
            "dorcas: [-0.80484504 -1.1256324  -0.1647381 ]\n",
            "isabel,: [ 0.0740222  -0.82011354  0.35922384]\n",
            "katharina,: [-0.11561839 -0.10787378  0.28062764]\n",
            "bianca,: [-0.17484632 -0.14793733  0.05065899]\n",
            "ferdinand: [-0.3443128  -0.04342102  0.17391223]\n",
            "authority: [ 1.0527817  -0.00185586  0.71469295]\n",
            "guess: [-1.9509422   0.7676346  -0.23607947]\n",
            "partly: [-0.4675619  0.983062   1.1099885]\n",
            "unknown: [-0.75928277  0.41924593  0.48107672]\n",
            "wondrous: [ 0.4839697  -0.47291878  0.91642094]\n",
            "serves: [-0.8546636   0.8943004   0.58927524]\n",
            "body's: [1.1958846 1.1758243 1.0961308]\n",
            "body,: [ 1.4085845   0.57459086 -0.14357366]\n",
            "'fore: [-0.72321844 -0.13373153 -0.42167565]\n",
            "answer.: [-0.12723038 -0.44813308 -1.1560993 ]\n",
            "finds: [-0.12382291 -0.32474798  0.25670856]\n",
            "they'll: [-0.2242063  -0.7534952  -0.33701265]\n",
            "lion: [1.4976568  0.61694646 0.55027795]\n",
            "titus: [ 0.09511967 -0.86332273  0.2983107 ]\n",
            "business.: [-0.01336232 -1.0372453  -0.1768568 ]\n",
            "where,: [-0.27763996 -1.464193    0.21361123]\n",
            "perform: [-0.5137811  1.1129516 -0.8014399]\n",
            "made,: [-0.9348716  0.3916322  1.0229021]\n",
            "enemy,: [ 0.39728633  0.44899392 -0.421209  ]\n",
            "rome.: [-0.20204881 -0.6737832  -1.2791451 ]\n",
            "sing: [ 0.3190297   1.0987253  -0.29135463]\n",
            "express: [-0.5363609   1.6061946  -0.09520908]\n",
            "brows: [ 1.0743259 -0.391997  -0.5009643]\n",
            "husband's: [ 0.7244626  -0.00856085  0.25651497]\n",
            "boy.: [ 0.35900787 -0.25957814  0.4902825 ]\n",
            "'s: [ 0.5405983  -0.20498471  0.5034125 ]\n",
            "child.: [ 0.30300623 -0.5156119  -0.04878043]\n",
            "obey: [-1.5022683   0.52572167 -1.0436987 ]\n",
            "news.: [-0.05478558 -0.49683067 -0.12353537]\n",
            "mile: [1.3829416 0.6612471 0.1870885]\n",
            "quick: [-0.29771313 -0.39141616  1.0796806 ]\n",
            "work,: [ 0.3354849  0.3219773 -0.2876136]\n",
            "drums: [ 0.17940024 -0.31110293 -0.9289323 ]\n",
            "amongst: [-0.22429596 -0.82524997 -0.7355872 ]\n",
            "proof: [ 0.8593319   0.36317277 -0.33472934]\n",
            "bury: [-0.29295942  1.8092875  -0.35896844]\n",
            "violent: [1.6431966  0.42953035 0.51906645]\n",
            "fight.: [-0.09547864  0.23876503 -0.41317555]\n",
            "praise: [-0.5030802   0.90315235 -1.0018281 ]\n",
            "charged: [-0.00450976  0.6490817  -0.2373095 ]\n",
            "own,: [ 0.06696887 -0.0744928  -0.3459896 ]\n",
            "hither.: [-0.38001028 -0.3045936  -0.4900245 ]\n",
            "able: [0.05670547 0.51808625 1.1185626 ]\n",
            "other's: [ 1.0281967  -0.42396417  0.92898583]\n",
            "seest: [-0.3934799   1.1624105  -0.02320028]\n",
            "whip: [ 0.32646003  0.5010037  -0.9543642 ]\n",
            "camest: [-0.986485    0.64646035  0.5683534 ]\n",
            "beheld: [-0.48817554  0.9035669   0.60246646]\n",
            "grows: [-0.3165314  -0.14617884  0.9602702 ]\n",
            "patience,: [-0.11609474  0.05385813 -0.21654633]\n",
            "'twill: [-0.4455935 -0.2039041  1.088783 ]\n",
            "tells: [-0.9070079   0.16815166 -0.666565  ]\n",
            "reverend: [ 1.5304928  -0.12275998  0.3940688 ]\n",
            "prosperous: [0.78934103 0.62500155 1.1403738 ]\n",
            "another,: [ 0.3526478   0.24206848 -0.5638776 ]\n",
            "wounded: [-0.9345993 -0.118832   0.5532858]\n",
            "a': [ 0.10173159 -0.26777154  0.04952429]\n",
            "large: [1.3065575  0.26768142 0.58464116]\n",
            "fools: [ 0.7755481  -0.36555675 -0.39335772]\n",
            "lie,: [ 0.22283706 -0.5176145  -0.9387287 ]\n",
            "willingly: [-0.36960876 -0.02840349  0.9025143 ]\n",
            "met,: [-0.29356477 -0.59258     0.6942808 ]\n",
            "coward: [ 0.61490303 -0.4532101   0.13252935]\n",
            "weeds: [ 0.91934437 -0.5095577  -0.52813524]\n",
            "death's: [0.6542747  0.8368062  0.15621677]\n",
            "devise: [-0.6305507   0.26235542  0.2628589 ]\n",
            "country,: [ 0.94534165 -0.2493466  -0.32565418]\n",
            "joy,: [ 0.0918043  -0.5920065  -0.31854802]\n",
            "thrice: [ 0.18211623 -0.48401344  0.8734826 ]\n",
            "people's: [ 0.854346   -0.19206496  0.74412525]\n",
            "bodies: [ 1.5324745  0.5159388 -0.9990586]\n",
            "denied: [-0.39084467  1.1374614   0.51830214]\n",
            "enforce: [-0.87290215  1.6657928   0.51252705]\n",
            "wrought: [-0.6616276  0.4248721  0.9258896]\n",
            "if,: [-0.3328358  -0.01564744  0.6537477 ]\n",
            "would,: [-1.2130463   0.06397387  0.1601172 ]\n",
            "aedile: [-0.5648615  -0.86834973  0.2600418 ]\n",
            "way.: [ 0.447621  -0.4604033 -0.7491392]\n",
            "stoop: [0.03197466 0.91830957 0.29535466]\n",
            "thyself,: [-0.20331895 -1.0665392  -0.37308076]\n",
            "hap: [ 0.3102983  -0.00126717 -0.74089533]\n",
            "comes.: [ 0.06241808 -0.17899475 -0.30659688]\n",
            "warlike: [ 1.5506784  -0.17544869  0.50604045]\n",
            "peril: [-0.37172136  0.48665565 -0.4248944 ]\n",
            "beast: [ 1.6825643  -0.7783859   0.67617995]\n",
            "wot: [-0.9984924   0.52087986 -0.25712165]\n",
            "course,: [-0.08107244 -0.13255523 -0.69976074]\n",
            "lament: [-1.3585315   0.33427623 -0.673628  ]\n",
            "banishment: [ 0.6852024  -0.85292786 -0.6472891 ]\n",
            "slay: [-1.1620704  0.6648211 -1.1695164]\n",
            "other,: [ 1.1469436  -0.91518927  0.53841   ]\n",
            "guest: [ 1.0570279   0.01692987 -0.45221034]\n",
            "breast,: [ 0.5705339   0.49309546 -0.69495565]\n",
            "better,: [ 0.5961237 -0.5773162  0.4260889]\n",
            "thrive: [-0.8122857  -0.13306376  0.4220036 ]\n",
            "age.: [-0.11712041 -0.09087175 -1.002749  ]\n",
            "freely: [ 0.00091341 -0.51133966 -0.08904584]\n",
            "raised: [0.1350469 0.7685791 0.6300484]\n",
            "takes: [-0.7106971   0.09780806  0.06475674]\n",
            "names: [ 0.49552834  0.26945254 -0.81090736]\n",
            "yourself.: [-0.91283804 -0.7265343  -0.33454692]\n",
            "dance: [-0.15360999  1.1430238  -0.5244575 ]\n",
            "perhaps: [-0.34915456 -0.27562997  0.8686089 ]\n",
            "charity: [ 0.4869061   0.09794935 -0.10714109]\n",
            "reasons: [ 0.26161343 -0.21878645  0.18240505]\n",
            "belike: [-1.1327019  -0.48429137  0.27529362]\n",
            "dreams: [ 0.33497235  0.78199357 -0.00237185]\n",
            "brother.: [ 0.624378   -0.34935424 -0.3387567 ]\n",
            "forbear: [-1.0994664   1.0815805  -0.41508818]\n",
            "grievous: [1.7092096  0.49196094 1.0825757 ]\n",
            "slaughter'd: [-0.37121725 -0.10161632 -0.26564887]\n",
            "stabb'd: [-0.04508209  0.3014254   0.36408186]\n",
            "fairer: [ 1.3394061  -0.1456301   0.88600683]\n",
            "mind.: [ 0.56564265 -0.46356687 -1.1866164 ]\n",
            "revenged: [-1.03678     0.45068607  0.9413384 ]\n",
            "weep,: [-0.884634    0.5513165  -0.08818377]\n",
            "happiness: [ 1.3286902  -0.51959115 -0.25603104]\n",
            "humour: [ 1.1372775   0.14595896 -0.05662569]\n",
            "been,: [-0.7713106   0.08140361  0.35823768]\n",
            "usurp: [-0.57989514  0.9559539  -0.56576854]\n",
            "lovely: [0.8767037  0.03301671 0.6485893 ]\n",
            "loss,: [ 1.4322069  -0.27455503  0.28146186]\n",
            "hateful: [0.86443496 0.41883877 0.6839836 ]\n",
            "wither'd: [ 0.67180943 -0.22681242  0.27929953]\n",
            "world's: [1.3407694  0.33874252 0.50693864]\n",
            "taught: [0.2788264  0.31834996 0.4574141 ]\n",
            "fall,: [-0.33105096  1.0338029  -1.1101273 ]\n",
            "hopes: [ 0.32146502  0.21261963 -0.6276787 ]\n",
            "flies: [-0.23177293  0.53445464  0.6507985 ]\n",
            "depart: [-0.7356633   0.7096009   0.25424322]\n",
            "perfect: [ 0.817042   -0.2672343   0.28338155]\n",
            "spent: [-0.45367667  1.0636861   0.4289548 ]\n",
            "liege: [ 0.52297574 -0.8192396  -0.41071495]\n",
            "committed: [-0.6168743  -0.2504199   0.24412659]\n",
            "bade: [-0.09304076  1.2476051  -0.18231523]\n",
            "brief,: [ 0.08089408 -0.01074153  0.27495506]\n",
            "rest.: [-0.19535606 -0.11782429  0.24799104]\n",
            "bethink: [-1.2323539  1.1751881 -0.5642143]\n",
            "wound: [-0.18553972  0.51389366 -1.1588328 ]\n",
            "morrow,: [-0.22831707 -1.1972657   0.13576573]\n",
            "sirrah: [-0.00087971 -0.57851446  0.23932722]\n",
            "margaret,: [-0.40122077 -0.6432145   0.14473028]\n",
            "form: [ 1.2090391   0.27561313 -0.9386372 ]\n",
            "liberty: [ 1.0237238  -0.16597153 -0.13481727]\n",
            "sorry: [-1.1755577   0.5045588   0.28078428]\n",
            "birth,: [ 0.5052575   0.01769999 -0.8073133 ]\n",
            "majesty,: [ 1.2553326   0.49970412 -1.0977609 ]\n",
            "widow,: [ 0.33343396 -1.3659467  -0.5679479 ]\n",
            "pure: [ 0.6450404   0.14975357 -0.13303967]\n",
            "match: [-0.52758396  0.49941722 -0.33563063]\n",
            "burthen: [0.7392327  0.7639973  0.10925429]\n",
            "youth,: [ 1.022018   -0.30778617 -0.8966361 ]\n",
            "highness': [0.980924   0.8794792  0.28368604]\n",
            "to-morrow.: [-0.9515326  -1.238719    0.53652257]\n",
            "needful: [ 1.0852326 -0.6264588  0.8669887]\n",
            "fly.: [-0.9218821  -0.6557597  -0.13137567]\n",
            "say'st: [-0.7318335  -0.10667208  0.55278456]\n",
            "lusty: [ 0.8377935   0.5579667  -0.17110015]\n",
            "dares: [-0.6043693   0.1552266   0.27244914]\n",
            "condemn'd: [-0.1708168   0.86421955 -0.15653083]\n",
            "special: [ 1.1614138  -0.429135    0.44534793]\n",
            "crowns: [ 0.9571544 -0.7117439 -1.1444571]\n",
            "credit: [-0.29699978  0.3590301  -0.18541797]\n",
            "palace: [ 1.272162   -0.60479957 -1.2941517 ]\n",
            "happily: [-0.42478743  0.43993348 -0.28605905]\n",
            "seeming: [-0.05406073  0.02941963 -0.7400057 ]\n",
            "passion: [ 0.50645685  0.22657482 -0.22378387]\n",
            "houses: [-0.2695141   0.15087232 -0.40728554]\n",
            "clear: [ 0.6033558   0.77615446 -0.16678688]\n",
            "bona: [-0.5610078  -0.06413074  0.37116596]\n",
            "instruct: [-0.34910107  1.3219422  -0.3795143 ]\n",
            "curst: [-0.57552457 -0.3197761   0.02101127]\n",
            "angelo.: [-0.20022026 -0.7671829  -0.25484955]\n",
            "claudio,: [-0.6718082  -0.5666207   0.38229668]\n",
            "provost,: [-0.2393659  -0.14077705  0.18578778]\n",
            "lucentio.: [-0.19522963 -0.24340647  0.16665463]\n",
            "alonso: [-0.08509213 -0.10547677  0.08697689]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The plot everyone wants to play with"
      ],
      "metadata": {
        "id": "UsIgO_DL_Kvh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b1J2dPJ3_fPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "aa447b2d",
        "outputId": "8a792a36-753c-41ea-bd65-47a2c860ce26"
      },
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Assuming you have embedding_3d and id_to_word from previous steps\n",
        "# You can create a DataFrame for easier plotting with Plotly\n",
        "import pandas as pd\n",
        "\n",
        "# Create a list of words corresponding to the indices in embedding_3d\n",
        "words = [id_to_word.get(i, '<UNK>') for i in range(len(embedding_3d))]\n",
        "\n",
        "# Create a DataFrame with the 3D coordinates and the words\n",
        "embeddings_df = pd.DataFrame({\n",
        "    'x': embedding_3d[:, 0],\n",
        "    'y': embedding_3d[:, 1],\n",
        "    'z': embedding_3d[:, 2],\n",
        "    'word': words\n",
        "})\n",
        "\n",
        "# Create the interactive 3D scatter plot\n",
        "fig = px.scatter_3d(embeddings_df, x='x', y='y', z='z', text='word',\n",
        "                    title='Word Embeddings (3D PCA)')\n",
        "\n",
        "# Adjust the text position to avoid overlap (optional)\n",
        "fig.update_traces(textposition='top center')\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"e2d1999f-1f03-4ac0-a2d9-c97fb4ec78f8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e2d1999f-1f03-4ac0-a2d9-c97fb4ec78f8\")) {                    Plotly.newPlot(                        \"e2d1999f-1f03-4ac0-a2d9-c97fb4ec78f8\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003ez=%{z}\\u003cbr\\u003eword=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"text\":[\"\\u003cPAD\\u003e\",\"\\u003cUNK\\u003e\",\"the\",\"and\",\"to\",\"i\",\"of\",\"my\",\"a\",\"you\",\"that\",\"in\",\"is\",\"for\",\"not\",\"with\",\"your\",\"be\",\"his\",\"it\",\"he\",\"this\",\"have\",\"as\",\"but\",\"me\",\"thou\",\"thy\",\"so\",\"what\",\"will\",\"by\",\"him\",\"we\",\"shall\",\"all\",\"if\",\"our\",\"king\",\"are\",\"no\",\"her\",\"do\",\"from\",\"good\",\"on\",\"or\",\"at\",\"which\",\"would\",\"they\",\"thee\",\"how\",\"was\",\"more\",\"than\",\"their\",\"she\",\"now\",\"hath\",\"am\",\"let\",\"then\",\"duke\",\"i'll\",\"when\",\"here\",\"were\",\"lord\",\"make\",\"one\",\"may\",\"upon\",\"them\",\"you,\",\"like\",\"come\",\"an\",\"must\",\"should\",\"richard\",\"had\",\"yet\",\"sir,\",\"'tis\",\"did\",\"first\",\"say\",\"where\",\"there\",\"some\",\"go\",\"us\",\"queen\",\"know\",\"well\",\"love\",\"give\",\"these\",\"take\",\"me,\",\"such\",\"o\",\"see\",\"who\",\"o,\",\"can\",\"edward\",\"tell\",\"henry\",\"man\",\"too\",\"most\",\"york\",\"gloucester\",\"romeo\",\"nor\",\"out\",\"vincentio\",\"come,\",\"mine\",\"lord,\",\"up\",\"speak\",\"why,\",\"lady\",\"now,\",\"made\",\"time\",\"never\",\"hear\",\"and,\",\"art\",\"doth\",\"sir\",\"much\",\"any\",\"being\",\"think\",\"cannot\",\"thee,\",\"very\",\"noble\",\"before\",\"petruchio\",\"him,\",\"been\",\"god\",\"ay,\",\"menenius\",\"death\",\"second\",\"coriolanus\",\"warwick\",\"thus\",\"father\",\"against\",\"fair\",\"both\",\"no,\",\"sweet\",\"though\",\"him.\",\"great\",\"poor\",\"heart\",\"well,\",\"hast\",\"you.\",\"own\",\"it.\",\"son\",\"juliet\",\"look\",\"till\",\"call\",\"name\",\"why\",\"nay,\",\"what,\",\"iii\",\"whose\",\"true\",\"many\",\"men\",\"even\",\"leave\",\"old\",\"life\",\"isabella\",\"brother\",\"leontes\",\"it,\",\"not,\",\"pray\",\"unto\",\"so,\",\"those\",\"me.\",\"blood\",\"away\",\"ever\",\"bear\",\"comes\",\"stand\",\"angelo\",\"nothing\",\"then,\",\"day\",\"sicinius\",\"could\",\"myself\",\"other\",\"lucio\",\"way\",\"put\",\"fear\",\"two\",\"prince\",\"bolingbroke\",\"clarence\",\"hand\",\"nurse\",\"o'\",\"ere\",\"iv\",\"better\",\"therefore\",\"since\",\"set\",\"done\",\"little\",\"say,\",\"whom\",\"grace\",\"elizabeth\",\"buckingham\",\"capulet\",\"eyes\",\"friar\",\"heaven\",\"down\",\"might\",\"but,\",\"still\",\"honour\",\"margaret\",\"tranio\",\"i,\",\"vi\",\"citizen\",\"stay\",\"keep\",\"bring\",\"ii\",\"young\",\"every\",\"bid\",\"gentleman\",\"once\",\"else\",\"katharina\",\"dear\",\"off\",\"live\",\"gentle\",\"into\",\"marcius\",\"find\",\"thing\",\"world\",\"brutus\",\"again\",\"long\",\"best\",\"i'\",\"go,\",\"news\",\"that's\",\"thine\",\"dost\",\"camillo\",\"came\",\"word\",\"dead\",\"master\",\"that,\",\"let's\",\"head\",\"king,\",\"makes\",\"full\",\"die\",\"what's\",\"hence\",\"has\",\"baptista\",\"please\",\"about\",\"death,\",\"show\",\"house\",\"night\",\"he's\",\"heard\",\"none\",\"escalus\",\"lucentio\",\"words\",\"soul\",\"thou,\",\"father,\",\"thought\",\"after\",\"lord.\",\"here,\",\"love,\",\"wilt\",\"duchess\",\"himself\",\"wife\",\"daughter\",\"provost\",\"hortensio\",\"mother\",\"forth\",\"power\",\"gracious\",\"servant\",\"royal\",\"cominius\",\"part\",\"grumio\",\"while\",\"rest\",\"tongue\",\"friends\",\"far\",\"third\",\"hastings\",\"thousand\",\"peace\",\"hold\",\"we'll\",\"only\",\"shalt\",\"pardon\",\"hope\",\"pompey\",\"lay\",\"beseech\",\"gremio\",\"rather\",\"within\",\"cause\",\"son,\",\"murderer\",\"autolycus\",\"help\",\"face\",\"holy\",\"life,\",\"benvolio\",\"mercutio\",\"back\",\"use\",\"said\",\"thank\",\"father's\",\"prove\",\"sir.\",\"happy\",\"paulina\",\"gone\",\"meet\",\"right\",\"is,\",\"answer\",\"mind\",\"tears\",\"polixenes\",\"clown\",\"prospero\",\"aufidius\",\"madam,\",\"without\",\"thee.\",\"hands\",\"there's\",\"save\",\"mean\",\"get\",\"lords,\",\"northumberland\",\"this,\",\"send\",\"another\",\"lies\",\"mistress\",\"her,\",\"claudio\",\"man,\",\"last\",\"lie\",\"so.\",\"clifford\",\"ne'er\",\"things\",\"three\",\"volumnia\",\"believe\",\"crown\",\"less\",\"saw\",\"brother,\",\"foul\",\"laurence\",\"them,\",\"joy\",\"shame\",\"servingman\",\"sorrow\",\"us,\",\"boy\",\"ah,\",\"worthy\",\"kind\",\"not.\",\"bloody\",\"state\",\"left\",\"because\",\"all,\",\"follow\",\"does\",\"turn\",\"home\",\"husband\",\"anne\",\"under\",\"therefore,\",\"earth\",\"false\",\"through\",\"high\",\"told\",\"present\",\"place\",\"people\",\"fall\",\"fortune\",\"here's\",\"lives\",\"talk\",\"comfort\",\"marry,\",\"grief\",\"he,\",\"messenger\",\"means\",\"shepherd\",\"years\",\"cousin\",\"swear\",\"gods\",\"end\",\"need\",\"hither\",\"on,\",\"lords\",\"death.\",\"'twas\",\"welcome\",\"sun\",\"bianca\",\"either\",\"grave\",\"body\",\"well.\",\"hate\",\"break\",\"see,\",\"pity\",\"god,\",\"edward's\",\"signior\",\"matter\",\"rome\",\"sit\",\"wish\",\"more,\",\"hour\",\"each\",\"john\",\"florizel\",\"brought\",\"looks\",\"again.\",\"eye\",\"heavy\",\"canst\",\"warwick,\",\"gaunt\",\"proud\",\"up,\",\"sent\",\"new\",\"wife,\",\"didst\",\"aumerle\",\"biondello\",\"friend\",\"senator\",\"play\",\"light\",\"having\",\"will,\",\"hand,\",\"montague\",\"kill\",\"for,\",\"cry\",\"seem\",\"truth\",\"lest\",\"god's\",\"sebastian\",\"enough\",\"us.\",\"common\",\"fellow\",\"you'll\",\"war\",\"fight\",\"know,\",\"'twere\",\"them.\",\"be,\",\"her.\",\"city\",\"friends,\",\"care\",\"blood,\",\"mark\",\"seen\",\"reason\",\"nature\",\"law\",\"she's\",\"to-morrow\",\"lost\",\"grey\",\"catesby\",\"paris\",\"hermione\",\"business\",\"do,\",\"yet,\",\"knows\",\"mother,\",\"free\",\"mercy\",\"there,\",\"all.\",\"something\",\"cut\",\"twenty\",\"kiss\",\"child\",\"king.\",\"tybalt\",\"speak,\",\"yield\",\"rivers\",\"says\",\"strange\",\"days\",\"indeed,\",\"children\",\"warrant\",\"horse\",\"heaven,\",\"done,\",\"dare\",\"which,\",\"faith,\",\"peace,\",\"unless\",\"queen,\",\"tender\",\"antonio\",\"speak.\",\"arms\",\"indeed\",\"justice\",\"pray,\",\"yourself\",\"brave\",\"deep\",\"desire\",\"remember\",\"soon\",\"cold\",\"farewell\",\"married\",\"gonzalo\",\"first,\",\"content\",\"report\",\"come.\",\"strike\",\"fire\",\"ye\",\"bed\",\"found\",\"lose\",\"beat\",\"stands\",\"sound\",\"o'er\",\"times\",\"mine,\",\"breath\",\"again,\",\"women\",\"lewis\",\"become\",\"ten\",\"heart,\",\"ready\",\"trust\",\"heavens\",\"return\",\"sleep\",\"wear\",\"too,\",\"honour,\",\"fearful\",\"near\",\"sea\",\"thyself\",\"knew\",\"oath\",\"land\",\"is't\",\"change\",\"ears\",\"yes,\",\"doubt\",\"love.\",\"seek\",\"charge\",\"hours\",\"late\",\"draw\",\"time,\",\"yea,\",\"brother's\",\"gave\",\"loving\",\"straight\",\"name,\",\"maid\",\"hence,\",\"bound\",\"given\",\"serve\",\"world,\",\"woman\",\"read\",\"past\",\"day,\",\"farewell,\",\"villain\",\"majesty\",\"romeo,\",\"kate\",\"miranda\",\"honest\",\"more.\",\"general\",\"half\",\"known\",\"pluck\",\"gone,\",\"open\",\"took\",\"ill\",\"welcome,\",\"live,\",\"traitor\",\"king's\",\"beauty\",\"good,\",\"cousin,\",\"woe\",\"richmond\",\"sin\",\"between\",\"born\",\"fetch\",\"neither\",\"citizens\",\"person\",\"loss\",\"heir\",\"prince,\",\"night,\",\"elbow\",\"loved\",\"true,\",\"sworn\",\"over\",\"prithee,\",\"hearts\",\"men,\",\"whilst\",\"away.\",\"sword\",\"same\",\"fly\",\"itself\",\"eyes,\",\"foot\",\"sight\",\"widow\",\"saint\",\"marry\",\"edward,\",\"thomas\",\"crown,\",\"fit\",\"who,\",\"together\",\"look,\",\"rome,\",\"hadst\",\"often\",\"fault\",\"friend,\",\"sad\",\"duty\",\"earth,\",\"dead,\",\"liege,\",\"post\",\"away,\",\"head,\",\"win\",\"attend\",\"madam\",\"look'd\",\"run\",\"pale\",\"here.\",\"princely\",\"ask\",\"worth\",\"grant\",\"die.\",\"weep\",\"myself,\",\"face,\",\"down,\",\"slain\",\"next\",\"uncle\",\"mad\",\"perdita\",\"revenge\",\"confess\",\"arm\",\"think,\",\"have,\",\"virtue\",\"rich\",\"throw\",\"worse\",\"way,\",\"methinks\",\"spoke\",\"ho\",\"haste\",\"gentlemen,\",\"coming\",\"purpose\",\"ground\",\"service\",\"spirit\",\"entreat\",\"said,\",\"house,\",\"banish'd\",\"thoughts\",\"alas,\",\"one,\",\"counsel\",\"clarence,\",\"mighty\",\"soul,\",\"title\",\"sly\",\"peter\",\"strong\",\"curse\",\"pretty\",\"almost\",\"five\",\"home,\",\"goes\",\"whether\",\"wherein\",\"fie,\",\"buy\",\"dangerous\",\"bad\",\"wert\",\"e'er\",\"heart.\",\"pleasure\",\"suit\",\"call'd\",\"voices\",\"behold\",\"gone.\",\"life.\",\"speed\",\"wrong\",\"sister\",\"earl\",\"master,\",\"3\",\"mariana\",\"enemy\",\"soft\",\"yours\",\"farewell.\",\"daughter,\",\"whence\",\"man.\",\"got\",\"prayers\",\"mine.\",\"or,\",\"gates\",\"merry\",\"air\",\"wounds\",\"ha\",\"ancient\",\"ear\",\"age\",\"die,\",\"towards\",\"strength\",\"sirrah,\",\"court\",\"just\",\"richard,\",\"france\",\"uncle,\",\"stanley\",\"marriage\",\"kate,\",\"tale\",\"point\",\"man's\",\"bold\",\"themselves\",\"virgilia\",\"want\",\"lartius\",\"hark\",\"issue\",\"further\",\"souls\",\"wast\",\"above\",\"wouldst\",\"learn\",\"loves\",\"fast\",\"sovereign\",\"forget\",\"deserved\",\"deed\",\"stay,\",\"now.\",\"presence\",\"quarrel\",\"nurse,\",\"boy,\",\"black\",\"dream\",\"green\",\"york,\",\"roman\",\"state,\",\"patience\",\"battle\",\"valiant\",\"needs\",\"youth\",\"night.\",\"march\",\"deny\",\"pay\",\"begin\",\"people,\",\"go.\",\"officer\",\"measure\",\"pass\",\"goodly\",\"lawful\",\"say.\",\"witness\",\"knock\",\"kill'd\",\"george\",\"henry's\",\"golden\",\"england's\",\"keeper\",\"oxford\",\"seems\",\"norfolk\",\"father.\",\"word,\",\"too.\",\"eye,\",\"wise\",\"hang\",\"greater\",\"fool\",\"he'll\",\"met\",\"feel\",\"wind\",\"i.\",\"down.\",\"consent\",\"toward\",\"touch\",\"office\",\"sure\",\"wherefore\",\"slew\",\"out,\",\"move\",\"to-day\",\"until\",\"murder\",\"friar,\",\"rest,\",\"small\",\"defend\",\"marcius,\",\"wonder\",\"held\",\"whither\",\"field\",\"hearing\",\"fear,\",\"done.\",\"work\",\"base\",\"gold\",\"letters\",\"lady,\",\"out.\",\"lips\",\"mortal\",\"wit\",\"child,\",\"watch\",\"himself,\",\"guilty\",\"order\",\"day.\",\"mayor\",\"music\",\"pedant\",\"ariel\",\"country\",\"side\",\"course\",\"appear\",\"wars\",\"tribunes\",\"where's\",\"glad\",\"is.\",\"chance\",\"visit\",\"myself.\",\"tear\",\"along\",\"hell\",\"this.\",\"breathe\",\"will.\",\"shed\",\"country's\",\"hard\",\"grow\",\"morning\",\"may,\",\"'twixt\",\"heads\",\"virtuous\",\"water\",\"commend\",\"rage\",\"promise\",\"oft\",\"bed,\",\"command\",\"soldiers\",\"living\",\"case\",\"highness\",\"lancaster\",\"villain,\",\"henry,\",\"hastings,\",\"to-night\",\"sovereign,\",\"ratcliff\",\"subject\",\"norfolk,\",\"mowbray\",\"prison\",\"watchman\",\"somerset\",\"clifford,\",\"xi\",\"antigonus\",\"alack,\",\"speaks\",\"quoth\",\"lead\",\"sword,\",\"already\",\"alone\",\"both,\",\"jest\",\"yours,\",\"hundred\",\"off,\",\"shows\",\"deeds\",\"hide\",\"'gainst\",\"in,\",\"hot\",\"calls\",\"twice\",\"piece\",\"kept\",\"place,\",\"on.\",\"laid\",\"there.\",\"blessed\",\"prepare\",\"news,\",\"forward\",\"bosom\",\"close\",\"hither,\",\"sampson\",\"curtis\",\"thither\",\"hear,\",\"note\",\"worst\",\"making\",\"broke\",\"four\",\"thence\",\"becomes\",\"fare\",\"soldier\",\"certain\",\"thanks\",\"quickly\",\"ta'en\",\"wash\",\"angry\",\"deadly\",\"true.\",\"gives\",\"faith\",\"white\",\"stood\",\"sudden\",\"precious\",\"sure,\",\"poison\",\"choose\",\"enter\",\"withal\",\"shake\",\"forbid\",\"rough\",\"teach\",\"going\",\"hand.\",\"book\",\"fresh\",\"she,\",\"tears,\",\"hope,\",\"men's\",\"kingdom\",\"alas\",\"dead.\",\"london\",\"early\",\"tybalt,\",\"camillo,\",\"nature,\",\"suffer\",\"act\",\"deliver\",\"offence\",\"behind\",\"faults\",\"husband,\",\"won\",\"bless\",\"fine\",\"shall,\",\"no.\",\"noise\",\"enemies\",\"carry\",\"gentleman,\",\"whiles\",\"together,\",\"are,\",\"beg\",\"request\",\"war,\",\"blows\",\"dry\",\"fled\",\"struck\",\"voice\",\"honour.\",\"ourselves\",\"six\",\"humble\",\"speech\",\"bears\",\"stir\",\"time.\",\"world.\",\"son.\",\"forgot\",\"shame,\",\"bids\",\"be.\",\"hands,\",\"part,\",\"was,\",\"weak\",\"queen's\",\"sister,\",\"secret\",\"gentlemen\",\"cheer\",\"bitter\",\"kings\",\"right,\",\"stay.\",\"grace,\",\"york.\",\"england\",\"weeping\",\"english\",\"montague,\",\"juliet,\",\"angelo,\",\"barnardine\",\"tranio,\",\"always\",\"'t\",\"idle\",\"fought\",\"company\",\"grown\",\"power,\",\"cruel\",\"truth,\",\"lend\",\"blow\",\"foe\",\"page\",\"foolish\",\"tent\",\"sometime\",\"weary\",\"least\",\"honourable\",\"dark\",\"cast\",\"among\",\"vengeance\",\"harm\",\"flesh\",\"wife.\",\"trouble\",\"crave\",\"do.\",\"words,\",\"none,\",\"hold,\",\"try\",\"presently\",\"stop\",\"longer\",\"fellow,\",\"bite\",\"mean,\",\"betwixt\",\"leisure\",\"rutland\",\"derby\",\"queen.\",\"buckingham,\",\"live.\",\"sacred\",\"thanks,\",\"musician\",\"remain\",\"seat\",\"hate,\",\"who's\",\"lack\",\"volsces\",\"i'ld\",\"besides,\",\"letter\",\"therein\",\"valeria\",\"short\",\"mock\",\"prisoner\",\"doing\",\"drink\",\"subjects\",\"seven\",\"lived\",\"sea,\",\"sense\",\"sake,\",\"conscience\",\"wisdom\",\"were,\",\"help,\",\"fie\",\"leave,\",\"although\",\"mayst\",\"thinks\",\"love's\",\"fool,\",\"vow\",\"kneel\",\"mother's\",\"brakenbury\",\"cursed\",\"devil\",\"wrong,\",\"grace.\",\"walk\",\"tut,\",\"sons\",\"pardon,\",\"bishop\",\"promised\",\"offer\",\"year\",\"gregory\",\"worship\",\"exeter\",\"bohemia\",\"overdone\",\"petruchio,\",\"proceed\",\"caius\",\"arms,\",\"former\",\"word.\",\"sick\",\"vile\",\"aufidius,\",\"face.\",\"honours\",\"dispatch\",\"danger\",\"business,\",\"fell\",\"knee\",\"brief\",\"art,\",\"drop\",\"plague\",\"to,\",\"slave,\",\"dull\",\"perceive\",\"cried\",\"encounter\",\"deserve\",\"action\",\"leaves\",\"manner\",\"last,\",\"by,\",\"taken\",\"thing,\",\"seal\",\"anon\",\"warm\",\"show'd\",\"doubt,\",\"mouth\",\"taste\",\"traitors\",\"law,\",\"bones\",\"seize\",\"scorn\",\"do't\",\"tongue,\",\"peace.\",\"lady's\",\"one.\",\"know'st\",\"quit\",\"much.\",\"woman's\",\"throne\",\"flowers\",\"few\",\"quiet\",\"daughter.\",\"fatal\",\"wicked\",\"thereof\",\"broken\",\"fain\",\"woes\",\"desperate\",\"head.\",\"went\",\"duke,\",\"swift\",\"woo\",\"proclaim\",\"bushy\",\"county\",\"mamillius\",\"abhorson\",\"dog\",\"intend\",\"knees\",\"receive\",\"flatter\",\"fire,\",\"passing\",\"aught\",\"hie\",\"corioli\",\"troth,\",\"alone,\",\"then.\",\"thoughts,\",\"up.\",\"friends.\",\"hour,\",\"own.\",\"shouldst\",\"before.\",\"fair,\",\"write\",\"in't\",\"rotten\",\"alone.\",\"victory\",\"right.\",\"wanton\",\"easy\",\"consul\",\"knowledge\",\"much,\",\"breast\",\"owe\",\"company.\",\"spake\",\"touch'd\",\"repent\",\"home.\",\"plain\",\"sleep,\",\"frown\",\"traitor,\",\"treason\",\"cure\",\"heart's\",\"cross\",\"burn\",\"force\",\"blood.\",\"fiery\",\"bend\",\"knave\",\"lo,\",\"justice,\",\"banish\",\"fond\",\"house.\",\"thine.\",\"moon\",\"moved\",\"scarce\",\"changed\",\"think'st\",\"clouds\",\"dreadful\",\"chamber\",\"humbly\",\"abroad\",\"wretched\",\"woman,\",\"comfort,\",\"gloucester,\",\"envious\",\"simple\",\"due\",\"quite\",\"pain\",\"northumberland,\",\"grief,\",\"bright\",\"ghost\",\"keeps\",\"sentence\",\"girl\",\"fares\",\"hereford,\",\"rise\",\"master's\",\"rid\",\"resolve\",\"crown.\",\"oxford,\",\"fury\",\"money\",\"tailor\",\"maid,\",\"mistress,\",\"mopsa\",\"padua\",\"hortensio,\",\"resolved\",\"good.\",\"account\",\"'em\",\"labour\",\"whole\",\"once,\",\"benefit\",\"matter,\",\"several\",\"aside\",\"stone\",\"arms.\",\"am,\",\"spare\",\"borne\",\"army\",\"absence\",\"ladies\",\"husband.\",\"yonder\",\"we,\",\"beyond\",\"slain,\",\"prize\",\"hark,\",\"spirit,\",\"party\",\"aid\",\"doom\",\"sign\",\"yours.\",\"town\",\"malice\",\"enough.\",\"are.\",\"place.\",\"tongues\",\"stand,\",\"maids\",\"himself.\",\"blest\",\"words.\",\"years,\",\"did,\",\"private\",\"claim\",\"drawn\",\"double\",\"dishonour\",\"despite\",\"chide\",\"despair\",\"wail\",\"laugh\",\"awhile\",\"still,\",\"tongue.\",\"turns\",\"mistake\",\"both.\",\"sits\",\"prison,\",\"excuse\",\"neck\",\"back.\",\"captain\",\"sighs\",\"accept\",\"stars\",\"advise\",\"mercy,\",\"herself\",\"begins\",\"kindred\",\"shore\",\"duke.\",\"lordship\",\"evil\",\"untimely\",\"damned\",\"effect\",\"creature\",\"piteous\",\"honesty\",\"dorset\",\"dread\",\"age,\",\"womb\",\"vain\",\"christian\",\"arm,\",\"rude\",\"priest\",\"france,\",\"glory\",\"land,\",\"kindness\",\"breath,\",\"tyrrel\",\"hit\",\"add\",\"haste,\",\"ross\",\"percy\",\"lands\",\"church\",\"bride\",\"balthasar\",\"froth\",\"people.\",\"vice\",\"masters,\",\"yourselves\",\"undone\",\"eat\",\"bearing\",\"affection\",\"smile\",\"to't\",\"deserves\",\"valour\",\"commanded\",\"guard\",\"safe\",\"sort\",\"seeing\",\"died\",\"thus,\",\"fill\",\"view\",\"swords\",\"followers\",\"fortune,\",\"purpose.\",\"endure\",\"silence\",\"used\",\"pride\",\"occasion\",\"deal\",\"blame\",\"motion\",\"spend\",\"pair\",\"wont\",\"received\",\"nine\",\"know.\",\"had,\",\"renowned\",\"triumph\",\"root\",\"leads\",\"blind\",\"pass'd\",\"sport\",\"painted\",\"when,\",\"of,\",\"judge\",\"air,\",\"dust\",\"repair\",\"judgment\",\"fortunes\",\"that.\",\"cares\",\"cause.\",\"hands.\",\"wild\",\"accuse\",\"have.\",\"spirits\",\"cunning\",\"red\",\"thrust\",\"ground,\",\"favour\",\"feast\",\"thou'rt\",\"wrongs\",\"twelve\",\"durst\",\"summer\",\"embrace\",\"understand\",\"haply\",\"back,\",\"sake\",\"withal.\",\"earth.\",\"tower.\",\"tower\",\"tower,\",\"warwick's\",\"light,\",\"empty\",\"brothers\",\"else,\",\"sun,\",\"woful\",\"gross\",\"england,\",\"drown\",\"innocent\",\"plantagenet,\",\"side,\",\"continue\",\"end.\",\"steal\",\"crown'd\",\"to-morrow,\",\"richard's\",\"fairly\",\"kingly\",\"hereford\",\"tyrant\",\"kinsman\",\"bolingbroke,\",\"sometimes\",\"hollow\",\"sour\",\"bawd\",\"romeo's\",\"its\",\"ship\",\"cleomenes\",\"boatswain\",\"caliban\",\"gain\",\"cannot,\",\"slander\",\"natural\",\"mind,\",\"granted\",\"choice\",\"puts\",\"shadow\",\"fault,\",\"prepared\",\"pleased\",\"sons,\",\"brow\",\"tread\",\"in.\",\"fellows\",\"hurt\",\"big\",\"forced\",\"slave\",\"his.\",\"command,\",\"whereof\",\"forth,\",\"steel\",\"nose\",\"truly\",\"proper\",\"power.\",\"condition\",\"times,\",\"stain\",\"prayer\",\"follows\",\"health\",\"brings\",\"off.\",\"cheeks\",\"giving\",\"cries\",\"called\",\"cause,\",\"standing\",\"practise\",\"mock'd\",\"yourself,\",\"ignorant\",\"daughter's\",\"hardly\",\"foes\",\"outward\",\"patient\",\"for't\",\"enough,\",\"divine\",\"withdraw\",\"trial\",\"on't.\",\"respected\",\"undertake\",\"anger\",\"frame\",\"envy\",\"fear'd\",\"truly,\",\"joyful\",\"ay\",\"comest\",\"drops\",\"looking\",\"men.\",\"children,\",\"can,\",\"led\",\"hair\",\"fear.\",\"urge\",\"forgive\",\"request,\",\"deceived\",\"glorious\",\"access\",\"affairs\",\"writ\",\"long.\",\"wings\",\"tidings\",\"paper\",\"sends\",\"wench\",\"miserable\",\"story\",\"since,\",\"plead\",\"pains\",\"princes\",\"alive\",\"remembrance\",\"resign\",\"none.\",\"tedious\",\"ely\",\"woe,\",\"castle\",\"deputy\",\"leave.\",\"harry\",\"drunk\",\"safety\",\"assurance\",\"colours\",\"marshal\",\"silver\",\"deposed\",\"raise\",\"satisfied\",\"goodness\",\"play'd\",\"thursday\",\"you're\",\"dorcas\",\"isabel,\",\"katharina,\",\"bianca,\",\"ferdinand\",\"authority\",\"guess\",\"partly\",\"unknown\",\"wondrous\",\"serves\",\"body's\",\"body,\",\"'fore\",\"answer.\",\"finds\",\"they'll\",\"lion\",\"titus\",\"business.\",\"where,\",\"perform\",\"made,\",\"enemy,\",\"rome.\",\"sing\",\"express\",\"brows\",\"husband's\",\"boy.\",\"'s\",\"child.\",\"obey\",\"news.\",\"mile\",\"quick\",\"work,\",\"drums\",\"amongst\",\"proof\",\"bury\",\"violent\",\"fight.\",\"praise\",\"charged\",\"own,\",\"hither.\",\"able\",\"other's\",\"seest\",\"whip\",\"camest\",\"beheld\",\"grows\",\"patience,\",\"'twill\",\"tells\",\"reverend\",\"prosperous\",\"another,\",\"wounded\",\"a'\",\"large\",\"fools\",\"lie,\",\"willingly\",\"met,\",\"coward\",\"weeds\",\"death's\",\"devise\",\"country,\",\"joy,\",\"thrice\",\"people's\",\"bodies\",\"denied\",\"enforce\",\"wrought\",\"if,\",\"would,\",\"aedile\",\"way.\",\"stoop\",\"thyself,\",\"hap\",\"comes.\",\"warlike\",\"peril\",\"beast\",\"wot\",\"course,\",\"lament\",\"banishment\",\"slay\",\"other,\",\"guest\",\"breast,\",\"better,\",\"thrive\",\"age.\",\"freely\",\"raised\",\"takes\",\"names\",\"yourself.\",\"dance\",\"perhaps\",\"charity\",\"reasons\",\"belike\",\"dreams\",\"brother.\",\"forbear\",\"grievous\",\"slaughter'd\",\"stabb'd\",\"fairer\",\"mind.\",\"revenged\",\"weep,\",\"happiness\",\"humour\",\"been,\",\"usurp\",\"lovely\",\"loss,\",\"hateful\",\"wither'd\",\"world's\",\"taught\",\"fall,\",\"hopes\",\"flies\",\"depart\",\"perfect\",\"spent\",\"liege\",\"committed\",\"bade\",\"brief,\",\"rest.\",\"bethink\",\"wound\",\"morrow,\",\"sirrah\",\"margaret,\",\"form\",\"liberty\",\"sorry\",\"birth,\",\"majesty,\",\"widow,\",\"pure\",\"match\",\"burthen\",\"youth,\",\"highness'\",\"to-morrow.\",\"needful\",\"fly.\",\"say'st\",\"lusty\",\"dares\",\"condemn'd\",\"special\",\"crowns\",\"credit\",\"palace\",\"happily\",\"seeming\",\"passion\",\"houses\",\"clear\",\"bona\",\"instruct\",\"curst\",\"angelo.\",\"claudio,\",\"provost,\",\"lucentio.\",\"alonso\"],\"x\":[-0.27103522,-0.20933999,-0.28122187,-0.46544057,-0.5463037,-0.4206137,-0.44358888,-0.2661458,-0.051303457,-0.41408998,-0.39657837,-0.4156024,-0.55725086,-0.5408285,-0.5379752,-0.48994714,-0.08366466,-0.73531836,-0.11455855,-0.399845,-0.1878209,-0.24750787,-0.5845023,-0.41640404,-0.4976243,-0.45184675,-0.22342812,-0.08724935,-0.35075098,-0.5076581,-0.61004335,-0.3761348,-0.35650906,-0.36448744,-0.56016004,-0.24225466,-0.56220263,-0.06934713,0.06443811,-0.42016935,-0.31144685,-0.19073077,-0.66443115,-0.6904263,0.10847024,-0.68645656,-0.2530146,-0.5413389,-0.28643668,-0.55709213,-0.30872154,-0.3471483,-0.5535491,-0.36095017,0.052825414,-0.42336398,0.14777462,-0.039415833,-0.28306618,-0.41161838,-0.93360174,-0.53186566,-0.58223623,0.095030986,-0.31431982,-0.30085146,-0.10947178,-0.47151,0.11463781,-0.5977955,0.21756442,-0.32364166,-0.8213499,-0.18939842,-0.4390475,-0.08902943,-0.76932025,0.08433982,-0.7130261,-0.26947176,-0.4949183,-0.9077559,-0.3553917,-0.31819376,-0.6091384,-0.57610476,0.018794756,-0.8826336,-0.6036449,-0.6807545,-0.048830245,-0.67823017,-0.59423304,0.08084229,-1.0874211,-0.26100892,-0.11783974,-1.0700195,0.13291323,-0.97319525,-0.4496293,0.12354816,-0.30160698,-0.95485425,-0.2234527,-0.57378346,-0.55751246,-0.34308562,-0.858423,-0.30276543,0.59841985,-0.22554573,0.70784825,-0.32642937,-0.29933745,-0.41554725,-0.63694745,-0.35118687,-0.59749323,-1.0057836,0.036983646,-0.0724314,-0.24586064,-0.6800752,-0.89099175,0.07166018,-0.9524191,-0.17188348,0.69098276,-0.018102398,-0.95705754,-0.6048497,-0.4502674,-0.14676009,-0.6938845,0.09923024,0.09663953,-0.079090595,-0.7485127,-0.58049834,-0.47773236,0.73494583,0.7107099,-0.664914,-0.26053718,-0.36131984,-0.51768935,-0.27489296,-0.5155499,-0.74844325,0.2907276,0.2918985,-0.47454497,-0.2753149,0.1295023,0.3400959,-0.9882883,1.0827655,-0.74820167,-0.7269317,0.5917299,-0.61855555,-0.6738301,0.7919844,0.5603484,0.8822799,-0.86476445,-0.45711222,-0.535558,0.09883662,-0.98143125,0.46410555,-0.24732554,-0.7686947,-1.0067365,-0.6682194,0.13811208,-0.62198216,-0.5564471,-0.89948857,-0.70299965,0.11811481,0.64812714,0.30685404,0.06917297,-0.6394783,-0.51552236,0.49832,0.5684349,-0.29484093,0.4951375,-0.54069585,-0.5858245,-0.9944131,-1.4767796,-0.68531543,-1.0748502,0.13065523,-0.4977107,0.4368562,-0.8984811,-0.1765967,-0.6631911,-0.914992,-0.6144526,-0.45559928,-0.32789597,-0.11054575,1.0475823,-0.57444215,-0.33579198,-0.44210863,0.99135023,0.2002389,0.60239697,-0.43452102,-0.5978766,0.45174763,0.61714965,-0.27209347,-0.5640369,0.76222765,-0.5581359,-0.12768248,-0.70507807,-1.0881547,0.2994044,-0.24605888,-0.51361144,-0.39638376,-0.48386073,0.96042144,-0.97717446,-0.50877166,0.5678165,-1.0940357,-0.5505818,-0.2181569,0.3829128,0.132119,0.55823076,-0.42696938,-0.9883953,-1.1211935,-0.22059304,0.05264328,0.07325973,-0.2780202,-0.32544515,-0.19178677,-0.5169878,-0.9095585,-0.62580967,-1.0149372,-0.8138382,0.97400004,0.25297967,-0.5398189,0.31959915,-0.08829675,-0.80025375,-0.34638962,0.1929599,-0.55329305,-0.42917368,0.64179766,-0.6393753,-0.32014424,-0.7695108,0.29339382,1.1269163,-0.30138472,-0.5716542,0.3414674,0.8089208,-0.89793503,-0.23117873,-0.2293236,-0.5597281,-0.4382991,-0.51054376,-0.5128524,-0.87710303,0.5134981,0.3372675,0.12073875,-0.35827166,-0.5876855,0.7110571,0.56606567,-0.47155517,0.156133,-0.42597285,-0.35267624,-0.23254351,-0.3901607,-0.057898797,-0.794977,-0.6050878,0.20518918,-0.5127109,1.0968,0.8178407,-0.2142503,-0.35845232,-0.43363926,-1.1022063,-0.41381338,0.78786165,0.594084,-0.4007727,0.34324485,-0.21959761,-0.43068388,0.1767111,-0.45593122,0.46665448,-0.65254366,-0.6750967,-0.3343489,0.34831893,0.56199586,-0.39550683,-0.08442505,0.5280324,-0.65702724,0.51622057,0.45897797,-0.33924162,1.0154841,-0.5745232,0.21705492,-0.23490664,0.19701137,0.34379745,0.6071722,0.1452272,0.16666704,-0.0891944,-0.6174577,0.6212874,0.4564468,-0.6250774,-0.49082276,-0.43921167,-0.2282534,-0.861343,0.13017806,-0.92904866,-0.8687988,-0.76819,-0.16734871,-0.43947843,-0.78803086,0.51215005,0.13623123,-0.17207664,-0.34517208,-0.47418138,0.5973413,0.87948865,0.061536938,-0.58097804,-0.7754068,0.20937137,-0.22226386,-0.22919755,-1.3002807,0.79956406,-0.50574625,-1.1251148,0.2391924,-0.719327,-0.40378454,-0.49464488,0.5584231,-0.80807704,-0.24010196,0.56917095,0.6970336,-0.5097061,-0.6259267,-0.26510173,-0.7825602,-0.8952543,-0.033473685,-0.5554425,0.40225008,-0.5178017,-0.7998657,0.021552958,-0.610302,-0.03621506,-0.22394547,0.10377191,-1.4266273,-0.031344026,-0.30737892,1.0672125,-0.18960682,-0.52594805,0.8830465,0.4416531,-0.33017364,-0.5017626,-0.21223058,-0.44797334,0.3521086,0.22391452,-0.3506568,-1.0795922,0.7080627,0.19175617,-0.66343033,0.051688086,0.7391411,-0.38645667,-0.57182646,0.36336115,0.546144,-0.112376824,0.64140755,-0.41224703,-0.20674725,-0.8305494,0.2720811,0.8196564,-0.9364899,1.5985205,1.0895499,-0.3172576,-0.85677814,-0.25660104,-0.67578447,-0.5914651,-0.4235441,-0.40074188,0.26227978,-0.47967592,-0.30222502,-0.15237051,0.58399516,0.94213134,-0.35742584,1.0302402,-0.59047216,0.19847038,0.2238891,0.6686045,-0.55368,0.30634135,-0.8004908,-0.11978791,-0.51175356,0.30549917,-0.9465769,0.7918056,-0.6991862,-0.0697551,0.35895726,-0.1186246,0.4360266,0.10210737,-1.5764687,0.49095228,-0.09178763,-0.038441155,-0.23348539,-0.492937,0.17412557,0.057151932,-0.3178204,-0.2471708,1.2726007,-0.2127872,-0.6442335,0.6641443,0.9628928,-0.4774842,-0.28312704,-0.47521338,-1.0103098,-0.11839732,0.1719017,0.17260821,0.3668621,0.5138983,0.0633784,-0.55984855,-0.2482829,-0.008442133,1.114445,-0.062163994,0.13624196,-0.73788476,0.2690393,-0.06566482,-0.5107677,0.7087794,1.1896412,-0.008282176,0.0021046659,-0.27744007,0.6864297,-0.30745807,-0.5296876,1.271096,-0.41264594,-0.37459147,-0.23358634,-0.19598147,0.018779904,-0.03405458,-0.8306146,0.70219374,-0.00092590926,-0.89566123,0.21397513,-0.19155978,-0.40817136,-1.079212,0.3180507,-0.3184737,0.4004699,-1.0187528,-0.19122015,-0.22645742,-0.28726712,-0.64475214,1.1103117,0.27422005,0.09354888,0.7655126,-0.36312982,-0.85478586,0.2142103,-0.396121,-1.158145,-1.0903454,0.78926444,0.025564242,-0.06276829,0.30110046,-0.92710376,-0.5152396,0.11045309,0.72974885,0.9691553,-0.18836042,-0.46462494,-0.5305021,-0.45797434,-0.62379026,0.28832868,-0.97569746,1.0764762,-1.4445368,-0.6515064,-0.30503786,0.6180184,0.321967,0.2666278,-0.27946717,-0.5572183,-0.06031249,-0.6990601,0.1399294,-0.66185194,1.1620075,0.7177556,0.018846385,-1.0924481,-1.4792429,-0.20660168,-0.8731238,0.7017684,0.75504625,-0.4568447,0.5810792,-0.35449183,0.80134714,0.2551616,-0.4906544,-0.6146246,-0.3730752,0.2703317,-0.25276232,-0.9896677,0.5611496,0.30351406,-0.30230108,-1.2107038,0.29108414,-1.1309956,0.39721668,-0.60676736,-0.20268242,1.0023326,1.3591093,-0.26729247,-0.99699897,0.24798083,0.7848219,-0.10082646,-0.5169242,-0.25319692,0.25821844,-0.17358147,-0.17415467,-0.68772537,-0.9139598,1.1890261,-0.33986685,0.088009134,-0.24271706,-0.72640854,-0.59307545,-0.74595135,0.506688,-0.8336156,0.90544784,-0.32641983,0.85667497,-0.05204225,0.2592405,-0.25885674,-1.0502155,0.32907534,0.6219616,-0.20892926,-0.6507772,1.387532,-0.71958905,-0.060527984,-0.18008564,-1.1552002,0.32998657,1.31403,0.15008634,1.2204072,-0.568887,-0.7226863,0.42862153,0.3629768,0.081483364,-0.24743347,0.5994591,-0.5941395,-0.15758339,0.087513186,-0.28958246,0.10819996,0.61450005,0.030337283,-0.9373589,0.37472713,-0.62642944,0.9804498,-0.62293947,0.81528586,-0.6710848,0.44313812,0.5873558,-0.9089409,-0.15926561,-0.29438394,-0.9226807,0.21119924,1.1276262,-0.4460214,0.18171555,0.72838926,-0.06982586,0.990993,0.37684292,-0.20774192,-0.23837313,-0.19890007,-0.25534153,0.13608895,0.69247085,-0.01078267,-0.11596045,-0.9196766,-0.34128037,0.5478316,-0.60435754,0.539103,-0.48216113,-0.68751675,0.95088726,1.388564,0.7480976,-0.28256685,0.4051026,0.022743022,-0.2210111,0.3338079,-0.45012382,-0.23418221,-0.8338333,-0.83771497,-0.526267,0.39587152,0.57359105,0.73824966,0.5510371,0.29998645,-0.74251604,-0.5673619,-0.47072473,-0.039519317,-0.50401896,-0.5007053,0.5931531,0.40626833,-0.047050964,-0.5059183,0.36395842,1.04082,-0.8952942,0.050008036,0.2694837,0.87958777,0.67121,0.69617623,0.6217591,-0.41881272,-0.47044277,-0.103897735,0.6827576,-0.8360998,-0.55527866,-0.48049226,-0.5302528,0.08043037,-0.87363994,-0.11153432,0.67791927,1.1112199,1.2235942,0.5884309,0.36012635,-0.8387799,0.21225882,-0.552657,-0.3220549,0.18037258,-1.452595,-1.1669517,0.064821735,-0.5349817,-0.3514327,0.9823217,-0.51395863,0.9545322,-1.3572571,0.41347286,-0.8690418,-0.7405564,-1.425066,-0.49584472,0.66815215,-0.35353875,0.02534906,0.762296,0.6647431,0.40426347,-0.48398206,-0.089899875,-1.2018815,0.3819173,-0.6039866,-1.3108261,1.1119306,0.8648103,-0.394044,0.45472708,0.65985876,-1.3068572,-0.22235827,-0.8098839,-0.6262387,-0.5235061,0.96922326,0.2628509,0.47456408,0.389762,0.39661944,-1.596274,-1.0477128,0.09126443,0.9237252,0.017135317,-0.8035867,0.19337347,-0.5188401,-0.4200173,0.82957935,0.18599583,0.8515762,1.7395566,-0.49920058,0.7115169,0.12012667,1.1429456,0.056012154,0.34868425,-0.09068958,-0.9934879,-0.8424539,-1.0985159,-0.99774003,-0.90964967,1.2116975,0.79073757,-0.5930044,-0.21733966,0.95620376,0.75872487,0.28149015,0.15471338,0.7993472,-0.6222757,-0.04890665,0.1714817,-0.07659196,-0.16541782,0.5483013,0.46938702,0.43229842,-0.528682,-0.34417772,0.6086036,0.54461783,0.13347106,-0.2422116,0.63479155,-0.66693604,0.2907118,-0.6919775,0.5245111,-0.19791196,-0.28262818,0.710955,0.49689618,0.77765983,0.039671972,-0.7038499,0.8384966,1.2807484,1.3838252,-0.342804,-0.069370545,0.7615641,-0.39761993,1.6552727,0.509244,-0.35920358,-0.1743945,-0.17690213,-0.53124833,0.6627993,-0.20660384,1.3290334,0.57125473,1.3589166,0.88005173,-0.74870056,-1.2810234,-0.41243654,-0.6416379,-0.46858385,0.41792116,-0.25221038,0.6099866,-0.25630647,-0.25985965,-0.21120028,-0.7895762,-0.057493486,0.6711023,0.5046185,-0.54841375,-0.3265603,0.712943,-0.85193175,-0.9126335,0.7040223,0.56395495,0.31619236,0.037282165,0.7212338,0.85557944,0.40480915,-0.02758881,0.6730182,1.0741912,-0.49929246,0.46207005,0.51233596,-0.646117,0.46467912,-0.015300651,0.014409877,-1.0749738,-0.7840791,-0.30088943,1.0212578,-1.0713516,-0.026099192,0.51397485,-0.56837696,1.5423425,0.6175396,-0.66216147,-0.3042802,-0.6466801,-0.44658116,-0.40993786,0.6525982,1.6269908,0.45332143,-0.20098254,-0.03169445,-0.7057852,-0.40383172,0.4394149,0.8514072,-0.810698,0.36581737,0.77252114,-0.89262056,1.0678955,-0.10973634,0.117324516,-0.9720716,-1.093194,0.33100942,-0.39581296,-0.6359335,0.24858405,-0.724806,0.08202413,1.0884757,-0.19525267,-0.9275054,-0.41157025,-1.2591928,-0.4738274,-0.3486678,-0.28291577,-0.8905846,0.6010099,0.78286463,1.1990334,-0.63558865,-0.4820398,-0.4783452,0.029923003,-0.9494344,1.1428385,0.59478515,-0.20271282,-0.5306143,0.09268498,1.4155316,0.1565313,0.38910025,0.03726283,-0.9391694,0.8317,1.6690582,1.1865623,0.4927471,-0.3061311,-0.31338653,0.8599479,0.22783373,0.8770232,0.11976623,0.63504744,-0.34811062,-0.23591514,0.35036573,0.10362833,-0.090580754,-0.76983064,0.7901767,0.5592408,-0.59403133,-0.007288811,-1.0061293,-0.082781054,-0.85104084,-1.10932,-0.14630733,-0.0745403,0.38696632,-0.48765838,-1.2518619,-0.470606,-0.6064902,0.8301702,0.368421,-0.20995054,0.6907434,-0.4670939,-0.6501923,0.285064,1.1902462,0.60362816,-0.75517607,0.6552839,0.37756222,-0.5971261,0.14147188,-0.550307,0.6642979,0.73441553,1.2924479,0.8548195,0.57455736,0.740074,0.19179824,-0.47209772,-1.2614955,0.0752728,-1.1337621,1.3371625,-0.18843229,-0.21737115,0.14636815,-0.74551463,-0.34377337,0.057384945,-0.47943562,-0.05305039,-1.1846331,-0.615864,-0.61065423,-0.73483455,0.4560352,0.13330509,-0.62576485,-0.2958296,-0.02679284,-0.01912695,1.5522612,-0.03943833,0.10686441,0.5654338,-0.5723443,0.09234089,-0.5107654,0.5541071,-0.85389364,-0.39293242,0.27556294,0.14871268,0.5303492,-0.47482765,-0.22506693,-0.69167393,0.8509132,-0.45336413,0.3932631,0.4628669,1.0184761,-0.05739386,0.043449763,-0.29580665,-0.21464248,-0.1743126,-1.0251745,0.59197366,1.2824087,-0.032637015,-0.119546846,0.08686214,0.13471095,-0.64814615,-0.77517384,0.17306268,0.3800159,-0.054973587,0.14259814,-0.51164126,-0.9640995,0.82166064,0.72249585,-0.23787443,-0.5940678,0.5590803,1.3716215,-0.69779605,0.6903574,0.94107234,-0.5293551,0.4239924,-0.5706834,-0.2198404,-0.74965274,-0.592406,-1.104156,0.91412574,-1.0600752,0.07259305,-0.12380427,1.0891681,1.0172887,-0.38625458,-0.010131019,-0.3660803,0.90601474,1.2270672,-1.3913589,-0.07589528,-0.41873077,0.3391066,-0.55883646,-0.30195722,1.0726149,-0.763709,0.47628984,-1.0639406,0.042848904,-0.9942979,0.2467497,0.30397418,-0.31031314,-1.2598022,0.49797913,-1.0706102,-0.5888731,0.39830935,0.36536998,-0.7175664,0.5899428,-0.9255764,0.23811151,-0.53303814,-0.5905816,-0.4457349,1.0168157,0.16203329,-0.41713414,-0.6731157,0.21174285,0.60621065,-0.43363678,-0.1021035,0.5830786,0.84190804,0.9117727,-0.13622564,0.12944339,-0.31150228,0.44084668,-0.1440178,-0.49227363,0.19071443,-0.8186574,-0.7953205,0.4622055,0.4771885,-0.30003846,1.6049287,1.3928283,0.4373635,0.7907282,-0.25523913,0.2965433,0.7762842,0.61030036,-0.26365602,-0.9732649,0.5662029,-0.08322652,0.15608573,0.7191878,1.4264864,-0.90819275,-0.050590713,-0.75747097,-0.29703256,-0.3248209,0.18385102,-0.34859088,0.9182007,-0.11457641,0.15735127,0.8109933,0.56553847,0.99946594,-0.014158478,-0.7300358,-0.19794373,1.1141388,-1.0676627,0.9485765,0.22629888,-0.21035753,1.3639892,0.9211181,1.044872,1.2534631,-0.96624786,-0.29342413,0.33724788,0.5306718,0.21986252,0.44941187,-0.8772691,-0.45206442,-1.1232867,0.10276624,-0.5073452,-0.14448519,-1.753589,-0.013211933,-0.6312572,0.5241306,-0.32782605,-0.67457503,-0.62688696,-0.48363963,0.4545143,-0.44288856,-0.441987,-0.024996214,-0.28678253,-1.4171406,1.710889,-0.0663291,-0.58443016,-0.9164727,0.60825,0.18503538,-0.5070937,0.07643272,0.8186707,-0.3903986,-0.45219,0.7036955,-0.85176176,-0.56110245,0.6985172,-0.9565336,0.5759239,0.21487424,-1.1482328,0.10100698,0.65879446,-0.46229228,1.1600876,0.5118396,0.001770851,0.60516804,0.56948066,-0.4609137,-0.36676863,-0.73696643,0.15409797,-0.90640426,-0.21878096,-0.11301412,0.83366233,0.57927716,-0.42192525,-0.102838695,1.5432127,-0.12890862,0.41914755,0.8074493,-0.27253008,0.11292541,-1.0542948,-0.5827599,1.1141235,-0.6135693,0.0947868,-0.2392639,0.21591792,1.0117735,-0.15708044,0.07653146,-0.0242298,0.20164402,-1.0989949,-0.19276492,-0.17405832,-0.17548166,0.543022,0.67392284,-0.09105156,-0.14631267,0.6314585,-0.05837762,0.52776253,0.34341416,0.070796125,0.6360921,0.7297702,0.46648753,0.2607351,0.25631562,0.15588883,-0.10404609,0.13215412,-1.1380442,0.40262255,0.7008899,-0.49963418,0.32312125,-0.17184184,-0.8478566,0.75902253,0.6528037,0.7068508,0.33913565,-0.68554986,0.260883,0.7851749,-0.2592538,-0.7950584,1.0514346,-0.24151787,-0.28618684,0.85774314,-0.09335211,0.51794755,1.1060443,1.3287668,-0.1939276,-0.34445366,-0.9987159,0.48543444,0.6229073,1.2374328,-0.09165615,-1.2199873,-0.6743771,-0.315975,1.5574769,0.99840015,0.9247203,0.9872927,1.107379,0.37097692,1.6848109,0.8021558,-0.00045069135,1.0303769,-0.44254282,0.69186825,0.9049332,0.8387119,-0.6009184,0.8782658,2.1246154,0.025782533,-1.079107,-0.86760634,0.9588193,-0.7006294,-0.3500072,0.8119981,-0.43402702,0.6914704,-0.5601995,-0.2286136,0.69080913,0.6306717,-0.18593858,-0.824361,-0.29282853,-0.036611248,-0.6634911,-0.30710372,-0.23393296,-0.66685057,0.2842383,0.35056174,0.31756333,-0.2575864,-1.0621214,0.13538449,-0.56470203,-1.25051,1.6821101,-0.39388272,0.2847755,0.16887882,1.3765104,1.2470429,-0.15979077,0.731599,0.73497146,0.63199806,-0.4721909,-0.10164449,-0.23070851,0.3117373,-0.9775846,-0.4021168,0.8633449,1.0824146,0.5512323,0.4537933,0.16383895,-0.10369915,0.6877869,-0.4456954,-0.06281596,-0.6647005,0.2841198,1.5047985,-1.075739,0.65083927,-0.63987225,-0.43951672,-0.1811244,0.7333688,0.5685891,-0.8941073,1.8005147,0.61278933,-0.57849514,0.36229938,-0.41805893,1.1977686,0.8156207,0.7850818,0.06324445,-0.6369496,0.7525142,0.38302675,0.37117246,0.027743176,1.9985529,0.89738584,0.61004764,0.48530594,0.09452999,-0.5112244,0.6709088,0.5884264,-0.073055126,-0.47119388,1.2803814,-0.102726884,-1.1559675,0.23490395,-1.1520625,0.60643977,-0.5049941,-0.61235785,0.94563085,-0.48697898,0.6000785,0.062054582,0.6004467,0.6718595,-0.27537695,-0.97674096,-0.2592801,-0.20625219,-0.05188239,-0.3952162,-0.009844431,-0.52061194,-0.16693343,1.6346517,-0.17731938,1.0336026,0.51229155,1.100839,-0.7234211,0.57534933,0.19731134,-0.36707807,-1.3504485,-0.4768432,1.2510633,0.6498311,0.8919581,0.13631107,1.1790863,0.18102366,-0.4724871,-0.17234492,-0.24921004,-0.82023406,1.3362226,1.0986803,-0.14829338,0.4647299,0.9054693,-0.23001407,0.80655295,0.71918535,-0.446058,-1.0163476,0.446803,0.26114997,0.1305497,0.22718693,-0.77492535,-0.39182457,0.11171096,0.79537797,-1.3782941,0.9638402,-0.9185759,0.70105994,0.86985105,1.1135584,0.14256988,-1.1080259,0.12451613,-1.2172109,-0.87860143,-0.2781981,0.032102685,0.7349706,-0.3838236,-0.7100031,-0.17455754,-0.39695236,0.8690744,-0.5787579,1.0885618,-0.62580764,0.5614977,0.6438693,-1.5367502,0.467301,-1.3718851,0.4442,0.21246517,-0.48748392,1.1423131,0.45290175,0.89989156,1.0144608,0.91332,0.57502913,1.0288652,-0.48353752,0.80998766,1.4084545,0.02430936,-0.61381185,0.21300311,-0.1304402,0.3135969,0.4711225,1.3130853,0.47334343,1.1495867,0.3115437,0.41307282,1.0880995,0.8203911,0.67782605,0.3641237,-0.4668656,-0.21140993,-0.21186005,0.25222903,-0.8178378,-1.3045304,0.6588779,-0.25462157,0.71782094,-0.5109322,-1.1203966,0.58255786,0.6421547,-0.23111139,-0.59316105,0.3703714,-0.3688276,0.60930765,0.29743922,-0.52404153,-0.99345595,-0.82021856,1.1965967,0.1733534,0.03346788,0.44468105,-0.34099403,-0.64519024,-0.587639,0.08965977,-0.07582268,0.7022926,0.5367133,1.2214103,0.33582732,-0.26500836,-0.48479086,0.08960678,-0.7162764,1.2141587,1.1674827,0.8399345,-1.3928509,0.40610886,-0.45868355,0.98135436,0.39047965,-0.3546683,0.4964766,-0.790643,-0.76935977,-0.19114868,0.110461965,0.86189294,0.46960956,0.86405325,-0.43640587,0.40001538,0.5248859,0.58067465,-0.3803364,-0.84522706,0.7848022,1.48372,-0.6682992,0.60413885,0.8724749,-0.47055212,0.21212031,0.6219738,0.92297995,1.6154767,-0.6905951,-1.7347385,1.001697,1.0146316,1.4474958,-0.42414448,0.68092585,0.6327241,0.6969749,-0.004626459,0.73756987,0.8130476,-0.74412626,0.7477679,-0.6377283,0.11866403,-0.52204776,-0.44303423,0.9007411,-0.5448408,0.7311534,0.62023604,0.37666407,1.2807724,0.21181281,0.2667242,1.1351873,0.83606064,-0.46468353,1.105759,0.6486592,1.3186562,0.09449484,-1.0063925,1.0549152,0.045846388,0.43523434,-0.4500863,-0.13723888,-0.27189776,-0.02268992,-1.0389416,-0.13401322,0.2937872,1.0666256,-0.32896683,0.59309804,1.2286516,0.611639,0.034301642,1.1359309,0.9109418,0.30707982,0.09119547,0.42327487,1.0585748,-0.12291936,-0.14103198,-0.363491,0.27689412,-1.1407936,-0.7979123,1.4155047,0.63534737,-0.25455052,0.2113438,-0.5922431,0.839013,0.22648509,-0.16117659,0.088207275,0.56804705,0.94660985,-0.82866055,-1.4502764,0.49096486,0.3698389,0.6738907,-0.25573972,0.03555452,-0.5075448,-0.30533817,0.40101114,-0.045409,0.6909093,0.4892949,-0.54746586,1.1764601,0.346849,0.7054064,1.0299158,0.32816523,0.8718935,-0.48878312,0.28041145,-0.022112697,-1.3861722,0.8005162,1.0486408,0.5352647,-0.01983372,0.33795708,0.75629663,-0.03966002,0.39498845,-0.91770965,1.5422348,0.6127833,-0.4349822,0.2438024,2.064136,1.1908021,-0.9040616,-0.7480163,1.358455,-1.0937058,0.17929636,-0.97317094,0.22618432,-0.2848536,-0.36667296,-0.15791664,0.20015582,0.04216938,-0.49735948,0.46934542,-1.0655663,-1.0468693,0.8111569,0.62709606,0.2989442,0.39949584,-0.9662402,0.078900374,0.91177446,-0.63114256,-1.2337267,-1.0978873,0.22303124,-0.41070333,1.3073547,0.011543197,0.09210541,0.35517302,-0.2845039,0.19559127,0.46247688,0.8038365,-0.09176054,1.5770303,0.4484844,1.2339805,0.34683797,-1.3859086,-0.4940859,0.4599789,-0.8445251,1.4261631,-0.7715392,-0.09560287,0.6186713,-0.39577124,0.07277314,0.94864476,1.2082812,-0.3556926,0.40617585,0.41360214,0.30783737,0.18397154,1.2418869,0.2155696,0.656492,0.025380306,-0.25369123,0.20533256,0.2995145,-0.26958403,-0.33395484,-0.40218583,-0.80484504,0.0740222,-0.11561839,-0.17484632,-0.3443128,1.0527817,-1.9509422,-0.4675619,-0.75928277,0.4839697,-0.8546636,1.1958846,1.4085845,-0.72321844,-0.12723038,-0.12382291,-0.2242063,1.4976568,0.09511967,-0.013362324,-0.27763996,-0.5137811,-0.9348716,0.39728633,-0.20204881,0.3190297,-0.5363609,1.0743259,0.7244626,0.35900787,0.5405983,0.30300623,-1.5022683,-0.054785583,1.3829416,-0.29771313,0.3354849,0.17940024,-0.22429596,0.8593319,-0.29295942,1.6431966,-0.09547864,-0.5030802,-0.004509757,0.066968866,-0.38001028,0.056705475,1.0281967,-0.3934799,0.32646003,-0.986485,-0.48817554,-0.3165314,-0.11609474,-0.4455935,-0.9070079,1.5304928,0.78934103,0.3526478,-0.9345993,0.10173159,1.3065575,0.7755481,0.22283706,-0.36960876,-0.29356477,0.61490303,0.91934437,0.6542747,-0.6305507,0.94534165,0.091804296,0.18211623,0.854346,1.5324745,-0.39084467,-0.87290215,-0.6616276,-0.3328358,-1.2130463,-0.5648615,0.447621,0.031974662,-0.20331895,0.3102983,0.062418077,1.5506784,-0.37172136,1.6825643,-0.9984924,-0.08107244,-1.3585315,0.6852024,-1.1620704,1.1469436,1.0570279,0.5705339,0.5961237,-0.8122857,-0.11712041,0.00091340847,0.1350469,-0.7106971,0.49552834,-0.91283804,-0.15360999,-0.34915456,0.4869061,0.26161343,-1.1327019,0.33497235,0.624378,-1.0994664,1.7092096,-0.37121725,-0.045082092,1.3394061,0.56564265,-1.03678,-0.884634,1.3286902,1.1372775,-0.7713106,-0.57989514,0.8767037,1.4322069,0.86443496,0.67180943,1.3407694,0.2788264,-0.33105096,0.32146502,-0.23177293,-0.7356633,0.817042,-0.45367667,0.52297574,-0.6168743,-0.09304076,0.080894075,-0.19535606,-1.2323539,-0.18553972,-0.22831707,-0.00087971287,-0.40122077,1.2090391,1.0237238,-1.1755577,0.5052575,1.2553326,0.33343396,0.6450404,-0.52758396,0.7392327,1.022018,0.980924,-0.9515326,1.0852326,-0.9218821,-0.7318335,0.8377935,-0.6043693,-0.1708168,1.1614138,0.9571544,-0.29699978,1.272162,-0.42478743,-0.05406073,0.50645685,-0.2695141,0.6033558,-0.5610078,-0.34910107,-0.57552457,-0.20022026,-0.6718082,-0.2393659,-0.19522963,-0.085092135],\"y\":[-0.037803188,-0.029856576,-0.20792985,-0.33584952,-0.20920393,-0.18306234,-0.10516313,-0.24126875,-0.33895633,-0.3372705,-0.20565575,0.07394608,-0.09385255,-0.29112718,0.06596622,0.013859334,-0.42621967,0.28941494,-0.07782812,-0.18766952,-0.14499275,-0.30679932,0.5100021,-0.24138513,-0.22015473,-0.37716594,-0.2125461,-0.2890527,-0.27432358,-0.4624273,0.031828497,-0.046186566,-0.25628874,-0.059315305,-0.052798882,0.050170463,-0.14728865,-0.2120947,-0.16670354,0.22629991,-0.21196267,-0.17720991,0.72998047,-0.07261524,-0.33619863,-0.16426298,-0.14756013,0.086243354,-0.2148458,-0.03682075,0.13301305,-0.2482676,-0.6435257,0.09421322,-0.15595835,-0.10378598,-0.30247012,-0.15499634,-0.4632349,-0.077155285,0.3495972,0.2555059,-0.42521974,-0.53314584,-0.6076871,-0.4818418,-0.2688142,0.24152914,-0.22163206,0.74851424,-0.25837108,-0.07236997,-0.025000649,-0.38327363,-0.77230895,0.356142,0.2578477,-0.2552112,0.040184204,-0.09351499,-0.59258866,0.35806435,-0.27248743,-0.7885404,0.044531167,0.32667002,-0.27474406,0.4391352,-0.24865448,-0.43293232,-0.29043594,0.19041422,-0.2278316,-0.3667448,0.8812377,-0.5142399,0.6348705,0.63135004,0.02441365,0.8932991,-0.80451834,-0.36190194,-0.4459006,0.77989817,-0.6222604,-0.5954481,0.561924,-0.70201576,0.53122526,-0.69321597,-0.52800566,-0.3662052,0.035554677,-0.7613736,-0.85119385,-0.948435,-0.63188523,-0.077091806,-0.92262274,-0.5789175,-0.3488023,-0.38517052,-0.22144173,0.5774313,-0.7889072,-0.56207865,-0.43937868,0.22004083,0.25515574,0.17689861,1.098256,-0.46688616,0.25739062,0.057750445,-1.0236737,0.15352988,0.024331762,0.13661799,0.5057069,0.008119386,-0.4154774,0.06820992,-0.36911166,0.16437484,-0.08956243,-0.47315437,0.7254493,-0.6193635,-0.48725846,-0.85672987,-0.4778229,-0.33259478,-1.0697083,-0.6840998,-0.3251275,0.37035057,0.2340429,-0.06356756,0.22048804,-0.38760218,-0.31477797,-0.2955584,-1.1568922,0.15949483,0.16286944,0.0070471875,-0.6839683,-0.0653971,-1.0725094,0.15359876,-0.91085976,-0.3841933,-1.0730525,0.81254786,-0.40868637,0.84694153,0.49747446,-0.5639297,-0.75558007,-0.6205009,-0.7885098,-0.67479527,0.0692835,-0.113115534,-0.41229853,-0.21337214,1.0218788,0.12513688,-0.11400814,-1.3075423,0.16603601,-1.1012084,-0.63359976,-0.7615928,0.08101495,-0.2864213,-0.21602772,0.11505133,-1.0872823,-0.25018626,-0.27622783,-0.2269562,1.0568546,-0.16973819,0.35561284,-1.5338653,-0.15854812,-0.546966,-0.21386696,-0.78309166,-0.27754313,0.10124454,0.0844577,-1.1975,0.072254665,1.1940194,1.0667497,0.031530157,-0.43022272,-0.22748724,-0.9347667,0.009510406,-0.7454812,-0.11515282,-0.3641253,-0.65558314,-0.2764005,-0.3402235,-0.21919505,0.7563699,0.063736506,0.15105478,0.14513668,-0.67640156,0.533772,-0.8976,-0.98702663,-1.1402465,-0.10916356,-0.70092446,-0.639067,-0.31889614,0.5885082,-0.56704974,0.35029674,0.26197085,-0.9795144,-0.043982353,0.058047473,-0.42343152,-0.5665051,0.55709773,1.0078107,0.7044684,-0.59529704,-0.26562393,0.16644712,0.60218346,-0.4206941,-0.28988165,-0.2649148,-0.067370735,-0.17936493,-0.33693904,0.92779416,-0.33233187,0.04740564,-0.7440889,1.117074,-0.40805972,0.102705374,-0.80897975,-0.52100796,-0.024856022,0.30550212,0.059967708,-0.14019066,-0.6377208,-0.15907125,-0.11824395,-0.3143675,-1.545667,0.5279289,0.17821257,-0.087695904,-0.49499983,-0.22828506,0.16287634,0.11305947,-0.3670276,0.37576544,0.6287643,0.0049060616,-0.3752815,-0.04927539,0.3012632,-0.006957195,0.7786401,0.62217987,-0.15896507,1.1281552,0.59168994,-0.2587836,0.0024046202,0.54456645,-0.14886236,-1.7334929,-0.21952796,-0.050104834,-0.3327445,-0.60436183,-0.35296538,0.8287381,0.025090951,-0.63693845,0.0035136284,0.20713708,0.0893146,-0.69605297,-0.11223183,-0.1751338,-0.12672973,-0.94728965,-0.12875487,-0.16420683,-0.33713543,0.631769,-0.42866102,-0.92502457,0.24824932,-0.7725075,0.7884985,-0.17773202,-0.44277915,0.22197242,-0.12003802,-0.4219671,-0.15385972,-0.6483854,-0.72156465,-0.029420502,-0.7123209,0.84949994,-0.43416968,0.31095904,0.1294275,0.8186909,0.29283988,-0.9490707,0.9312011,0.42445135,0.041297708,-0.0013815232,0.020529322,0.32442668,-0.046688173,-0.91106963,-0.9104194,0.9314117,0.3446288,-0.10047752,0.06416743,-0.90595794,-0.73320234,0.49895987,0.57154036,0.66021067,0.5713445,-0.08764366,1.145697,-1.3861696,0.097567976,-1.3496134,0.021523625,1.1476887,0.35566702,-0.58208907,1.0067803,0.123486616,-0.11010508,-1.0256398,-1.4286815,-0.15918054,-1.1778218,-1.325582,0.21360932,-0.87862086,0.09318717,-0.40960273,0.34956732,0.8751489,0.53146243,-0.9170686,-1.1830194,-1.1839947,1.1707265,-0.44977245,-0.008737534,-0.55525804,-0.0496549,-0.86743754,-0.8272712,0.16224864,0.5941754,-0.9144538,-0.6547226,0.64596564,-0.09315953,0.017599775,-1.697941,1.1541713,0.39533547,0.021733033,0.6155195,-0.6820672,0.38477176,-0.56044275,-0.6660332,-0.07438039,-0.00822407,-0.84671026,-0.52273965,-0.84596735,-0.67789954,-0.97386295,-0.12879123,0.014209632,-1.0176845,-0.08105176,0.15053429,0.8086941,-0.14152826,-0.5956992,1.0016818,-0.09280981,1.2283654,-0.0635969,-0.17885411,-0.63001955,0.58270097,-0.25584805,-0.4808089,0.33800045,-0.09173405,0.2450637,0.8526692,0.6266094,0.23332834,0.045521583,1.2940621,0.04714068,-0.48780367,0.38746965,0.72072953,0.34016848,-0.48377228,-0.08998033,-0.821747,-0.92905396,-0.0108922785,-1.0699149,-0.10552715,-0.9470587,0.731458,0.04305489,0.6953311,0.5383544,-0.33275914,-0.46953598,-1.1632901,-0.60584235,0.19046965,0.5321152,0.40331674,-0.11916763,-0.0059376983,-0.10943395,0.22508784,-1.1902344,0.66774815,0.9923884,0.5092062,0.7922178,-0.55668485,-0.14949961,0.073462546,-0.2053107,-0.55349755,0.5380773,1.5821428,-0.25355542,-0.104596175,-0.65767807,-1.2393508,-1.2013936,1.1328884,0.09782557,-0.99421394,0.1287714,0.5913879,0.029293556,-0.3533308,-0.16009963,0.1109016,-0.6555156,1.0238899,-0.18170804,-0.039509084,0.31967777,-0.9284692,-0.070349924,-0.28518748,-0.71629614,0.89965737,0.3973366,0.21637833,0.14083253,-0.25169346,-0.4217798,1.2777749,-0.69260573,0.5229233,1.4030188,0.10813093,-0.4863539,-0.7506178,-0.09846369,0.05639719,-0.97526294,-0.3739883,-0.32178694,-0.88456017,0.28034973,0.7999046,0.41842106,0.10531964,-1.4680839,0.34296662,-1.0147066,0.14854975,-0.72079146,0.094145216,-0.103288785,0.9337256,0.90025634,0.31754088,-0.0651372,-0.4629671,-0.08651651,-0.5505963,0.4757795,-0.73949504,-1.1678706,-1.0099545,-1.2022539,-0.29253593,-0.18709473,-0.39888805,-0.48468167,-0.338968,1.1291362,-0.07136978,0.4014812,-0.95278347,0.3236821,0.8285574,0.040204212,0.92031646,-0.623335,-0.35819367,-1.3207008,0.3523395,0.8896041,-1.2839305,-0.52575135,-0.06112792,-0.830204,-0.09631957,-0.07856681,0.7516556,-0.5084019,-0.6472936,-0.14370152,0.48223114,-0.52520335,-0.02156875,0.19060692,0.13932061,-0.24830252,0.30383885,-0.17608172,-0.40811682,-0.033124186,-0.15263207,-0.37974027,-0.016736088,0.04603718,-0.24100472,0.31478915,1.4182622,1.300357,0.053072944,-0.08343602,-0.39697313,0.6606284,-0.05518321,-0.20061632,0.27206194,1.2137457,0.49171498,0.82353956,0.05303197,-0.6902327,0.03139715,0.65563214,0.6022498,1.5333952,0.38254386,0.26829886,-0.49406588,-0.8191454,0.059981618,0.28753552,-0.31109023,-0.13408111,0.11371888,1.0626318,0.2910383,-0.37493074,0.08751347,0.52655244,-0.16727242,1.5675694,0.38480556,0.9306282,-0.13950063,-0.57331383,0.18020043,0.508314,-0.13940278,-0.4494929,0.826678,0.72086287,0.26086035,-0.01770447,0.48000202,0.06518313,-0.8846346,0.5416042,-0.5777606,0.89910936,0.49264312,-0.6368939,-0.083010815,1.46335,-0.52577853,-0.30062193,-0.46620405,0.47624615,0.533327,-0.1105992,0.2614376,0.14115459,0.069228135,0.7876566,0.8783243,0.8221314,-0.1256306,0.025603775,0.8447064,0.34197664,-0.56737983,-1.1569083,-0.056334127,-0.26629424,-0.7763695,0.14653938,-0.18254516,0.18973565,-0.5468313,0.14532994,0.6930761,0.7238076,1.0281739,-0.22586566,0.48360026,0.8700854,-0.09978978,-0.47353882,0.55341405,-0.3311217,-0.22288035,0.65234137,-0.64475095,-0.3787986,-0.16216017,-0.22669835,0.118761145,-0.17236483,1.079813,0.7817398,0.06293441,-0.8865516,-0.08835316,0.2853079,0.061468244,0.05966106,-0.6062075,-1.6485643,0.84446007,-0.55885905,0.0561161,0.15677324,-0.09613364,0.6691591,-1.1289914,-0.45828354,-0.30071518,-0.099773616,0.27474344,0.9404922,-0.3013126,-0.5411899,0.71407473,-0.011889808,0.022355046,0.23613822,0.41001764,-0.5742231,-0.7165544,0.26741785,0.75016147,-0.3987345,-0.51090187,-0.8261621,-0.45929798,0.6807571,0.40975598,-0.17328231,-0.6022891,-0.29896128,0.341762,0.202967,-0.68434846,-0.48039234,0.14210242,-0.38996124,0.21365283,1.4250518,0.48185423,-0.7368092,0.31686673,0.94558436,0.013999697,-0.9891393,-0.42125353,0.16317369,0.8333542,0.96982366,0.13096415,0.3750853,-0.23288716,-0.07540273,-0.69300795,0.63584846,0.32940114,0.015299501,-0.44788244,-1.1795672,0.32558346,0.35575435,0.85427195,0.6389341,0.32091582,0.12907279,-0.002560916,0.55897653,0.5759274,-0.06391707,-0.7184283,0.19216798,-1.3432679,0.051423594,-0.6083725,-0.5595798,0.7755229,-0.26032484,0.53517646,0.31999767,0.8945195,0.6583693,-0.065860234,0.20055404,0.44350362,-0.47909176,-1.0081365,0.42952597,-0.28414315,-0.06333247,0.06755925,0.36602005,0.1679473,-1.2552428,-0.055495963,0.6069442,0.21284948,0.4459098,0.21085653,-0.40813476,-0.41026893,-0.43126982,0.561502,-0.44045305,1.2517891,0.8275046,0.628412,0.3991341,-0.1193131,-0.6977389,-0.17042498,0.045931146,1.031298,-0.40379187,0.49193066,-0.114040434,-0.5518713,0.27703628,-0.109890886,-0.19413961,0.51363105,0.004931061,-1.2279284,-0.08637316,0.13856314,0.8479475,0.053146802,-0.62328416,-0.24517356,-1.2573645,-0.6579041,0.3001576,0.44248158,-0.87358654,-0.45684454,-0.21726961,0.6500547,0.507765,0.24494326,-1.3239594,-0.20104916,0.1864757,0.20649108,1.0085675,0.1932155,0.27447075,-1.3975513,0.050294153,0.24806088,-0.3243447,-0.3455658,-0.97108394,-1.3157738,0.32219738,-0.043313157,-0.09403301,0.5022317,-0.15085734,-0.10611873,-0.12678495,-1.1255628,0.88525134,-0.42439288,-0.12870388,0.6440274,0.42377603,-0.057375927,0.5758681,0.27107844,-0.17617434,1.3389994,0.9245379,-0.32793805,-0.45351934,1.3327179,0.60622793,-0.04314177,-0.0800973,-0.9827773,0.5178259,0.86243635,-0.5388695,-0.5184949,0.13136804,1.3355594,-0.75895387,0.07145838,-0.11692881,-0.1701636,-0.1080962,-0.3419602,-0.7124174,0.31099972,-0.0015398872,-0.89283156,0.49642074,1.5025734,0.9085967,1.1531795,-0.40323606,-0.6366604,-0.96468073,0.09089419,1.3824309,-0.31530812,-0.31778893,-0.20287094,1.32933,0.14667639,0.73987657,0.11910924,-0.6446541,0.3087738,0.08583351,-1.0362269,-1.3268659,0.678379,-0.5133679,-0.40800422,-0.5418516,-0.67742014,-0.24217896,0.25431982,0.4248977,0.11815526,0.27909288,-0.97415596,1.0967416,0.43112284,0.11819532,-1.0891546,-0.68711543,1.008958,0.82476354,1.163238,0.72060466,0.6040013,-0.8363458,-0.040538635,-0.58801144,1.130418,-0.545651,-0.47158703,0.7773936,-0.22515443,0.1980941,0.22248231,0.84619033,-0.13110857,0.19679013,0.02113545,-0.99387735,0.26525533,0.22949727,-0.1936173,-0.58889014,-0.500345,0.2595368,-0.57567245,0.85403734,-0.61031246,-1.1421165,0.4714551,0.4658658,-0.11673354,-0.40901974,0.64284503,-0.4990474,0.46162027,-0.03380135,-1.048821,-0.6349809,0.20091787,-0.16956899,-0.044186775,-0.023969386,0.24245238,-0.46676642,1.0007921,-0.015863823,0.07389172,-0.4177868,-0.24262653,-1.0736378,0.5765194,0.36142454,-1.7730188,1.078519,-0.43985176,-0.76647586,0.013658197,0.5654496,-0.44468486,0.0028714046,0.35889083,0.14357959,1.0665748,-0.4651821,-0.5768789,0.019387705,0.18878451,0.41840696,0.6350588,0.64858663,0.85954833,0.6026611,0.22458066,0.1585489,1.1785136,0.16915613,0.3932887,-0.7524785,0.5826758,-0.6772722,-0.95456487,-0.6690576,-0.3107944,-0.4760991,0.9379137,-1.2047734,0.33660433,-0.067554794,-0.38804302,0.41101918,-1.2316661,-1.1517223,-1.0173327,-0.58969957,-1.8170795,-0.77538455,-0.0073887086,-1.2758002,0.9753898,-0.17789748,-0.08433637,-0.27530667,0.2939373,0.17814244,0.12516558,0.59660584,-0.7817638,0.38093704,-0.65079564,1.5112011,-0.1733616,-0.5505921,0.09620278,0.005513044,-0.379043,0.6515702,0.7041884,-0.90135974,-1.1941338,-0.03289078,-1.4352019,-0.63973117,0.6676366,-1.6895028,-0.115700744,0.74565786,0.20262401,-0.59930146,-1.3687541,-0.30422792,-0.8374912,1.1123024,0.8157231,0.11411221,-0.053093955,0.90026927,0.45582157,1.0032321,0.10198191,-0.4380176,-0.37716562,-0.10664917,0.89088976,1.0855536,0.13257797,1.2305692,0.28140056,0.5678591,-0.89989066,0.46376956,0.24677607,0.3090448,1.0949864,0.50727284,0.09647888,-0.69867545,0.18051404,0.69497067,0.49161825,-0.3987208,0.82696545,0.74143696,-0.19994631,0.7245243,1.0077667,-0.6622144,0.3024823,0.26135215,0.1700814,0.700307,0.58909106,-0.34540406,-0.19753729,-1.1643643,-0.45811883,0.3814679,0.10319196,-0.7398662,0.06700309,0.4487716,0.90240216,0.87170017,0.87060285,-0.17734925,-0.2466854,-0.23983897,-0.4190579,0.18265638,0.59453875,0.24783494,0.47856712,-1.2164648,0.32468212,0.41640827,1.0238166,-0.2609726,-0.59430414,-0.657166,0.30123398,0.7764436,0.5676858,-0.5210784,-0.53556544,0.8746754,0.3146584,0.75561404,0.8300597,-0.27699876,-0.5426001,-0.0055379407,0.26440057,0.22647823,0.87437147,0.66774935,-0.2764072,-0.39552507,-0.50399476,0.44281805,0.07380331,0.29451373,-0.38429078,-0.1384171,-0.0003303881,0.059207667,0.21915701,0.27943316,0.52645224,0.25481802,-0.15813014,0.40068033,0.7419576,-0.5900476,0.043562844,0.16583829,0.24802373,-0.7820147,-0.6003925,-0.21804263,-0.6399117,-0.88229513,-0.53871864,0.06622489,-0.1366318,-0.13338579,-0.20928809,-0.67442054,0.5382147,0.15833178,-0.24943697,0.27118492,0.109222226,0.88544804,-0.27774328,1.1585542,1.31157,0.26776928,-1.4068656,-0.035484936,0.2584969,0.02945274,0.83095825,-0.3726473,-0.2001525,0.208383,0.29033446,-0.010637032,-0.6635445,0.086824045,0.13169453,-0.07084033,0.5868453,1.3211985,0.09768675,-0.48828888,-0.68910426,-0.87164193,0.42390606,0.16371538,0.93528736,0.040321004,-0.903399,1.0591497,0.780581,0.35888287,0.3231473,-0.9869086,-0.5035585,0.028515447,-0.32772562,0.87829393,0.1657091,0.18726082,-0.5687505,0.34588134,-0.2542111,0.030308386,-0.5767172,0.46276727,0.032626484,-0.58311254,-0.74069923,-0.5729353,-0.24249703,-1.1062287,0.55161864,0.22487292,0.26008654,0.017270721,0.45213267,-0.1261571,0.1944489,0.53371054,-0.073968016,-0.29657084,0.1007316,-0.28888118,0.62402385,0.53469473,-0.18923013,-0.8202,-0.32984453,-0.5837344,0.56543887,0.32089162,-0.008288669,-0.30852115,-0.27101701,0.9998073,0.85771006,-0.9170126,0.22965826,-0.2532433,-0.104184106,-0.45748636,0.7079483,-0.07665357,1.0198338,0.13275252,0.27687904,1.1860996,0.54482025,-0.5755692,-1.0779763,0.6054122,-1.0809232,0.041508183,-0.99579287,0.08052491,1.0074693,-0.51884127,0.160973,-0.9395606,0.034260303,0.18549639,0.30977264,-0.21448913,-0.9210751,0.5388433,0.2625538,-0.17479903,-0.8729792,0.7014304,0.2626441,-0.1259876,0.75742793,0.81312096,0.30948332,0.017909776,-0.7002499,-0.5007212,0.6636584,0.652615,0.77361757,1.4998444,-0.18401544,0.26690853,0.07708526,0.1495142,-0.038864654,-0.18933423,-0.07043846,-0.091990516,-0.4125864,-0.34688053,0.75295734,0.18961965,0.17149661,1.1110414,-0.050268065,0.2088984,0.4679414,1.3632792,1.6701921,0.13370061,0.7991612,-0.53835523,0.07812003,-0.15037179,0.9102792,0.761617,-0.83798873,-0.033099927,0.46728402,-0.18011044,0.2880412,0.5383129,-1.0169775,0.5058282,-0.048829883,-0.17050928,0.051660135,-0.41443372,0.6383363,0.64361185,-0.44428858,0.05329894,-0.4770082,0.40369147,0.5594536,0.57360005,-0.34816736,-1.0182457,-0.9534142,-0.08427344,0.10296311,1.1175863,0.23343264,0.5396778,0.8419177,0.10704638,0.23539202,-0.35933992,-0.120327465,0.18114895,-0.03573211,-0.029937467,-0.70750886,-0.565958,-1.525571,-0.5139195,-0.19390386,-0.4348163,-0.6455132,-1.4153349,-0.61180925,0.7979632,0.059466045,0.27265462,-1.215542,-0.581549,-0.38225952,0.5162738,0.67389077,-0.0878819,0.33457878,-0.07314478,-0.09754312,0.98503816,-0.52253526,0.5972907,0.48949754,1.1033964,-0.1881265,-0.35408974,0.1807431,0.9733597,0.08900452,-0.4847848,0.39148828,0.1441469,0.6491037,1.1727941,0.5733779,-0.7805538,0.021140493,1.2104594,-1.4767473,-0.3660223,-0.27592972,0.6823437,0.31763437,-0.43872985,-0.7160646,0.3219026,0.7865359,0.8611357,-0.26526615,0.1959923,-0.035310965,0.22819906,0.25344265,0.13059059,-0.8310168,-0.50714517,-0.5369712,0.24829794,-0.6010139,0.6266846,0.31846595,0.76678693,0.41388685,-0.7093559,-0.905454,-0.03257468,0.15080394,-0.009487148,0.37098598,-0.4314711,-0.61474645,0.266191,0.11314536,0.43190235,-0.19757876,0.81064934,0.9031099,-0.48330078,0.19179046,0.5271823,0.008403843,0.63756007,-0.036882892,0.7283544,-1.3491822,-0.1668233,-0.06939978,0.4401395,-1.1075418,0.7733813,-0.7630212,0.71460944,-0.01870514,-1.0114385,0.13248432,0.011915026,0.38440675,-0.7051166,-0.6257806,-0.4003538,0.94456446,1.4283103,0.7880157,-0.18108189,0.5345885,-1.1689167,-0.36859676,-0.62978935,-0.47807607,0.42089278,-0.36029825,1.0094149,-1.0346123,-0.5067969,0.84935373,-0.039258778,1.0416512,0.6577271,-0.43865618,-0.38451523,0.23037173,-0.75562835,-0.03278684,-0.64488876,-0.5129983,0.12079744,0.21120746,-0.56166077,0.16540575,-0.36521906,-0.49951085,0.03852579,0.57532537,0.8569973,0.80676603,0.8291558,1.0780059,0.11860524,0.6824616,-0.24145637,1.1212984,0.4639759,-0.67693025,-0.46349722,-0.50112003,0.8157456,1.0175506,-0.30160147,0.49318674,-0.18683665,0.95296204,-0.1515181,-0.805844,-0.7886352,0.20515771,1.3751544,-0.24911465,0.7361363,-0.2436681,-0.20185135,0.19097522,0.7942652,0.23705444,-0.35543594,0.15632638,0.6228243,0.52175266,0.043802693,0.1014801,0.2293513,0.47125354,-0.2724782,-0.038289227,-0.013543279,0.09162186,-0.25671142,0.41993546,0.16720669,0.13320062,-0.5072487,0.256662,-0.27855375,0.8288027,0.4304035,-0.1899598,-0.20798813,-1.2570658,1.4111776,0.13172884,-0.5739741,-0.46782622,-0.34715724,0.23117451,-0.07352213,-0.64859587,-1.3651667,-0.8231031,-0.6472909,0.32941425,-0.96461385,-0.4099819,0.2744587,1.0883207,0.25078616,0.51449704,0.6513853,-0.42622104,0.21127482,0.510274,-0.0012145902,0.5606563,-0.05124751,0.12589353,-0.47723335,0.8599392,-0.93557876,1.4269445,0.43082577,0.46351466,-0.10890932,-0.76400024,-0.5534706,0.89518976,0.005572579,1.0836687,-0.31571868,-0.2964557,1.064849,0.83162844,-0.38608563,1.4802226,0.51492316,0.3630728,0.8709936,-0.20625061,-0.17251149,0.30931187,-0.10763045,-0.53718704,0.28822675,0.27240235,0.38415432,1.1431817,-0.79546,0.18834263,-0.48886535,-0.7607957,-0.10611692,-0.4744271,0.66557,1.0265435,-0.21041358,-0.81023407,-0.5862604,0.52272874,-0.40222317,-0.17003228,-0.58846587,0.9669655,0.13200314,0.4005859,0.099300414,0.56213814,1.0631177,-0.8469432,0.2345326,0.045979675,-0.10962476,-0.12192126,-0.21157435,0.4298221,-0.36803967,0.98013836,1.1251954,0.6088513,-0.6640096,-0.29478878,-0.56877714,-1.3317319,-0.3822396,0.40650964,-0.50502217,-0.30103576,-0.10155238,0.28721434,0.1709255,0.11135109,0.028070798,-0.20737706,-0.15863551,-0.043998946,0.94158924,0.32274485,-0.27984866,-0.4916927,0.9783779,-1.0786251,0.7376049,0.69845235,-0.96009797,-0.38016003,-1.3501017,0.7670067,-0.100248,-0.43334395,0.430384,-0.20729114,-0.35692307,0.26008338,-0.12729093,-0.1634277,-0.28669003,-0.3142429,0.8703042,-0.414854,-0.24650028,-0.1653959,0.64863026,0.7468893,1.0232164,0.67869496,0.3150803,0.36564374,1.0170275,0.3966661,0.1332042,0.114623174,0.17472252,0.23130882,0.34713078,-0.26379147,0.47251406,-0.5239378,-0.25315723,-0.08303839,0.24539363,0.61897063,0.047691524,-0.7964387,0.15022367,-0.8382152,-0.4393378,-0.7667108,1.0826529,0.20838286,0.5100953,-0.17015752,-0.18592176,-0.77029514,-0.1657218,0.26066786,-0.06734426,0.13746107,0.7022668,-1.2369677,0.49789566,0.2392414,0.11648481,0.3195131,-0.53257996,-0.005367104,0.23796381,0.34615052,-0.2426958,0.78301656,0.73305285,0.8505125,0.38408056,0.19345503,1.002571,-0.35396197,-1.2218429,-0.29433513,0.30029324,0.22423421,0.054008763,0.09722226,1.4650726,0.1202996,0.7222803,0.9189051,-0.23446132,-1.3491086,0.233641,0.0037463978,0.27368912,-0.18772428,-0.13693677,-0.35524505,-0.074738964,-0.07159651,0.2529342,0.3439228,-0.25179467,0.56814826,0.6372213,0.18617645,-0.56554925,0.38996154,-0.0050312076,-0.22275497,0.31954962,-0.25478745,-0.09426669,-0.68314,-0.12869762,0.31327328,-0.09272492,-0.22408038,0.28020722,-0.047392398,1.2315427,0.65264726,0.06918164,-0.95663196,0.44490373,1.8949244,-0.6301281,-0.43833604,-0.84334797,-0.08840674,0.17100087,0.51340955,-0.7408453,-0.06504974,0.36104012,0.0053614816,-0.38985544,0.16193382,-1.0128927,0.32112786,0.34293598,1.351425,-0.01948574,-0.53127337,0.86470854,-0.9858636,-0.5522139,-1.1256324,-0.82011354,-0.10787378,-0.14793733,-0.043421015,-0.0018558636,0.7676346,0.983062,0.41924593,-0.47291878,0.8943004,1.1758243,0.57459086,-0.13373153,-0.44813308,-0.32474798,-0.7534952,0.61694646,-0.86332273,-1.0372453,-1.464193,1.1129516,0.3916322,0.44899392,-0.6737832,1.0987253,1.6061946,-0.391997,-0.008560853,-0.25957814,-0.20498471,-0.5156119,0.52572167,-0.49683067,0.6612471,-0.39141616,0.3219773,-0.31110293,-0.82524997,0.36317277,1.8092875,0.42953035,0.23876503,0.90315235,0.6490817,-0.0744928,-0.3045936,0.51808625,-0.42396417,1.1624105,0.5010037,0.64646035,0.9035669,-0.14617884,0.05385813,-0.2039041,0.16815166,-0.12275998,0.62500155,0.24206848,-0.118832,-0.26777154,0.26768142,-0.36555675,-0.5176145,-0.028403489,-0.59258,-0.4532101,-0.5095577,0.8368062,0.26235542,-0.2493466,-0.5920065,-0.48401344,-0.19206496,0.5159388,1.1374614,1.6657928,0.4248721,-0.015647436,0.063973874,-0.86834973,-0.4604033,0.91830957,-1.0665392,-0.0012671652,-0.17899475,-0.17544869,0.48665565,-0.7783859,0.52087986,-0.13255523,0.33427623,-0.85292786,0.6648211,-0.91518927,0.016929869,0.49309546,-0.5773162,-0.13306376,-0.09087175,-0.51133966,0.7685791,0.097808056,0.26945254,-0.7265343,1.1430238,-0.27562997,0.09794935,-0.21878645,-0.48429137,0.78199357,-0.34935424,1.0815805,0.49196094,-0.10161632,0.3014254,-0.1456301,-0.46356687,0.45068607,0.5513165,-0.51959115,0.14595896,0.08140361,0.9559539,0.03301671,-0.27455503,0.41883877,-0.22681242,0.33874252,0.31834996,1.0338029,0.21261963,0.53445464,0.7096009,-0.2672343,1.0636861,-0.8192396,-0.2504199,1.2476051,-0.010741533,-0.11782429,1.1751881,0.51389366,-1.1972657,-0.57851446,-0.6432145,0.27561313,-0.16597153,0.5045588,0.017699989,0.49970412,-1.3659467,0.14975357,0.49941722,0.7639973,-0.30778617,0.8794792,-1.238719,-0.6264588,-0.6557597,-0.10667208,0.5579667,0.1552266,0.86421955,-0.429135,-0.7117439,0.3590301,-0.60479957,0.43993348,0.02941963,0.22657482,0.15087232,0.77615446,-0.06413074,1.3219422,-0.3197761,-0.7671829,-0.5666207,-0.14077705,-0.24340647,-0.105476774],\"z\":[0.23448652,0.10057826,0.49075952,0.3411419,0.3822745,0.2040831,0.45383886,0.44891065,0.5146702,0.13028586,0.3862762,0.5056672,0.25307846,0.47276872,0.33327785,0.35845754,0.50161755,-0.10072008,0.45676172,0.2633434,0.41226974,0.41946015,0.24033356,0.2922561,0.29716128,0.10612077,0.484607,0.57135624,0.5444781,0.37597945,-0.031505667,0.2851654,-0.18024229,0.40709728,0.4707473,0.1894204,0.26052096,0.47527528,-0.22345217,0.076305635,0.753592,0.2897531,-0.22249545,0.31785882,0.5337484,0.3469768,0.21545963,0.5891724,0.27312103,0.5851348,0.27103513,0.116529174,0.459571,0.28071463,0.5865901,0.42894915,0.4508034,0.32083932,0.804431,0.4920246,0.47041348,-0.09454134,0.31979585,-0.031507496,0.2522076,0.3468952,0.37419382,0.36807457,-0.15206097,-0.062022988,0.2830386,0.31464273,0.3887271,-0.239074,0.34348944,0.525344,0.015936336,0.93009883,0.45158294,0.4842035,-0.0498355,0.506429,0.6404108,0.06773375,0.27347943,0.5903603,0.1729115,0.23138691,0.3497962,0.5404403,0.43872428,0.004064449,-0.062868506,-0.50080293,-0.07055231,0.91959834,-0.21136233,-0.1049496,0.5548395,0.030258665,-0.44070235,0.35112086,0.3892927,-0.3873955,0.61294216,-0.03168729,0.14671132,0.23202789,-0.29148114,0.06621233,0.01605495,0.8044999,0.40632534,-0.17787011,-0.27536356,-0.13355897,0.33478117,0.35650566,-0.014521255,0.17384326,0.21151252,-0.5227572,0.16097419,0.03874999,0.44576254,-0.071410276,0.02151879,0.6950878,-0.1413816,0.9546585,-0.60586554,0.77177244,0.7509529,0.1443997,-0.045937475,1.1384294,0.5230677,0.4213961,-0.16457783,-0.013420921,0.17567794,1.01553,0.53650385,-0.06828241,0.21622439,-0.51168853,1.0651138,-0.31743926,0.51119685,-0.14279012,-0.48021537,0.0024452137,-0.094800204,-0.2887529,0.8388473,-0.36722007,0.09249059,0.5700739,0.2495118,0.15387884,0.46109426,0.4936275,-0.35574177,0.90459764,0.51662296,-0.58941215,0.53219205,0.70019144,-0.22097617,0.014427773,0.02997011,-0.2187001,-0.32772037,-0.48019332,0.10814947,-0.19326235,-0.8455913,0.6065343,0.023233825,0.699797,0.27288613,0.4048364,0.96564555,0.99287766,0.102926746,0.31249484,0.21700902,0.5623619,-0.37069148,-0.07895859,-0.45859388,-0.2060259,-0.29163045,0.47975555,0.011104838,0.06353878,0.20546085,0.41907093,0.14213394,-0.44145212,0.4804569,1.1030939,0.046682842,0.37752548,0.06634042,-0.06593165,0.013128923,0.40115872,-0.6522441,-0.22246167,0.07919762,0.27118626,0.7543668,0.13759392,-0.2261085,0.09370545,-0.463397,0.5353202,-0.62197185,-0.09765934,-0.26047695,-0.7713375,-0.43248022,0.35614604,0.23199697,0.020322844,0.6730701,0.46992666,0.7429771,0.43232712,1.1595179,0.5031618,0.36297578,-0.35667717,-0.81082284,-0.21495451,-0.08924887,-0.0073614423,-0.4996908,-0.6583365,-0.41995466,0.05399921,0.64276475,0.10681477,0.7888836,-0.26672772,0.18413201,0.22337094,-0.19909205,0.32457376,0.059919212,-0.32260537,-0.5235265,-0.10424957,0.05218642,0.3929459,1.0104858,-0.31795555,-0.08325889,-0.12992087,0.44468385,0.19343188,0.3824654,0.08754195,-0.16218676,0.27206787,-0.03941072,-0.3927758,0.08709138,-0.014055321,-0.6694609,-0.11918078,-0.002911692,0.9883995,0.23849642,0.3960964,0.015939342,-0.77069867,0.3458318,0.093242876,0.51068896,-0.05458801,0.4561057,0.16461156,0.4818901,-0.12740946,0.45130607,0.22351664,-0.5719182,-0.09147259,0.3934965,0.4497913,-0.34880945,0.14244133,-0.09753194,0.35688877,0.077769876,-0.4484411,0.3755954,-0.63725275,-0.24359222,-0.7543559,-0.21126044,0.8676791,0.06363015,0.5091254,-0.46151748,0.16151753,-0.67845,-0.5485982,0.29467222,-0.7599996,-0.11453918,0.25611714,-0.6967915,0.31783658,-0.54460394,0.5039523,-0.98645097,0.08210596,-0.9360512,-0.22972845,-0.41564587,0.27228448,-0.39514562,0.21467659,0.094502605,1.0087869,0.30056873,0.6102188,0.32421938,-0.6321738,0.20708823,0.1395909,-0.92294407,-0.5421705,-0.7798637,1.1807288,0.15654832,-0.5380619,0.8380548,-0.47971264,-0.7154069,0.13261226,-0.024234885,0.6626516,-0.6057518,0.07431242,0.17323455,-0.6956278,0.15386376,0.17470965,0.621229,-0.09505223,0.57312554,-0.27063775,-0.39825162,-0.57488513,-0.514427,-0.76362854,0.9312159,-0.040952995,-0.037486445,-0.26995277,-0.4244358,-0.4782059,1.6007704,-0.10107838,0.08025761,-0.6589034,0.59963065,1.0827235,-0.1900493,1.1327772,0.4393217,-0.5971939,0.07678874,-0.18595624,-0.40079638,-0.71380913,-0.71650517,-0.46622285,0.19536729,-0.3145882,0.2658365,0.89846635,-0.30811104,-1.0037175,0.47869605,-1.5705677,-0.07875289,-0.25025344,-0.25676212,-0.14821847,0.40313432,-0.37007698,0.850589,0.3948526,0.18611315,-0.7289685,-1.1202385,-0.2315105,-0.98971987,-0.3819056,0.7271971,-0.42398718,0.39369166,0.052335918,0.7332499,0.12590948,-0.09169865,-0.24904399,0.6036059,0.27778116,-0.30270892,0.4959137,-0.16960043,-0.16898198,0.052455258,-0.58208895,0.17847952,-0.5632519,-0.06913691,-0.4464554,-0.1845608,0.66349465,-0.20233887,0.23594001,0.47774702,-0.6390326,0.654708,0.38629043,0.055876683,-0.1026462,0.78979224,-0.64147115,-0.6347043,-0.2323007,0.33546174,0.43917486,0.4731716,-0.58269894,0.9134357,0.24267843,0.4874348,0.41217428,0.43309724,-0.37475616,-0.7692229,-0.39194587,-0.77876836,0.48849285,-0.12366733,-0.4058303,-0.31352818,0.40888715,-0.32370678,-0.028107282,-0.27028072,-0.6125515,-0.59026945,-0.3699611,-1.1378924,0.010490511,-0.032632176,-0.7635953,-0.21714324,0.3501896,-0.68921924,-0.55929005,-0.19223091,0.17333168,-0.18875848,-0.394586,0.16346173,0.24967334,0.20490465,-0.52993685,0.8745986,-0.5526552,-0.30576766,0.34223634,0.0755858,-0.48962405,0.8320325,-0.25222427,-0.82931006,-1.0576869,-0.53210604,-0.3920609,-0.33079302,0.5171231,0.40189803,-0.3611639,0.20858474,0.3480461,-0.5512669,-0.29482678,-0.910908,0.64357215,0.5961004,-0.09272526,-0.64150643,1.053804,-0.14062828,0.23039344,0.6944748,-0.27277535,0.43506813,-0.016511494,0.10348371,-0.0060108555,0.13846788,-0.27748463,-0.4439468,0.09182765,-0.03641304,-0.5627103,-0.40135944,0.52026874,0.42025247,0.16921566,0.37378025,-0.7153285,0.049989987,0.06720436,0.05647167,0.72657305,0.071781665,0.18590143,-0.47399008,0.32168117,-0.25796053,-0.6212649,-0.89678574,0.5033098,-0.0765611,0.1540096,-0.27556184,-0.4020996,-0.118407294,-0.03034494,-0.6128398,-0.82651615,0.5137554,-0.27735588,0.13953955,-0.47593677,0.7868047,0.60917354,0.057400446,-0.18840326,0.5740276,0.54354036,-0.15282291,-0.039745502,-0.404991,0.62131923,0.09788264,-0.34627184,0.6671424,0.1569881,0.521649,-0.2259778,0.7236406,-0.39440647,1.2874445,-0.60774857,-0.5686081,0.48485497,-0.18171105,-0.2892738,-0.2792489,-0.5831921,0.26391938,0.96013767,-0.45300695,0.5567664,-0.05704846,0.11929941,-0.23773894,-1.0958449,0.75988156,0.7644439,-0.5562107,-0.5068708,0.05253329,0.044213846,0.010780286,0.6386694,0.308142,-0.67120624,-1.0285692,-0.17845358,-0.8280428,0.24531938,0.5395118,0.26961857,0.4224306,-0.6059696,0.0069295117,1.0801263,0.46542296,0.031361043,1.4940413,0.23414996,0.19353457,0.65995896,-0.44053394,-0.9394718,-0.48251027,0.34671903,-0.053749517,-0.91580296,0.9933039,-0.51156646,-0.15055922,0.19632897,0.055324104,0.18685998,0.034788277,0.5847736,-0.64466244,0.14790733,-0.46794143,-0.021381926,0.18886347,0.73775643,-1.1472259,0.5059132,-0.3746707,0.08123575,0.04572048,-0.04940799,-0.4496079,0.35013336,-0.04413011,0.8997232,0.5583336,-0.7316633,-0.17119282,0.34792468,-0.4218859,-0.49862134,0.5520012,-0.91348463,-0.122118615,0.4050508,-0.08206295,-0.33315045,-0.4265932,-0.6672266,-0.3832735,0.32669342,-0.42347008,0.03872821,0.8024695,-0.6392657,0.103591666,0.4978895,0.15249404,-0.6324145,-0.18679287,-0.2523137,-0.24556363,0.8583665,-0.6306442,-0.82983834,-0.03278089,0.013956323,0.78531945,-0.3829482,-0.15058431,-0.60408175,-0.693262,0.08188265,0.27106065,0.23978461,1.0889169,-0.34339774,0.10736037,0.03474372,0.5208288,-0.08699001,1.044922,0.17892186,0.73024,1.1927979,0.39680263,-0.45975968,-0.22850075,0.2595488,-0.10714152,-0.35189146,-0.41926265,0.63039523,-0.50969386,-0.22116947,-0.04026866,1.6706933,-0.36417738,0.5998643,-0.693418,-0.2887887,-0.97788835,-0.5051514,-0.40588745,-0.6239011,0.22857207,0.014000037,0.74287677,0.2676132,-0.059624564,0.1627645,-0.77893233,-1.0387082,0.18400243,0.70110047,-0.5433322,0.36848035,-0.7614233,-0.38825804,-0.51838505,-0.5456274,-0.5563206,-0.46136978,0.29061943,-1.0669703,-0.18748531,0.5478267,-0.6344717,0.5452181,1.0375603,0.12871158,-0.2627799,-0.2575507,0.3464865,0.84933233,-0.2955009,-0.9195007,1.088968,-0.2781429,-0.58199203,0.3594061,-0.9060121,-0.04302965,0.52872396,-0.8622026,-0.49957603,-0.9702269,0.5025492,0.8239124,-0.1737347,0.40567294,0.2682161,0.17272481,-0.1241395,-0.0018775027,-0.3335889,0.065161064,-0.3339518,0.57837486,-0.68782556,-0.44253674,0.7213305,0.44727117,-0.29690504,0.62391317,-0.96612453,-1.1096057,-0.2797592,-0.69559133,-0.25445628,0.2701496,-0.57714605,1.1272215,-0.30769816,0.9633905,-0.4049734,0.13705638,1.002687,-0.06259056,-0.80427915,-0.10706471,0.7721977,-0.88331413,-0.9101412,-0.62005687,-0.4076786,-0.36151832,0.9722637,-0.73462796,0.43531126,0.15418828,-0.46070883,-0.0075614527,-0.6123905,-0.36721,0.6264614,-0.95792556,-0.18857187,0.0851416,-0.17001939,1.0915692,-0.069888964,0.593359,1.2348541,0.4593433,-0.44019902,0.18945664,0.19967134,-0.16756253,-0.23013398,0.08768116,0.9543668,0.8222058,0.08859364,0.14972636,-0.16638729,-0.99111056,-0.5202637,0.6286992,-0.2582299,-0.14950907,0.6068721,-0.431133,0.37406862,0.4561824,-0.9571437,-1.1347708,-0.20146553,-0.046689488,0.1311082,-0.2510093,0.630031,0.71043974,-0.031317584,-0.20555612,0.29750666,0.53702533,0.14922763,-1.057155,-0.01620122,-0.5136391,0.036835607,0.45611817,-0.7798095,-0.5810457,0.60092115,0.23896918,-0.011402196,-0.55009574,-0.13051246,1.0920041,-0.66901743,0.13392855,-0.2605157,0.73179847,0.30053422,-1.0530301,-1.0244395,0.5870279,-0.7852842,0.11263785,-0.38504243,-1.2303106,0.9210154,1.0694317,-0.58081794,-0.18425098,-0.30493072,0.33991534,-0.11844306,-0.24374828,0.5467407,-0.38636142,0.3431228,0.8353728,0.84029615,-0.55008566,0.4431572,0.8553644,0.031704277,-0.38188198,1.270317,-0.055258654,-0.35401276,0.45659176,-1.445326,-1.1822981,-0.1880811,-0.17477229,0.7370593,0.0418522,0.041229703,-0.4315273,-0.39017695,-0.2931093,-0.27627873,-0.068735324,1.1551647,0.7907898,-0.7675218,-0.40635583,-0.11873579,0.8907624,-1.0733413,-0.15558074,0.30177456,-0.45187542,0.34041753,-1.0412672,-0.5527341,1.564506,0.70731467,-0.3567843,0.05406198,-0.4055597,0.39186096,-0.9669662,0.51803195,0.58737147,1.2297292,-0.47858796,-0.7280734,0.46143097,0.3585336,-0.3785464,-0.18766855,-0.1359828,-0.58467007,0.8901574,-0.21466143,1.1611491,0.28633833,0.9497384,0.45039472,-0.135803,-0.31035087,0.09956016,-0.06688954,-0.7704804,0.7149769,-0.38691613,-1.0892422,0.9578871,0.84905887,0.075519145,-0.4212324,-0.4718295,0.52814335,-0.059483796,-0.80973524,-0.6307942,-1.059364,0.88521975,-0.48014164,-0.0015729895,-0.57065314,0.4698972,-0.12635262,-0.5751894,-0.18874389,0.46462685,0.6059315,-0.65948457,0.26744944,-0.058078736,-0.16104451,-0.68384403,-0.023069823,-0.8124654,0.65451574,-0.34678283,0.107937776,-1.059975,0.48279864,0.6586019,-0.42370656,-0.039416097,-0.11485028,-0.03994661,0.12454954,0.28456914,-0.23391788,0.063425265,-1.3885609,0.28459078,-0.07210527,-0.7392845,0.5957562,0.9739269,0.45278394,-0.061328348,-0.15046923,-0.6011707,-0.5758156,0.24358304,-0.522161,-0.7049371,-0.8414726,-0.101495765,0.32087228,0.008284441,1.2046758,0.07811165,-0.23625384,1.02109,0.2534623,-0.6011258,1.2391696,-0.30131486,0.21674049,-0.21596976,-0.7769314,0.95358264,-0.8324734,-0.4309471,0.4173791,0.21904889,-0.15124473,-0.21919736,-0.26310706,-0.35634285,-0.32918382,0.017993087,0.1927281,-0.5028522,-0.052838083,-0.25002354,-0.05813871,0.02746158,-1.775805,-0.69351476,-0.46320495,0.093263194,-0.08445303,0.5765861,-0.50033957,0.14356722,-0.75112355,-0.6053908,-1.468963,0.4769427,-0.10363456,-0.1948005,-0.7832625,0.06645925,1.3713716,0.114003755,0.29006425,-0.6909846,-0.6972236,-0.2817608,-0.105941564,0.880031,-0.18035537,0.22973864,-0.7821334,-0.16760533,-0.5823449,-0.2043611,0.87821376,-0.60227245,1.0354416,-0.2568733,-0.0870054,1.3331379,-0.81208724,0.3100584,0.6961687,0.54548746,0.21996874,0.24253505,-0.06615033,0.0983764,0.13796426,-0.09250435,0.63411176,0.7404625,0.66657835,0.4605278,0.07564397,0.5734742,1.1951997,-0.0477941,1.3917148,0.9271176,-0.5057829,0.62185997,0.9482497,0.71192455,0.22740725,-0.85793686,-0.057653617,0.05282249,0.7731504,0.641642,1.2666669,-0.20039201,0.026661905,-1.1040921,0.7573219,-0.33726248,0.31115872,1.1598767,0.028855303,0.51277786,-1.3354385,-0.9671278,0.64702296,0.011606554,-0.85651416,-0.43179178,-0.40691772,-1.8371586,0.05827065,0.26235548,-0.74760497,1.0839672,-0.7292708,-0.2451082,-0.46389073,-0.47777423,-0.7554834,-0.17065996,0.14712992,0.14861575,-0.54849416,0.5482452,0.528775,-0.488759,0.096375525,0.79090583,0.3622586,-0.19594939,0.14584929,0.5537161,0.25304964,1.3511614,-0.11268959,0.03574162,-1.2434757,-0.55562335,-0.014593225,0.36364022,-0.70844716,1.0884072,0.43779778,-0.95665413,-0.31950715,0.04269087,1.0342829,0.3390082,-0.6492459,0.022252435,-0.23489998,-0.26978412,-0.30668315,-0.5923295,0.78871393,-0.4250728,0.24448462,-0.78563493,-0.6152293,-0.41964573,0.60717773,0.42446426,-0.40202013,-0.35791355,0.8464866,0.6883614,-0.82902527,0.75439954,-0.13365711,-0.37835136,-0.8012038,-0.54940313,0.2125698,0.033111423,-0.5564803,0.2829691,-0.8322913,-0.23803692,0.17018983,0.2960597,0.18725637,0.9894923,0.17024823,0.5052461,1.2341067,-0.34842408,1.3977629,-0.16553116,1.1647649,-0.6003911,0.5369142,-1.1605574,0.16740718,-0.24560317,1.0915662,-0.5760942,0.7602423,0.20495877,0.07961764,1.0787494,0.64526933,0.573497,0.41557935,-1.1014216,-0.40392327,-1.4976047,-0.08533175,-0.2598278,-0.6764991,-0.5051965,-0.4536185,-0.4212787,-0.46122804,-1.2465571,0.10351102,-0.38386872,0.39626458,0.38899875,-0.3309095,0.38316447,-0.3572557,0.10990036,-0.6525977,-0.16221361,0.2682724,-0.12374415,-0.21043226,0.4407539,-0.63297707,-0.06492762,-0.7579875,-0.44900623,-0.95340616,-0.20557098,-0.5491053,-0.786106,0.6554751,0.3364991,-0.32517987,0.19657797,-0.9303726,1.0017829,0.41613665,1.1330345,0.046118755,-0.93056357,-0.58522457,0.56168365,1.0226355,-0.53478086,-0.43554762,-0.53489447,-1.0349318,0.31125513,0.36051866,-0.07805379,0.29759654,-0.34189698,0.22735693,0.18619289,0.04734868,0.10819906,0.4827793,-0.434739,-0.40922174,0.32449377,-0.042516842,0.074678026,-0.4863781,0.28704578,-1.1025591,-1.3620143,0.36993504,-0.3941634,-0.39511123,-0.5795463,1.0234538,-0.3933505,-0.47155645,0.05965132,-0.9180902,0.0409133,-0.11914659,-0.37082532,0.19985533,-0.22816144,-0.24981886,-1.1345286,-0.35375264,-0.24773312,0.5879525,0.83733433,-0.4068802,-1.1052835,-0.42359522,-1.5060214,-0.17779839,-0.6047906,0.30821115,-0.79049605,1.1606302,0.021241404,-0.41225708,-0.47752145,0.17271282,-0.026663564,1.2360778,-0.5550867,0.6135805,-0.6916822,-0.22128141,-0.5943155,-0.5435402,-0.78869945,-1.0168147,0.37490714,0.43753994,-0.37188923,-0.27427402,0.6885035,0.5191946,0.3112576,0.12490491,-1.1839621,-0.6100425,0.1861085,-0.15552644,-0.20413327,-0.1088011,-0.90963286,-0.36919427,-0.16991875,-1.2765591,0.082641944,0.24091768,0.12699838,-0.5805664,0.44534004,0.23358409,-1.4039005,-0.07213706,1.4217944,1.4465802,-0.6143056,-0.0858166,0.9840431,-0.5882118,1.0745393,0.21970484,-0.93436605,1.3755257,-1.2327304,-0.113622986,-0.6739392,2.100594,-1.0592034,-0.41091722,-0.13074943,-0.26114863,-0.13646275,0.24702464,-0.96907187,0.41174027,-1.5134308,-0.25286645,-0.052796386,-0.4548613,0.80025417,0.06979343,-0.28479794,-0.8828443,-0.38958162,0.10318249,0.27519196,-0.386087,0.7187254,-0.7650545,-0.08845745,-0.41523075,0.68808436,-0.11118319,0.46497124,-0.8653375,1.1350261,0.8518241,0.093734525,-0.03331918,-0.57358927,0.49860796,1.7237592,0.38933918,-0.054797806,0.6867915,-0.44399256,-0.118173495,0.5340446,0.22273396,1.179089,-0.43851092,-0.50743765,0.7834416,-0.660873,-0.84706247,0.06128935,-0.34354112,-1.0958246,0.17577918,-1.214252,-0.014949502,-0.21987467,-0.2548639,0.57408655,-0.40742406,-1.1430554,0.26448983,-1.2577509,-0.22312787,1.0562543,-0.804215,0.07017,-0.6241197,0.24209273,0.591127,0.68892914,0.25493968,-0.72084016,0.6349672,-0.6274961,1.11228,0.5284579,0.7733964,0.030514667,0.4044696,-0.117816254,0.73668826,0.65012574,-0.39835352,1.9422923,-0.09795305,-0.16419205,0.16200241,0.35784543,-1.4789735,0.6203968,-0.6371061,-0.90614736,0.4088012,-0.14595371,-0.30326515,0.28243232,-0.42867675,-0.6276877,-0.6875354,0.65182877,-0.675541,-0.9035696,-0.13483794,-0.07809946,-0.47885993,-0.18048592,0.13360876,0.21918446,0.57393634,0.34349155,-0.5357805,-0.68658113,-0.7239635,0.14591001,-0.1620194,-0.8362483,-0.4184269,1.0000553,-0.5532551,0.9735037,-1.060979,0.6802265,-0.09316732,0.8135362,-0.48169613,0.018783819,-0.45153496,-0.2914024,0.9226036,0.18040384,0.34630674,0.24796976,0.23883952,-0.110634424,-0.035207435,-0.07713942,-0.5334796,-0.59444374,-0.47103104,-0.08105344,-0.78105664,0.30171528,-0.2544262,0.12748134,-1.1116241,-0.79260296,-0.46009624,0.050723236,0.3132003,1.5535,-0.7084355,-0.43508735,1.001733,0.35899016,-1.4474311,0.8403908,1.3886164,0.6186813,-0.46097055,-0.41408992,-0.39407903,-0.4741524,0.40133166,-0.13488542,0.0828559,-0.29413784,-0.24869311,0.38763636,0.6510698,0.72727716,-0.53971726,-0.5107433,-0.3647266,-0.12597781,-0.6951898,-0.603531,-0.0070300517,-0.5583546,0.5213876,0.14264195,0.051529195,0.02221865,-0.59509176,-0.5973966,0.51200783,-0.30342722,0.008455626,0.9208072,1.2183644,-0.7420767,-0.2266013,0.96399415,0.29375365,-0.49791628,-0.22126752,-0.3311165,-0.9975886,0.46451584,0.86469936,-0.76936424,0.7444892,-0.7407366,-0.4773208,0.019487597,-0.43968832,0.057190433,0.40072238,-0.4773501,0.47092986,-0.29586342,-0.011452364,-0.19216886,0.252651,-0.6602563,-0.24833904,-0.5843835,0.0141586205,0.5690493,-0.88026214,-0.40029937,-0.22916058,0.38761443,0.2864569,0.08970055,0.3358515,-0.16092992,-0.28083783,-0.15841998,0.17115113,0.014486644,0.16607544,-0.775914,0.7530488,-0.44410732,-0.08874834,0.7680694,0.43446139,0.11658433,-1.0842932,-1.0086147,-0.8634741,-0.24849282,-0.7264026,-0.5070514,-1.8372293,0.9021509,-0.33394632,-0.09735858,0.28877768,-0.6400247,0.061002113,-0.62057465,-0.2415972,1.1667262,0.9217238,-0.271066,0.17696634,-0.5014989,-0.27627766,-0.9593293,-0.089298695,0.47465283,0.6238028,0.58651227,-0.29960293,0.7219853,-0.08603131,0.73690337,-0.88747114,-0.9725124,-0.7378165,-0.42641765,-0.4836938,-0.3587096,-0.34875196,-0.17583346,0.81908876,-0.9763339,0.83873487,-0.17355159,0.02095958,-0.40549228,1.0946414,0.42072374,-0.41306323,-0.310732,0.23120818,0.10256014,1.1614739,0.027597388,0.9050323,0.067536406,0.41197705,-0.986822,0.4082369,-0.038392417,-0.7026014,-1.1738778,-0.2529937,-1.0253966,-0.50472534,-0.8030509,-0.8453925,0.747103,-0.19618325,0.19515091,-0.4645023,-0.18982205,-0.5339951,1.0409962,0.89400595,-0.442016,-0.025794556,0.29790345,-0.4648149,-0.28262156,-1.1241033,-1.2836857,-0.54096305,0.6960331,-0.05999334,0.8812789,0.8903664,0.23329355,0.28519434,0.1956238,-0.386403,0.16145033,-0.6323296,0.5382359,0.85682774,0.28858042,0.31965837,-0.59891003,0.1958649,-0.35463488,0.11146797,0.18331113,-0.47619095,0.586274,1.0389735,0.9426378,-0.57543993,0.83066684,-0.3193553,0.43325737,-0.7375446,-0.4899823,-0.18548927,0.94179344,-0.76950467,-0.6954461,-0.36014095,0.54050386,-0.0016724671,-0.48747796,1.2689059,0.4448158,-0.22148605,0.096320435,-0.7807582,-0.60361195,0.6066439,0.061573707,-0.43606606,0.44604945,0.8501738,-1.0567199,-0.6405343,-0.48526627,-1.5153894,-0.23656064,0.6935855,-0.07720007,0.53495395,0.65728015,-0.7182458,0.34449527,1.0936121,0.018862514,0.06961155,-0.16802213,-0.0778872,0.7165006,0.11507179,0.58090365,0.023330973,1.1607062,-0.38683414,0.34307042,0.88856375,0.62002075,0.51876277,0.55409074,0.5516145,-1.4842945,-0.09572013,1.6764876,0.52171695,-0.48282963,-0.62488234,-0.23148103,-0.2008064,0.5860511,1.0954586,0.5482082,0.6254594,-0.47048488,0.5950551,0.37756222,0.27904087,0.18777327,0.6309093,0.16256684,-0.2799776,0.73541975,-0.1428469,-0.6831793,1.1950072,0.15550634,-0.9364179,-0.76185805,0.47521093,0.6374406,-0.26895133,-0.90440744,-0.17121996,0.41791248,-0.14810324,0.2294027,0.12619305,0.62515056,-0.4599637,-0.9154988,-0.7534317,0.28710398,-0.860141,-0.21090938,0.3529652,1.1802816,-0.8445872,0.13314311,0.37619197,-0.9020935,0.54551715,-0.0049340245,-0.93335325,-0.46321896,-0.38786468,-0.49228004,-0.1652642,0.23587692,0.9319698,-1.3571433,1.108489,-0.20108865,0.34912315,0.13522476,0.66542447,-0.1647381,0.35922384,0.28062764,0.050658986,0.17391223,0.71469295,-0.23607947,1.1099885,0.48107672,0.91642094,0.58927524,1.0961308,-0.14357366,-0.42167565,-1.1560993,0.25670856,-0.33701265,0.55027795,0.2983107,-0.1768568,0.21361123,-0.8014399,1.0229021,-0.421209,-1.2791451,-0.29135463,-0.09520908,-0.5009643,0.25651497,0.4902825,0.5034125,-0.048780426,-1.0436987,-0.12353537,0.1870885,1.0796806,-0.2876136,-0.9289323,-0.7355872,-0.33472934,-0.35896844,0.51906645,-0.41317555,-1.0018281,-0.2373095,-0.3459896,-0.4900245,1.1185626,0.92898583,-0.02320028,-0.9543642,0.5683534,0.60246646,0.9602702,-0.21654633,1.088783,-0.666565,0.3940688,1.1403738,-0.5638776,0.5532858,0.04952429,0.58464116,-0.39335772,-0.9387287,0.9025143,0.6942808,0.13252935,-0.52813524,0.15621677,0.2628589,-0.32565418,-0.31854802,0.8734826,0.74412525,-0.9990586,0.51830214,0.51252705,0.9258896,0.6537477,0.1601172,0.2600418,-0.7491392,0.29535466,-0.37308076,-0.74089533,-0.30659688,0.50604045,-0.4248944,0.67617995,-0.25712165,-0.69976074,-0.673628,-0.6472891,-1.1695164,0.53841,-0.45221034,-0.69495565,0.4260889,0.4220036,-1.002749,-0.08904584,0.6300484,0.064756736,-0.81090736,-0.33454692,-0.5244575,0.8686089,-0.10714109,0.18240505,0.27529362,-0.0023718528,-0.3387567,-0.41508818,1.0825757,-0.26564887,0.36408186,0.88600683,-1.1866164,0.9413384,-0.08818377,-0.25603104,-0.056625694,0.35823768,-0.56576854,0.6485893,0.28146186,0.6839836,0.27929953,0.50693864,0.4574141,-1.1101273,-0.6276787,0.6507985,0.25424322,0.28338155,0.4289548,-0.41071495,0.24412659,-0.18231523,0.27495506,0.24799104,-0.5642143,-1.1588328,0.13576573,0.23932722,0.14473028,-0.9386372,-0.13481727,0.28078428,-0.8073133,-1.0977609,-0.5679479,-0.13303967,-0.33563063,0.10925429,-0.8966361,0.28368604,0.53652257,0.8669887,-0.13137567,0.55278456,-0.17110015,0.27244914,-0.15653083,0.44534793,-1.1444571,-0.18541797,-1.2941517,-0.28605905,-0.7400057,-0.22378387,-0.40728554,-0.16678688,0.37116596,-0.3795143,0.021011267,-0.25484955,0.38229668,0.18578778,0.16665463,0.08697689],\"type\":\"scatter3d\",\"textposition\":\"top center\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"zaxis\":{\"title\":{\"text\":\"z\"}}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Word Embeddings (3D PCA)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e2d1999f-1f03-4ac0-a2d9-c97fb4ec78f8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}