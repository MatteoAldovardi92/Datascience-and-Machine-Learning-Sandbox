{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZ/P1GMEG0iVYKjJ59p9OY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoAldovardi92/Datascience-and-Machine-Learning-Sandbox/blob/main/ContinuousBagOfWords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMBD keras dataset\n"
      ],
      "metadata": {
        "id": "nekxgpqu44Fd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P204DBkQz91y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import re # For more robust tokenization\n",
        "import matplotlib.pyplot as plt # For visualization\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "# --- Preprocessing Functions (from previous response) ---\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and tokenizes raw text.\n",
        "    Converts to lowercase, removes most special characters, and splits into words.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    # Remove characters that are not letters, numbers, spaces, or selected punctuation (', -, .)\n",
        "    text = re.sub(r'[^a-z0-9\\s\\'-.]', '', text)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def build_vocabulary(tokens, min_freq=5):\n",
        "    \"\"\"\n",
        "    Builds a word-to-ID and ID-to-word mapping, filtering by minimum frequency.\n",
        "    Adds special tokens for padding and unknown words.\n",
        "    \"\"\"\n",
        "    word_counts = defaultdict(int)\n",
        "    for word in tokens:\n",
        "        word_counts[word] += 1\n",
        "\n",
        "    # Filter out words that appear less than min_freq times\n",
        "    filtered_vocab_items = [item for item in word_counts.items() if item[1] >= min_freq]\n",
        "    # Sort by frequency for consistent ID assignment\n",
        "    sorted_vocab = sorted(filtered_vocab_items, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    word_to_id = {'<PAD>': 0, '<UNK>': 1} # Initialize with special tokens\n",
        "    id_to_word = {0: '<PAD>', 1: '<UNK>'}\n",
        "\n",
        "    # Assign IDs to words based on sorted frequency\n",
        "    for word, _ in sorted_vocab:\n",
        "        if word not in word_to_id: # Ensure special tokens aren't overwritten\n",
        "            word_to_id[word] = len(word_to_id)\n",
        "            id_to_word[len(id_to_word)] = word\n",
        "\n",
        "    vocab_size = len(word_to_id)\n",
        "    return word_to_id, id_to_word, vocab_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Dowload tinyshakespeare.txt\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "file_path = 'tinyshakespeare.txt'\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(response.text)\n",
        "    print(f\"Downloaded '{url}' to '{file_path}'\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading the file: {e}\")\n",
        "except IOError as e:\n",
        "    print(f\"Error writing the file '{file_path}': {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJxF8wFx1lH-",
        "outputId": "830c4083-e1b0-48ff-b06e-56a10412de98"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt' to 'tinyshakespeare.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "file_path = 'tinyshakespeare.txt'\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_text = f.read()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '{file_path}' not found. Please download it from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
        "    exit()\n",
        "\n",
        "print(\"--- Step 1: Initial Text Loading ---\")\n",
        "print(f\"First 500 characters of raw text:\\n{raw_text[:500]}...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0I2bKmb0p_N",
        "outputId": "a5ad17c5-17c1-4eb9-ea90-78e656756e50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Initial Text Loading ---\n",
            "First 500 characters of raw text:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Perform tokenization\n",
        "tokens = preprocess_text(raw_text)\n",
        "\n",
        "print(\"--- Step 2: Preprocessing and Tokenization ---\")\n",
        "print(f\"Total tokens after preprocessing: {len(tokens)}\")\n",
        "print(f\"First 20 tokens:\\n{tokens[:20]}\\n\")\n",
        "print(f\"Last 20 tokens:\\n{tokens[-20:]}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IlWs_T-e1V76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26964fb5-0a92-4b34-db63-b296822cac68"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 2: Preprocessing and Tokenization ---\n",
            "Total tokens after preprocessing: 202649\n",
            "First 20 tokens:\n",
            "['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.', 'all', 'speak,', 'speak.', 'first', 'citizen', 'you', 'are', 'all', 'resolved', 'rather']\n",
            "\n",
            "Last 20 tokens:\n",
            "['moving,', 'and', 'yet', 'so', 'fast', 'asleep.', 'antonio', 'noble', 'sebastian,', 'thou', \"let'st\", 'thy', 'fortune', 'sleep--die,', 'rather', \"wink'st\", 'whiles', 'thou', 'art', 'waking.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary\n",
        "min_word_frequency = 10 # Words appearing less than 10 times will be <UNK>\n",
        "word_to_id, id_to_word, vocab_size = build_vocabulary(tokens, min_freq=min_word_frequency)\n",
        "\n",
        "print(\"--- Step 3: Vocabulary Construction ---\")\n",
        "print(f\"Vocabulary Size (with min_freq={min_word_frequency}): {vocab_size}\")\n",
        "\n",
        "print(\"\\nTop 10 most frequent words (by ID):\")\n",
        "for i in range(2, 12): # Start from 2 to skip <PAD> and <UNK>\n",
        "    if i < vocab_size:\n",
        "        print(f\"ID: {i}, Word: '{id_to_word[i]}'\")\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print(\"\\nBottom 10 words (least frequent words that met min_freq, by ID):\")\n",
        "# Get the last 10 entries from the sorted vocabulary (before special tokens)\n",
        "num_to_show = min(10, vocab_size - 2) # Don't show more than available\n",
        "for i in range(vocab_size - num_to_show, vocab_size):\n",
        "      print(f\"ID: {i}, Word: '{id_to_word[i]}'\")\n",
        "\n",
        "\n",
        "# Convert entire corpus to numerical IDs\n",
        "indexed_corpus = [word_to_id.get(word, word_to_id['<UNK>']) for word in tokens]\n",
        "\n",
        "print(\"\\n--- Step 4: Corpus Indexing (Conversion to Numbers) ---\")\n",
        "print(f\"Length of indexed corpus: {len(indexed_corpus)}\")\n",
        "print(f\"First 20 indexed tokens:\\n{indexed_corpus[:20]}\\n\")\n",
        "print(f\"Last 20 indexed tokens:\\n{indexed_corpus[-20:]}\\n\")\n",
        "\n",
        "# Verify a few translations\n",
        "print(\"--- Step 5: Verification ---\")\n",
        "sample_text = \"the king loves his queen, and the queen loves her king.\"\n",
        "sample_tokens = preprocess_text(sample_text)\n",
        "sample_indexed = [word_to_id.get(word, word_to_id['<UNK>']) for word in sample_tokens]\n",
        "\n",
        "print(f\"Sample text: '{sample_text}'\")\n",
        "print(f\"Sample tokens: {sample_tokens}\")\n",
        "print(f\"Sample indexed: {sample_indexed}\")\n",
        "\n",
        "# Decode back for verification\n",
        "decoded_sample = [id_to_word.get(idx, '<UNK>') for idx in sample_indexed]\n",
        "print(f\"Decoded sample: {decoded_sample}\")\n",
        "\n",
        "# Check for an unknown word\n",
        "unknown_word = \"xyzzy\" # Highly unlikely to be in Shakespeare\n",
        "unknown_id = word_to_id.get(unknown_word, word_to_id['<UNK>'])\n",
        "print(f\"\\nID for unknown word '{unknown_word}': {unknown_id} (which should be {word_to_id['<UNK>']})\")\n",
        "print(f\"Word for ID {word_to_id['<UNK>']}: '{id_to_word[word_to_id['<UNK>']]}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnegaIgv3ArG",
        "outputId": "be44bd0c-a74a-4b3e-887e-57a2a1254783"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 3: Vocabulary Construction ---\n",
            "Vocabulary Size (with min_freq=10): 2001\n",
            "\n",
            "Top 10 most frequent words (by ID):\n",
            "ID: 2, Word: 'the'\n",
            "ID: 3, Word: 'and'\n",
            "ID: 4, Word: 'to'\n",
            "ID: 5, Word: 'i'\n",
            "ID: 6, Word: 'of'\n",
            "ID: 7, Word: 'my'\n",
            "ID: 8, Word: 'a'\n",
            "ID: 9, Word: 'you'\n",
            "ID: 10, Word: 'that'\n",
            "ID: 11, Word: 'in'\n",
            "\n",
            "Bottom 10 words (least frequent words that met min_freq, by ID):\n",
            "ID: 1991, Word: 'houses'\n",
            "ID: 1992, Word: 'clear'\n",
            "ID: 1993, Word: 'bona'\n",
            "ID: 1994, Word: 'instruct'\n",
            "ID: 1995, Word: 'curst'\n",
            "ID: 1996, Word: 'angelo.'\n",
            "ID: 1997, Word: 'claudio,'\n",
            "ID: 1998, Word: 'provost,'\n",
            "ID: 1999, Word: 'lucentio.'\n",
            "ID: 2000, Word: 'alonso'\n",
            "\n",
            "--- Step 4: Corpus Indexing (Conversion to Numbers) ---\n",
            "Length of indexed corpus: 202649\n",
            "First 20 indexed tokens:\n",
            "[86, 250, 143, 33, 1291, 136, 1, 130, 25, 591, 35, 571, 591, 86, 250, 9, 39, 35, 1468, 352]\n",
            "\n",
            "Last 20 indexed tokens:\n",
            "[1, 3, 82, 28, 881, 1, 590, 142, 1, 26, 1, 27, 450, 1, 352, 1, 1139, 26, 132, 1]\n",
            "\n",
            "--- Step 5: Verification ---\n",
            "Sample text: 'the king loves his queen, and the queen loves her king.'\n",
            "Sample tokens: ['the', 'king', 'loves', 'his', 'queen,', 'and', 'the', 'queen', 'loves', 'her', 'king.']\n",
            "Sample indexed: [2, 38, 880, 18, 588, 3, 2, 93, 880, 41, 569]\n",
            "Decoded sample: ['the', 'king', 'loves', 'his', 'queen,', 'and', 'the', 'queen', 'loves', 'her', 'king.']\n",
            "\n",
            "ID for unknown word 'xyzzy': 1 (which should be 1)\n",
            "Word for ID 1: '<UNK>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexed_corpus = torch.tensor(indexed_corpus)\n",
        "window_size = 2"
      ],
      "metadata": {
        "id": "agm8ubgg6XZa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "5BhnU7E344g6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "499cf99a"
      },
      "source": [
        "# Task\n",
        "Generate context-target pairs from \"input.txt\" where the context is the two words before and two words after the target word, excluding the target word. Implement padding for contexts at the beginning and end of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7efe01aa"
      },
      "source": [
        "## Modify context window generation\n",
        "\n",
        "### Subtask:\n",
        "Update the code to generate context-target pairs where the context includes two words before and two words after the target word, excluding the target word itself.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "359fff3d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to generate context-target pairs failed due to an incorrect loop structure and an attempt to drop elements from a tensor in place. This code will iterate through the `indexed_corpus` with the correct bounds to create context-target pairs as specified in the instructions, ensuring the context includes two words before and two words after the target word, excluding the target word itself.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f8df4df",
        "outputId": "13629d3a-0ab7-4af1-88ab-6f89954e7dc7"
      },
      "source": [
        "# Assuming you have generated context_target_pairs in a previous step\n",
        "# If not, you'll need to run the code to generate them first.\n",
        "\n",
        "print(\"\\n--- Step 6: Displaying Sample Context-Target Pairs ---\")\n",
        "# Print a few sample pairs\n",
        "num_samples_to_display = 5\n",
        "\n",
        "for i in range(min(num_samples_to_display, len(context_target_pairs))):\n",
        "    context_ids = context_target_pairs[i][0]\n",
        "    target_id = context_target_pairs[i][1]\n",
        "\n",
        "    context_words = [id_to_word.get(idx.item() if isinstance(idx, torch.Tensor) else idx, '<UNK>') for idx in context_ids]\n",
        "    target_word = id_to_word.get(target_id.item() if isinstance(target_id, torch.Tensor) else target_id, '<UNK>')\n",
        "\n",
        "    print(f\"Pair {i+1}:\")\n",
        "    print(f\"  Context IDs: {context_ids}\")\n",
        "    print(f\"  Context Words: {context_words}\")\n",
        "    print(f\"  Target ID: {target_id}\")\n",
        "    print(f\"  Target Word: {target_word}\\n\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 6: Displaying Sample Context-Target Pairs ---\n",
            "Pair 1:\n",
            "  Context IDs: tensor([  86,  250,   33, 1291])\n",
            "  Context Words: ['first', 'citizen', 'we', 'proceed']\n",
            "  Target ID: 143\n",
            "  Target Word: before\n",
            "\n",
            "Pair 2:\n",
            "  Context IDs: tensor([ 250,  143, 1291,  136])\n",
            "  Context Words: ['citizen', 'before', 'proceed', 'any']\n",
            "  Target ID: 33\n",
            "  Target Word: we\n",
            "\n",
            "Pair 3:\n",
            "  Context IDs: tensor([143,  33, 136,   1])\n",
            "  Context Words: ['before', 'we', 'any', '<UNK>']\n",
            "  Target ID: 1291\n",
            "  Target Word: proceed\n",
            "\n",
            "Pair 4:\n",
            "  Context IDs: tensor([  33, 1291,    1,  130])\n",
            "  Context Words: ['we', 'proceed', '<UNK>', 'hear']\n",
            "  Target ID: 136\n",
            "  Target Word: any\n",
            "\n",
            "Pair 5:\n",
            "  Context IDs: tensor([1291,  136,  130,   25])\n",
            "  Context Words: ['proceed', 'any', 'hear', 'me']\n",
            "  Target ID: 1\n",
            "  Target Word: <UNK>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2c7d9d4",
        "outputId": "fc7f1b4a-db8e-4bc5-e9a5-e8d365a527cf"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --- Hints for Building Your Model ---\n",
        "\n",
        "# 1. Define the model architecture:\n",
        "#    - You'll typically use an Embedding layer to convert word IDs into dense vectors.\n",
        "#    - A few Linear (dense) layers with activation functions (like ReLU) can process the context embeddings.\n",
        "#    - The final layer should be a Linear layer with an output size equal to your vocabulary size.\n",
        "#    - Why vocab_size and not batch_size for the final layer's output?\n",
        "#      # The model's goal is to predict the probability of each possible word in your vocabulary\n",
        "#      # being the target word, given the context.\n",
        "#      # Therefore, the output layer needs to produce a score (or logit) for every word\n",
        "#      # in your vocabulary, representing how likely that word is to be the target.\n",
        "#      # The size of this output is directly tied to the total number of unique words\n",
        "#      # your model knows (the vocabulary size), not the number of examples\n",
        "#      # being processed in a single step (the batch size).\n",
        "#    - A Softmax layer (or combine with the loss function) will convert the final layer's outputs into probabilities over the vocabulary.\n",
        "\n",
        "# Example (replace with your actual model definition):\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, context_window_size):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # linear1 takes the flattened context embeddings.\n",
        "        # The input dimension here is (context_window_size * embedding_dim) because\n",
        "        # for each example in the batch, we have 'context_window_size' words,\n",
        "        # and each word is represented by an 'embedding_dim' vector.\n",
        "        # The batch size is handled implicitly by PyTorch's linear layer;\n",
        "        # it operates on each example in the batch independently.\n",
        "        self.linear1 = nn.Linear(embedding_dim * context_window_size, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context_ids):\n",
        "        # context_ids shape: (batch_size, context_window_size)\n",
        "        embedded = self.embedding(context_ids) # shape: (batch_size, context_window_size, embedding_dim)\n",
        "        # Flatten the embedded context for the linear layers\n",
        "        # The .view() operation preserves the batch dimension implicitly.\n",
        "        # It reshapes each item in the batch from (context_window_size, embedding_dim)\n",
        "        # to a single vector of size (context_window_size * embedding_dim).\n",
        "        flattened_context = embedded.view(embedded.size(0), -1) # shape: (batch_size, context_window_size * embedding_dim)\n",
        "        hidden = self.relu(self.linear1(flattened_context)) # shape: (batch_size, hidden_dim)\n",
        "        output = self.linear2(hidden) # shape: (batch_size, vocab_size)\n",
        "        # Note: Softmax is often included in the loss function (e.g., nn.CrossEntropyLoss)\n",
        "        return output\n",
        "\n",
        "# 2. Instantiate the model:\n",
        "embedding_dim = 100 # Choose an appropriate dimension\n",
        "hidden_dim = 128   # Choose an appropriate dimension\n",
        "# # Remember to define context_window_size based on your context (e.g., 4 for 2 before and 2 after)\n",
        "context_window_size = 4 # Example value, adjust based on your definition\n",
        "model = LanguageModel(vocab_size, embedding_dim, hidden_dim, context_window_size)\n",
        "\n",
        "# --- Set device for CUDA ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# 3. Define the loss function and optimizer:\n",
        "#    - For multi-class classification like predicting the next word, Cross-Entropy Loss is suitable.\n",
        "#    - Adam or SGD are common optimizers.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Choose a learning rate\n",
        "\n",
        "# 4. Prepare data for training (create DataLoaders for batching):\n",
        "#    - You'll need to convert your train_data and test_data lists of tuples into PyTorch Tensors.\n",
        "#    - Use `torch.utils.data.TensorDataset` and `torch.utils.data.DataLoader` to handle batching and shuffling (for training data).\n",
        "train_contexts = torch.stack([pair[0] for pair in train_data])\n",
        "train_targets = torch.tensor([pair[1] for pair in train_data])\n",
        "train_dataset = torch.utils.data.TensorDataset(train_contexts, train_targets)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) # Choose a batch size\n",
        "\n",
        "test_contexts = torch.stack([pair[0] for pair in test_data])\n",
        "test_targets = torch.tensor([pair[1] for pair in test_data])\n",
        "test_dataset = torch.utils.data.TensorDataset(test_contexts, test_targets)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False) # No need to shuffle test data\n",
        "\n",
        "# 5. Training loop:\n",
        "#    - Iterate over epochs.\n",
        "#    - In each epoch, iterate over batches from the train_loader.\n",
        "#    - For each batch:\n",
        "#        - context_batch, target_batch = batch # Get context and target tensors for the batch\n",
        "#        - Zero the gradients: optimizer.zero_grad()\n",
        "#        - Forward pass: outputs = model(context_batch)\n",
        "#        - Calculate loss: loss = criterion(outputs, target_batch)\n",
        "#        - Backward pass: loss.backward()\n",
        "#        - Update weights: optimizer.step()\n",
        "#        - Print loss periodically to monitor training progress.\n",
        "\n",
        "number_of_epochs = 10 # Choose an appropriate number of epochs\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        # Move batch to the chosen device\n",
        "        context_batch, target_batch = batch\n",
        "        context_batch, target_batch = context_batch.to(device), target_batch.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(context_batch)\n",
        "\n",
        "        # Calculate loss\n",
        "        # Use the criterion instance you defined earlier\n",
        "        loss = criterion(outputs, target_batch)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Print loss periodically\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{number_of_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    average_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{number_of_epochs}] Average Training Loss: {average_train_loss:.4f}')\n",
        "\n",
        "\n",
        "# 6. Evaluation loop (after training):\n",
        "#    - Use the test_loader.\n",
        "#    - In each batch:\n",
        "#        - context_batch, target_batch = batch # Get context and target tensors for the batch\n",
        "#        - Forward pass: outputs = model(context_batch)\n",
        "#        - Calculate loss: test_loss = criterion(outputs, target_batch)\n",
        "#        - Calculate accuracy or other relevant metrics.\n",
        "#    - Report the average test loss and metrics.\n",
        "\n",
        "# Remember to adjust the 'context_window_size' in the model definition based on your actual context size (2 words before + 2 words after = 4).\n",
        "# The padding strategy will affect how you handle the input to the embedding layer, potentially requiring masks or special padding tokens."
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch [1/10], Step [100/2534], Loss: 5.6691\n",
            "Epoch [1/10], Step [200/2534], Loss: 5.3364\n",
            "Epoch [1/10], Step [300/2534], Loss: 4.7885\n",
            "Epoch [1/10], Step [400/2534], Loss: 5.3390\n",
            "Epoch [1/10], Step [500/2534], Loss: 4.7881\n",
            "Epoch [1/10], Step [600/2534], Loss: 4.8176\n",
            "Epoch [1/10], Step [700/2534], Loss: 5.0202\n",
            "Epoch [1/10], Step [800/2534], Loss: 5.2031\n",
            "Epoch [1/10], Step [900/2534], Loss: 5.6448\n",
            "Epoch [1/10], Step [1000/2534], Loss: 4.7315\n",
            "Epoch [1/10], Step [1100/2534], Loss: 5.5177\n",
            "Epoch [1/10], Step [1200/2534], Loss: 4.6571\n",
            "Epoch [1/10], Step [1300/2534], Loss: 5.2650\n",
            "Epoch [1/10], Step [1400/2534], Loss: 5.0860\n",
            "Epoch [1/10], Step [1500/2534], Loss: 5.2039\n",
            "Epoch [1/10], Step [1600/2534], Loss: 4.9878\n",
            "Epoch [1/10], Step [1700/2534], Loss: 4.7819\n",
            "Epoch [1/10], Step [1800/2534], Loss: 4.4300\n",
            "Epoch [1/10], Step [1900/2534], Loss: 4.9931\n",
            "Epoch [1/10], Step [2000/2534], Loss: 5.2429\n",
            "Epoch [1/10], Step [2100/2534], Loss: 4.9289\n",
            "Epoch [1/10], Step [2200/2534], Loss: 4.8040\n",
            "Epoch [1/10], Step [2300/2534], Loss: 4.4023\n",
            "Epoch [1/10], Step [2400/2534], Loss: 4.9886\n",
            "Epoch [1/10], Step [2500/2534], Loss: 4.8389\n",
            "Epoch [1/10] Average Training Loss: 4.9979\n",
            "Epoch [2/10], Step [100/2534], Loss: 4.2328\n",
            "Epoch [2/10], Step [200/2534], Loss: 4.2264\n",
            "Epoch [2/10], Step [300/2534], Loss: 4.5671\n",
            "Epoch [2/10], Step [400/2534], Loss: 4.2037\n",
            "Epoch [2/10], Step [500/2534], Loss: 4.6004\n",
            "Epoch [2/10], Step [600/2534], Loss: 4.2039\n",
            "Epoch [2/10], Step [700/2534], Loss: 4.0474\n",
            "Epoch [2/10], Step [800/2534], Loss: 4.4382\n",
            "Epoch [2/10], Step [900/2534], Loss: 4.1503\n",
            "Epoch [2/10], Step [1000/2534], Loss: 4.5223\n",
            "Epoch [2/10], Step [1100/2534], Loss: 4.0702\n",
            "Epoch [2/10], Step [1200/2534], Loss: 4.1949\n",
            "Epoch [2/10], Step [1300/2534], Loss: 4.0827\n",
            "Epoch [2/10], Step [1400/2534], Loss: 4.2921\n",
            "Epoch [2/10], Step [1500/2534], Loss: 3.5671\n",
            "Epoch [2/10], Step [1600/2534], Loss: 4.4443\n",
            "Epoch [2/10], Step [1700/2534], Loss: 4.6216\n",
            "Epoch [2/10], Step [1800/2534], Loss: 4.4166\n",
            "Epoch [2/10], Step [1900/2534], Loss: 4.0788\n",
            "Epoch [2/10], Step [2000/2534], Loss: 4.1637\n",
            "Epoch [2/10], Step [2100/2534], Loss: 4.2316\n",
            "Epoch [2/10], Step [2200/2534], Loss: 4.0483\n",
            "Epoch [2/10], Step [2300/2534], Loss: 4.6146\n",
            "Epoch [2/10], Step [2400/2534], Loss: 4.2834\n",
            "Epoch [2/10], Step [2500/2534], Loss: 4.4402\n",
            "Epoch [2/10] Average Training Loss: 4.2661\n",
            "Epoch [3/10], Step [100/2534], Loss: 3.8708\n",
            "Epoch [3/10], Step [200/2534], Loss: 3.6930\n",
            "Epoch [3/10], Step [300/2534], Loss: 3.4592\n",
            "Epoch [3/10], Step [400/2534], Loss: 3.8923\n",
            "Epoch [3/10], Step [500/2534], Loss: 4.0272\n",
            "Epoch [3/10], Step [600/2534], Loss: 4.0135\n",
            "Epoch [3/10], Step [700/2534], Loss: 3.9951\n",
            "Epoch [3/10], Step [800/2534], Loss: 4.4271\n",
            "Epoch [3/10], Step [900/2534], Loss: 3.8934\n",
            "Epoch [3/10], Step [1000/2534], Loss: 4.3344\n",
            "Epoch [3/10], Step [1100/2534], Loss: 3.8990\n",
            "Epoch [3/10], Step [1200/2534], Loss: 3.6197\n",
            "Epoch [3/10], Step [1300/2534], Loss: 4.3935\n",
            "Epoch [3/10], Step [1400/2534], Loss: 3.5268\n",
            "Epoch [3/10], Step [1500/2534], Loss: 3.4198\n",
            "Epoch [3/10], Step [1600/2534], Loss: 4.1585\n",
            "Epoch [3/10], Step [1700/2534], Loss: 3.8956\n",
            "Epoch [3/10], Step [1800/2534], Loss: 4.0706\n",
            "Epoch [3/10], Step [1900/2534], Loss: 4.3232\n",
            "Epoch [3/10], Step [2000/2534], Loss: 4.2947\n",
            "Epoch [3/10], Step [2100/2534], Loss: 4.6138\n",
            "Epoch [3/10], Step [2200/2534], Loss: 3.9605\n",
            "Epoch [3/10], Step [2300/2534], Loss: 4.0074\n",
            "Epoch [3/10], Step [2400/2534], Loss: 4.0912\n",
            "Epoch [3/10], Step [2500/2534], Loss: 4.0974\n",
            "Epoch [3/10] Average Training Loss: 3.9088\n",
            "Epoch [4/10], Step [100/2534], Loss: 3.3084\n",
            "Epoch [4/10], Step [200/2534], Loss: 3.4479\n",
            "Epoch [4/10], Step [300/2534], Loss: 3.5408\n",
            "Epoch [4/10], Step [400/2534], Loss: 3.4872\n",
            "Epoch [4/10], Step [500/2534], Loss: 3.1542\n",
            "Epoch [4/10], Step [600/2534], Loss: 3.9064\n",
            "Epoch [4/10], Step [700/2534], Loss: 3.1163\n",
            "Epoch [4/10], Step [800/2534], Loss: 3.3084\n",
            "Epoch [4/10], Step [900/2534], Loss: 3.2913\n",
            "Epoch [4/10], Step [1000/2534], Loss: 3.9100\n",
            "Epoch [4/10], Step [1100/2534], Loss: 3.7060\n",
            "Epoch [4/10], Step [1200/2534], Loss: 4.0142\n",
            "Epoch [4/10], Step [1300/2534], Loss: 3.4966\n",
            "Epoch [4/10], Step [1400/2534], Loss: 3.3387\n",
            "Epoch [4/10], Step [1500/2534], Loss: 3.8235\n",
            "Epoch [4/10], Step [1600/2534], Loss: 3.1238\n",
            "Epoch [4/10], Step [1700/2534], Loss: 3.7286\n",
            "Epoch [4/10], Step [1800/2534], Loss: 3.5252\n",
            "Epoch [4/10], Step [1900/2534], Loss: 4.0072\n",
            "Epoch [4/10], Step [2000/2534], Loss: 4.3163\n",
            "Epoch [4/10], Step [2100/2534], Loss: 3.3350\n",
            "Epoch [4/10], Step [2200/2534], Loss: 4.1252\n",
            "Epoch [4/10], Step [2300/2534], Loss: 4.0793\n",
            "Epoch [4/10], Step [2400/2534], Loss: 4.0838\n",
            "Epoch [4/10], Step [2500/2534], Loss: 3.2923\n",
            "Epoch [4/10] Average Training Loss: 3.6605\n",
            "Epoch [5/10], Step [100/2534], Loss: 3.0322\n",
            "Epoch [5/10], Step [200/2534], Loss: 3.4867\n",
            "Epoch [5/10], Step [300/2534], Loss: 3.1024\n",
            "Epoch [5/10], Step [400/2534], Loss: 3.1283\n",
            "Epoch [5/10], Step [500/2534], Loss: 3.0985\n",
            "Epoch [5/10], Step [600/2534], Loss: 3.4763\n",
            "Epoch [5/10], Step [700/2534], Loss: 3.2991\n",
            "Epoch [5/10], Step [800/2534], Loss: 3.2316\n",
            "Epoch [5/10], Step [900/2534], Loss: 3.0516\n",
            "Epoch [5/10], Step [1000/2534], Loss: 3.7167\n",
            "Epoch [5/10], Step [1100/2534], Loss: 3.5761\n",
            "Epoch [5/10], Step [1200/2534], Loss: 3.2788\n",
            "Epoch [5/10], Step [1300/2534], Loss: 3.0416\n",
            "Epoch [5/10], Step [1400/2534], Loss: 3.4205\n",
            "Epoch [5/10], Step [1500/2534], Loss: 3.1851\n",
            "Epoch [5/10], Step [1600/2534], Loss: 3.9658\n",
            "Epoch [5/10], Step [1700/2534], Loss: 3.2322\n",
            "Epoch [5/10], Step [1800/2534], Loss: 3.3575\n",
            "Epoch [5/10], Step [1900/2534], Loss: 3.8156\n",
            "Epoch [5/10], Step [2000/2534], Loss: 3.7370\n",
            "Epoch [5/10], Step [2100/2534], Loss: 3.6652\n",
            "Epoch [5/10], Step [2200/2534], Loss: 3.1109\n",
            "Epoch [5/10], Step [2300/2534], Loss: 3.2765\n",
            "Epoch [5/10], Step [2400/2534], Loss: 3.5148\n",
            "Epoch [5/10], Step [2500/2534], Loss: 3.4188\n",
            "Epoch [5/10] Average Training Loss: 3.4690\n",
            "Epoch [6/10], Step [100/2534], Loss: 3.5697\n",
            "Epoch [6/10], Step [200/2534], Loss: 3.1725\n",
            "Epoch [6/10], Step [300/2534], Loss: 3.4666\n",
            "Epoch [6/10], Step [400/2534], Loss: 3.2156\n",
            "Epoch [6/10], Step [500/2534], Loss: 3.5519\n",
            "Epoch [6/10], Step [600/2534], Loss: 3.2925\n",
            "Epoch [6/10], Step [700/2534], Loss: 2.9080\n",
            "Epoch [6/10], Step [800/2534], Loss: 3.4514\n",
            "Epoch [6/10], Step [900/2534], Loss: 3.4297\n",
            "Epoch [6/10], Step [1000/2534], Loss: 3.2352\n",
            "Epoch [6/10], Step [1100/2534], Loss: 3.1782\n",
            "Epoch [6/10], Step [1200/2534], Loss: 3.6038\n",
            "Epoch [6/10], Step [1300/2534], Loss: 3.1884\n",
            "Epoch [6/10], Step [1400/2534], Loss: 3.3273\n",
            "Epoch [6/10], Step [1500/2534], Loss: 3.4693\n",
            "Epoch [6/10], Step [1600/2534], Loss: 3.6596\n",
            "Epoch [6/10], Step [1700/2534], Loss: 3.4342\n",
            "Epoch [6/10], Step [1800/2534], Loss: 2.9013\n",
            "Epoch [6/10], Step [1900/2534], Loss: 3.5841\n",
            "Epoch [6/10], Step [2000/2534], Loss: 3.3897\n",
            "Epoch [6/10], Step [2100/2534], Loss: 3.5334\n",
            "Epoch [6/10], Step [2200/2534], Loss: 3.4401\n",
            "Epoch [6/10], Step [2300/2534], Loss: 3.0276\n",
            "Epoch [6/10], Step [2400/2534], Loss: 3.7379\n",
            "Epoch [6/10], Step [2500/2534], Loss: 3.1374\n",
            "Epoch [6/10] Average Training Loss: 3.3188\n",
            "Epoch [7/10], Step [100/2534], Loss: 2.4503\n",
            "Epoch [7/10], Step [200/2534], Loss: 2.7753\n",
            "Epoch [7/10], Step [300/2534], Loss: 3.3576\n",
            "Epoch [7/10], Step [400/2534], Loss: 3.2443\n",
            "Epoch [7/10], Step [500/2534], Loss: 3.3447\n",
            "Epoch [7/10], Step [600/2534], Loss: 3.3343\n",
            "Epoch [7/10], Step [700/2534], Loss: 2.8088\n",
            "Epoch [7/10], Step [800/2534], Loss: 2.8736\n",
            "Epoch [7/10], Step [900/2534], Loss: 3.3612\n",
            "Epoch [7/10], Step [1000/2534], Loss: 3.4889\n",
            "Epoch [7/10], Step [1100/2534], Loss: 3.0407\n",
            "Epoch [7/10], Step [1200/2534], Loss: 3.2983\n",
            "Epoch [7/10], Step [1300/2534], Loss: 2.7989\n",
            "Epoch [7/10], Step [1400/2534], Loss: 3.2726\n",
            "Epoch [7/10], Step [1500/2534], Loss: 3.0714\n",
            "Epoch [7/10], Step [1600/2534], Loss: 3.4607\n",
            "Epoch [7/10], Step [1700/2534], Loss: 3.1994\n",
            "Epoch [7/10], Step [1800/2534], Loss: 3.1966\n",
            "Epoch [7/10], Step [1900/2534], Loss: 2.8683\n",
            "Epoch [7/10], Step [2000/2534], Loss: 3.2877\n",
            "Epoch [7/10], Step [2100/2534], Loss: 3.3518\n",
            "Epoch [7/10], Step [2200/2534], Loss: 3.2262\n",
            "Epoch [7/10], Step [2300/2534], Loss: 3.3166\n",
            "Epoch [7/10], Step [2400/2534], Loss: 3.2201\n",
            "Epoch [7/10], Step [2500/2534], Loss: 3.7491\n",
            "Epoch [7/10] Average Training Loss: 3.1940\n",
            "Epoch [8/10], Step [100/2534], Loss: 2.5420\n",
            "Epoch [8/10], Step [200/2534], Loss: 2.9625\n",
            "Epoch [8/10], Step [300/2534], Loss: 2.8384\n",
            "Epoch [8/10], Step [400/2534], Loss: 2.6284\n",
            "Epoch [8/10], Step [500/2534], Loss: 3.0695\n",
            "Epoch [8/10], Step [600/2534], Loss: 2.6590\n",
            "Epoch [8/10], Step [700/2534], Loss: 3.5130\n",
            "Epoch [8/10], Step [800/2534], Loss: 3.1344\n",
            "Epoch [8/10], Step [900/2534], Loss: 2.5274\n",
            "Epoch [8/10], Step [1000/2534], Loss: 2.8809\n",
            "Epoch [8/10], Step [1100/2534], Loss: 3.1059\n",
            "Epoch [8/10], Step [1200/2534], Loss: 3.3856\n",
            "Epoch [8/10], Step [1300/2534], Loss: 3.1029\n",
            "Epoch [8/10], Step [1400/2534], Loss: 3.1799\n",
            "Epoch [8/10], Step [1500/2534], Loss: 3.0394\n",
            "Epoch [8/10], Step [1600/2534], Loss: 2.7853\n",
            "Epoch [8/10], Step [1700/2534], Loss: 3.1263\n",
            "Epoch [8/10], Step [1800/2534], Loss: 3.6759\n",
            "Epoch [8/10], Step [1900/2534], Loss: 3.3086\n",
            "Epoch [8/10], Step [2000/2534], Loss: 3.6638\n",
            "Epoch [8/10], Step [2100/2534], Loss: 3.3747\n",
            "Epoch [8/10], Step [2200/2534], Loss: 3.4899\n",
            "Epoch [8/10], Step [2300/2534], Loss: 3.2125\n",
            "Epoch [8/10], Step [2400/2534], Loss: 2.8202\n",
            "Epoch [8/10], Step [2500/2534], Loss: 2.9235\n",
            "Epoch [8/10] Average Training Loss: 3.0896\n",
            "Epoch [9/10], Step [100/2534], Loss: 2.7612\n",
            "Epoch [9/10], Step [200/2534], Loss: 2.8692\n",
            "Epoch [9/10], Step [300/2534], Loss: 2.6465\n",
            "Epoch [9/10], Step [400/2534], Loss: 2.9397\n",
            "Epoch [9/10], Step [500/2534], Loss: 3.1130\n",
            "Epoch [9/10], Step [600/2534], Loss: 2.8226\n",
            "Epoch [9/10], Step [700/2534], Loss: 2.7715\n",
            "Epoch [9/10], Step [800/2534], Loss: 3.5214\n",
            "Epoch [9/10], Step [900/2534], Loss: 2.8098\n",
            "Epoch [9/10], Step [1000/2534], Loss: 2.6906\n",
            "Epoch [9/10], Step [1100/2534], Loss: 2.6593\n",
            "Epoch [9/10], Step [1200/2534], Loss: 2.9716\n",
            "Epoch [9/10], Step [1300/2534], Loss: 2.8245\n",
            "Epoch [9/10], Step [1400/2534], Loss: 3.5115\n",
            "Epoch [9/10], Step [1500/2534], Loss: 2.9156\n",
            "Epoch [9/10], Step [1600/2534], Loss: 3.2095\n",
            "Epoch [9/10], Step [1700/2534], Loss: 2.9243\n",
            "Epoch [9/10], Step [1800/2534], Loss: 3.5557\n",
            "Epoch [9/10], Step [1900/2534], Loss: 3.0381\n",
            "Epoch [9/10], Step [2000/2534], Loss: 3.8757\n",
            "Epoch [9/10], Step [2100/2534], Loss: 2.8010\n",
            "Epoch [9/10], Step [2200/2534], Loss: 3.0782\n",
            "Epoch [9/10], Step [2300/2534], Loss: 3.2449\n",
            "Epoch [9/10], Step [2400/2534], Loss: 3.3818\n",
            "Epoch [9/10], Step [2500/2534], Loss: 3.2704\n",
            "Epoch [9/10] Average Training Loss: 2.9954\n",
            "Epoch [10/10], Step [100/2534], Loss: 2.6355\n",
            "Epoch [10/10], Step [200/2534], Loss: 2.9424\n",
            "Epoch [10/10], Step [300/2534], Loss: 2.5249\n",
            "Epoch [10/10], Step [400/2534], Loss: 2.5070\n",
            "Epoch [10/10], Step [500/2534], Loss: 3.0108\n",
            "Epoch [10/10], Step [600/2534], Loss: 2.9942\n",
            "Epoch [10/10], Step [700/2534], Loss: 2.7996\n",
            "Epoch [10/10], Step [800/2534], Loss: 2.4098\n",
            "Epoch [10/10], Step [900/2534], Loss: 2.8922\n",
            "Epoch [10/10], Step [1000/2534], Loss: 3.0788\n",
            "Epoch [10/10], Step [1100/2534], Loss: 2.7248\n",
            "Epoch [10/10], Step [1200/2534], Loss: 3.5307\n",
            "Epoch [10/10], Step [1300/2534], Loss: 3.0507\n",
            "Epoch [10/10], Step [1400/2534], Loss: 3.6449\n",
            "Epoch [10/10], Step [1500/2534], Loss: 2.4792\n",
            "Epoch [10/10], Step [1600/2534], Loss: 3.6472\n",
            "Epoch [10/10], Step [1700/2534], Loss: 3.3077\n",
            "Epoch [10/10], Step [1800/2534], Loss: 2.9440\n",
            "Epoch [10/10], Step [1900/2534], Loss: 3.3929\n",
            "Epoch [10/10], Step [2000/2534], Loss: 3.4715\n",
            "Epoch [10/10], Step [2100/2534], Loss: 3.0561\n",
            "Epoch [10/10], Step [2200/2534], Loss: 2.6784\n",
            "Epoch [10/10], Step [2300/2534], Loss: 3.0489\n",
            "Epoch [10/10], Step [2400/2534], Loss: 3.0884\n",
            "Epoch [10/10], Step [2500/2534], Loss: 3.3150\n",
            "Epoch [10/10] Average Training Loss: 2.9142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55a5bb06",
        "outputId": "5785160e-8c17-4201-8fe4-5a90ee5def46"
      },
      "source": [
        "# --- Step to get a prediction from the model ---\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation for evaluation\n",
        "with torch.no_grad():\n",
        "    # Get one batch from the test loader\n",
        "    # We can iterate through the test_loader or use `next(iter(test_loader))` to get a single batch\n",
        "    context_batch, target_batch = next(iter(test_loader))\n",
        "\n",
        "    # Move the batch to the same device as the model\n",
        "    context_batch, target_batch = context_batch.to(device), target_batch.to(device)\n",
        "\n",
        "    # Get the model's output for the context batch\n",
        "    outputs = model(context_batch)\n",
        "\n",
        "    # The outputs are logits (raw scores). To get probabilities, you'd typically use softmax.\n",
        "    # However, for finding the predicted class (word ID), we can just find the index\n",
        "    # with the maximum logit value, as softmax preserves the order of the logits.\n",
        "    # torch.argmax returns the index of the maximum value along a dimension.\n",
        "    # Here, dim=1 means we find the max index for each item in the batch (each example's output).\n",
        "    predicted_ids = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Move the tensors back to CPU and convert to numpy arrays or lists for easier iteration and use with the dictionary\n",
        "    context_batch_cpu = context_batch.cpu().numpy()\n",
        "    target_batch_cpu = target_batch.cpu().numpy()\n",
        "    predicted_ids_cpu = predicted_ids.cpu().numpy()\n",
        "\n",
        "    # Display predictions for a few samples in the batch\n",
        "    num_samples_to_display = 5\n",
        "    print(\"\\n--- Sample Predictions ---\")\n",
        "    for i in range(min(num_samples_to_display, context_batch_cpu.shape[0])):\n",
        "        context_ids = context_batch_cpu[i]\n",
        "        real_target_id = target_batch_cpu[i]\n",
        "        predicted_target_id = predicted_ids_cpu[i]\n",
        "\n",
        "        # Translate IDs back to words\n",
        "        context_words = [id_to_word.get(idx, '<UNK>') for idx in context_ids]\n",
        "        real_target_word = id_to_word.get(real_target_id, '<UNK>')\n",
        "        predicted_target_word = id_to_word.get(predicted_target_id, '<UNK>')\n",
        "\n",
        "        print(f\"Context: {context_words}\")\n",
        "        print(f\"Real Target: {real_target_word}\")\n",
        "        print(f\"Predicted Target: {predicted_target_word}\")\n",
        "        print(\"-\" * 20)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample Predictions ---\n",
            "Context: ['i', 'do', 'your', 'words.']\n",
            "Real Target: <UNK>\n",
            "Predicted Target: beseech\n",
            "--------------------\n",
            "Context: ['do', '<UNK>', 'words.', 'be']\n",
            "Real Target: your\n",
            "Predicted Target: your\n",
            "--------------------\n",
            "Context: ['<UNK>', 'your', 'be', 'that']\n",
            "Real Target: words.\n",
            "Predicted Target: <UNK>\n",
            "--------------------\n",
            "Context: ['your', 'words.', 'that', 'you']\n",
            "Real Target: be\n",
            "Predicted Target: and\n",
            "--------------------\n",
            "Context: ['words.', 'be', 'you', 'are,']\n",
            "Real Target: that\n",
            "Predicted Target: as\n",
            "--------------------\n"
          ]
        }
      ]
    }
  ]
}